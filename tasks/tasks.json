{
  "master": {
    "tasks": [
      {
        "id": 4,
        "title": "Refactor High-Complexity Modules and Duplicated Code",
        "description": "Improve code quality by refactoring large modules (`smart_filters.py`, `smart_retrieval.py`), reducing code duplication in AI engine modules, and simplifying high-complexity functions as identified in the PRD, applying research-backed refactoring best practices.",
        "status": "pending",
        "dependencies": [],
        "priority": "medium",
        "details": "This refactoring will be performed on the new `src/backend` structure, incorporating modern refactoring best practices and an incremental, test-driven approach.\n\n1.  **Module Splitting & Incremental Refactoring for `smart_filters.py` and `smart_retrieval.py`**:\n    *   **Goal**: Decompose these large, monolithic modules (`src/backend/smart_filters.py` at 1598 lines and `src/backend/smart_retrieval.py` at 1198 lines) into smaller, more cohesive, and single-responsibility modules, strictly adhering to the Single Responsibility Principle (SRP) and aiming for high cohesion and low coupling.\n    *   **Strategy**: Employ a phased 'Extract Module' or 'Extract Class' refactoring pattern. For each large module:\n        *   **Identify Cohesive Units**: Systematically analyze the existing files to identify distinct logical concerns, architectural layers, or sets of closely related functions/data that exhibit 'Feature Envy' (methods using data from other objects more than their own) or form 'Data Clumps'. These are prime candidates for extraction.\n        *   **Create Dedicated Packages**: Create a dedicated package directory, e.g., `src/backend/smart_filters/` and `src/backend/smart_retrieval/`. This clearly delineates the new module boundaries.\n        *   **Module Breakdown Examples**:\n            *   `smart_filters.py`: Potential modules include `rule_parsers.py` (for parsing rule definitions from configuration, e.g., `src/backend/smart_filters/rule_parsers.py`), `filter_evaluators.py` (for applying filter logic), `data_transformers.py` (for preparing data for filtering), `persistence.py` (for loading/saving filter configurations).\n            *   `smart_retrieval.py`: Potential modules include `query_builders.py` (for constructing retrieval queries), `ranking_algorithms.py` (for scoring and ordering results), `source_integrations.py` (for interfacing with data sources, potentially using 'Ports and Adapters' pattern), `result_processors.py` (for post-processing retrieved items).\n        *   **Incremental Steps**: For each identified concern, move the related functions, classes, and their associated data into a new, dedicated Python file within its respective package directory. Update all internal and external imports to reflect the new structure. Each extraction should be a small, self-contained, and testable commit.\n        *   **Public API Control**: Utilize `__init__.py` within these new packages to define and control the public API, exposing only necessary functions/classes to avoid tightly coupling consumers to internal module structures (e.g., using `from .sub_module import func_name` in `__init__.py` to expose `func_name` at the package level).\n\n2.  **Reduce Code Duplication in AI Engine**:\n\n    *   **Goal**: Eliminate the 165-195 reported duplication occurrences across the AI engine modules (e.g., `src/backend/ai/email_filter_node.py` and other related AI components), adhering to the Don't Repeat Yourself (DRY) principle.\n    *   **Strategy**: Identify recurring code blocks, patterns, and boilerplate. Apply 'Extract Method', 'Extract Class', or 'Extract Superclass' refactorings to abstract common logic into shared, reusable utility functions and classes.\n    *   **Location**: Create a new module `src/backend/ai/utils.py` for general AI-related helper functions specific to the AI domain. For broader, cross-cutting utilities (e.g., data validation, common error handling), consider `src/backend/utils/common.py`.\n    *   **Examples of Abstraction**: Look for repeated data validation logic (e.g., using Pydantic models for input validation), common pre-processing steps for text/email content (e.g., tokenization, normalization), standardized error handling patterns, logging boilerplate, or shared data access patterns. Prioritize creating small, pure functions where inputs map deterministically to outputs, making them easily testable and reusable.\n\n3.  **Function Simplification (`setup_dependencies`, `migrate_sqlite_to_json`, `run`)**:\n    *   **Goal**: Significantly reduce the cyclomatic complexity of `setup_dependencies` (complexity 21, likely found in `src/backend/dependencies.py`), `migrate_sqlite_to_json` (complexity 17, likely in `src/backend/utils/database.py`), and the `run` method in `src/backend/ai/email_filter_node.py` (complexity 16), improving readability and maintainability.\n    *   **Strategy**: Apply the 'Extract Method' refactoring pattern incrementally. Break down these large functions into smaller, private helper functions (conventionally prefixed with `_`) that each encapsulate a single, distinct responsibility or logical step. Consider breaking down based on:\n        *   **Sequence of Operations**: Separate distinct sequential steps.\n        *   **Decision Points**: Extract complex conditional logic into its own method.\n        *   **Loop Processing**: Encapsulate the body of a loop or its initialization/finalization.\n    *   **Specifics**:\n        *   `setup_dependencies` (in `src/backend/dependencies.py`): Extract initialization steps into private methods like `_initialize_database_connection()`, `_load_application_config()`, `_setup_logging()`, `_register_event_handlers()`, etc.\n        *   `migrate_sqlite_to_json` (in `src/backend/utils/database.py`): Decompose into steps such as `_read_data_from_sqlite(db_path)`, `_transform_row_to_json_record(row)`, `_write_json_records_to_file(records, output_path)`.\n        *   `run` (in `src/backend/ai/email_filter_node.py`): Identify distinct phases of its execution lifecycle, such as `_ingest_email_messages()`, `_preprocess_message(message)`, `_apply_ai_filters(processed_message)`, `_collate_results(filter_results)`, `_store_or_dispatch_results(final_results)`. Each phase should become a separate, focused private method.\n    *   **Naming**: Ensure extracted methods have descriptive names that clearly convey their purpose and avoid generic names. If a function has too many parameters after extraction, consider 'Introduce Parameter Object' refactoring.\n\n4.  **Continuous Integration with Refactoring**: Throughout this task, maintain small, atomic commits for each refactoring step, ensuring that the system remains in a working state after every commit. This facilitates easy rollback and debugging.",
        "testStrategy": "This is a pure refactoring task; no new functionality should be introduced, and the external behavior of the system must remain unchanged. A robust test-driven refactoring strategy will be employed:\n1.  **Pre-refactoring Characterization Tests (Golden Master)**: Before initiating any changes, ensure all target modules (`src/backend/smart_filters.py`, `src/backend/smart_retrieval.py`, `src/backend/ai/email_filter_node.py`, `src/backend/dependencies.py`, `src/backend/utils/database.py`, and any relevant AI engine modules) have comprehensive test coverage. If coverage is lacking, write 'characterization tests' (also known as 'golden master tests') to rigorously capture and lock in the current observable behavior, even if it's imperfect. These tests are critical to ensure functional equivalence post-refactoring.\n2.  **Incremental Testing and Verification**: After *each small, atomic refactoring step* (e.g., extracting a single method, moving a set of functions to a new module, changing an import), immediately run the relevant test suite (including characterization tests) to catch regressions early. This iterative testing approach is fundamental to safe and effective refactoring. The goal is to always have a green test suite.\n3.  **Behavioral Preservation**: All existing tests for the modified code must pass without any alterations to the tests themselves. The external behavior of the system should remain unchanged. New tests should only be added to cover newly extracted units or to improve coverage where it was initially insufficient for safe refactoring.\n4.  **Static Analysis Integration and Monitoring**: \n    *   **Baseline**: Run a static analysis tool like `radon` (`radon cc -a .` for cyclomatic complexity and `radon mi -a .` for maintainability index) across the codebase (or targeted modules) *before* starting the refactoring to establish a quantitative baseline for complexity and maintainability.\n    *   **Continuous Monitoring**: Integrate these checks into a pre-commit hook or CI/CD pipeline if possible. Alternatively, run them regularly during the refactoring process to monitor progress, ensuring complexity metrics decrease and maintainability indices improve. Tools like `pylint` and `flake8` should also be run to enforce coding standards and identify potential issues.\n    *   **Post-refactoring Verification**: Re-run `radon`, `pylint`, and `flake8` after the refactoring is complete to quantify the reduction in cyclomatic complexity, confirm improved maintainability, and verify adherence to coding standards, thereby proving the task's success.\n5.  **Performance Check**: Conduct basic performance smoke tests or run existing benchmarks to ensure that the refactoring has not introduced any significant performance degradations, especially in critical paths involving the AI engine or data processing. Tools like `cProfile` can be used for targeted analysis if needed.",
        "subtasks": [
          {
            "id": 1,
            "title": "Establish Refactoring Strategy & Static Analysis Setup",
            "description": "Define the overall refactoring approach, set up static analysis tools (e.g., Pylint, Mypy) for complexity and duplication detection, and configure the development environment for incremental, test-driven refactoring.",
            "dependencies": [],
            "details": "This involves ensuring alignment on refactoring best practices, setting up pre-commit hooks for linting, and establishing clear guidelines for small, atomic commits to maintain system stability. Define what 'research-backed best practices' means.",
            "status": "pending",
            "testStrategy": "Validate tool integration by running static analysis on existing codebase and checking reports. Ensure pre-commit hooks function as expected."
          },
          {
            "id": 2,
            "title": "Develop Characterization Tests for `smart_filters.py`",
            "description": "Create a comprehensive suite of characterization tests (golden master tests) for `src/backend/smart_filters.py` to ensure its external behavior is preserved during refactoring.",
            "dependencies": [
              "4.1"
            ],
            "details": "These tests should cover all major public functions and classes, including edge cases, and capture current outputs. Aim for high code coverage to act as a safety net for subsequent refactoring steps.",
            "status": "pending",
            "testStrategy": "Execute the newly created test suite against the original `smart_filters.py` to confirm they accurately capture the current behavior and pass without issues."
          },
          {
            "id": 3,
            "title": "Develop Characterization Tests for `smart_retrieval.py`",
            "description": "Create a comprehensive suite of characterization tests for `src/backend/smart_retrieval.py` to ensure its external behavior is preserved during refactoring.",
            "dependencies": [
              "4.1"
            ],
            "details": "Similar to `smart_filters.py`, these tests will cover major public functions, classes, and edge cases, capturing current outputs to act as a safety net during decomposition.",
            "status": "pending",
            "testStrategy": "Execute the newly created test suite against the original `smart_retrieval.py` to confirm they accurately capture the current behavior and pass without issues."
          },
          {
            "id": 4,
            "title": "Analyze and Identify Cohesive Units in `smart_filters.py`",
            "description": "Conduct a detailed analysis of `src/backend/smart_filters.py` (1598 lines) to identify distinct logical concerns, architectural layers, and data clumps suitable for extraction into smaller, cohesive modules.",
            "dependencies": [],
            "details": "This involves manual code review, using static analysis tools to pinpoint areas exhibiting high coupling or low cohesion, and mapping potential new module boundaries based on SRP principles.",
            "status": "pending",
            "testStrategy": "Review proposed module breakdowns with team leads/architects for approval. Document findings and proposed new module structure."
          },
          {
            "id": 5,
            "title": "Extract Rule Parsing & Data Transformation from `smart_filters.py`",
            "description": "Create the `src/backend/smart_filters/` package and extract functions/classes related to rule parsing and data transformation into new modules like `rule_parsers.py` and `data_transformers.py`.",
            "dependencies": [
              "4.4"
            ],
            "details": "Move relevant code from `smart_filters.py` into the new dedicated files. Update all internal and external imports to reflect the new structure. Ensure `__init__.py` controls the public API for the package. Run characterization tests after extraction.",
            "status": "pending",
            "testStrategy": "Run existing characterization tests for `smart_filters.py` to ensure no regressions. Verify the new module structure and imports are correct."
          },
          {
            "id": 6,
            "title": "Extract Filter Evaluation & Persistence from `smart_filters.py`",
            "description": "Extract functions/classes related to filter evaluation logic and persistence mechanisms from `smart_filters.py` into new modules like `filter_evaluators.py` and `persistence.py` within `src/backend/smart_filters/`.",
            "dependencies": [
              "4.4",
              "4.5"
            ],
            "details": "Continue the incremental refactoring process, moving the identified code, updating necessary imports, and continuously verifying against the characterization tests to ensure functionality remains unchanged.",
            "status": "pending",
            "testStrategy": "Execute all `smart_filters.py` characterization tests after this extraction. Conduct manual spot checks if specific functionality is heavily impacted."
          },
          {
            "id": 7,
            "title": "Finalize `smart_filters` Module Refactoring & API Definition",
            "description": "Complete the decomposition of the original `smart_filters.py`, ensure all extracted modules are integrated correctly, and refine the `src/backend/smart_filters/__init__.py` for clear public API exposure.",
            "dependencies": [
              "4.4",
              "4.5",
              "4.6"
            ],
            "details": "Verify that the original `smart_filters.py` is now a much smaller orchestrator or has been completely replaced by the package structure. Ensure all internal dependencies are resolved and the package's external interface is stable.",
            "status": "pending",
            "testStrategy": "Run the full `smart_filters.py` characterization test suite. Review the final module structure and public API surface for consistency and clarity."
          },
          {
            "id": 8,
            "title": "Analyze and Identify Cohesive Units in `smart_retrieval.py`",
            "description": "Conduct a detailed analysis of `src/backend/smart_retrieval.py` (1198 lines) to identify distinct logical concerns and units suitable for extraction into smaller, cohesive modules.",
            "dependencies": [],
            "details": "Apply the same analytical approach as for `smart_filters.py`, focusing on identifying components responsible for query building, ranking algorithms, source integrations, and result processing.",
            "status": "pending",
            "testStrategy": "Review proposed module breakdowns with team leads/architects for approval. Document findings and proposed new module structure."
          },
          {
            "id": 9,
            "title": "Extract Query Building & Ranking Algorithms from `smart_retrieval.py`",
            "description": "Create `src/backend/smart_retrieval/` package and extract components related to query construction and result ranking into new modules like `query_builders.py` and `ranking_algorithms.py`.",
            "dependencies": [
              "4.8"
            ],
            "details": "Move relevant code from `smart_retrieval.py` into the new dedicated files within the package. Update all necessary imports and define the public API via `__init__.py`. Verify with characterization tests.",
            "status": "pending",
            "testStrategy": "Run existing characterization tests for `smart_retrieval.py` to ensure no regressions. Verify the new module structure and imports are correct."
          },
          {
            "id": 10,
            "title": "Extract Source Integrations & Result Processors from `smart_retrieval.py`",
            "description": "Extract components for interfacing with various data sources (`source_integrations.py`) and post-processing retrieved items (`result_processors.py`) from `smart_retrieval.py` into its new sub-package.",
            "dependencies": [
              "4.8",
              "4.9"
            ],
            "details": "Continue incremental extraction, moving the identified code, updating all references, and validating against the characterization test suite to ensure functional integrity.",
            "status": "pending",
            "testStrategy": "Execute all `smart_retrieval.py` characterization tests after this extraction. Conduct manual spot checks if specific functionality is heavily impacted."
          },
          {
            "id": 11,
            "title": "Reduce Code Duplication in AI Engine Modules",
            "description": "Identify and refactor duplicated code patterns (e.g., validation, pre-processing, error handling, logging) across AI engine modules (e.g., `email_filter_node.py`) into reusable functions/classes.",
            "dependencies": [
              "4.1"
            ],
            "details": "Create `src/backend/ai/utils.py` for AI-specific utilities and `src/backend/utils/common.py` for broader utilities. Use static analysis tools to pinpoint 165-195 duplication occurrences. Apply 'Extract Method/Class' refactoring.",
            "status": "pending",
            "testStrategy": "Run existing unit/integration tests for affected AI modules. Create new unit tests for the extracted utility functions/classes. Static analysis should report reduced duplication."
          },
          {
            "id": 12,
            "title": "Simplify `setup_dependencies` Function",
            "description": "Refactor `setup_dependencies` (likely in `src/backend/dependencies.py`) to reduce its cyclomatic complexity (from 21) by extracting logical blocks into smaller, private helper methods.",
            "dependencies": [
              "4.1"
            ],
            "details": "Break down into methods like `_initialize_database_connection()`, `_load_application_config()`, `_setup_logging()`, `_register_event_handlers()`. Ensure original functionality is preserved through existing tests.",
            "status": "pending",
            "testStrategy": "Ensure existing tests covering `setup_dependencies` pass. If no dedicated tests exist, create characterization tests or focused unit tests for the refactored parts."
          },
          {
            "id": 13,
            "title": "Simplify `migrate_sqlite_to_json` Function",
            "description": "Refactor `migrate_sqlite_to_json` (likely in `src/backend/utils/database.py`) to reduce its cyclomatic complexity (from 17) by extracting logical steps into smaller, private helper methods.",
            "dependencies": [
              "4.1"
            ],
            "details": "Decompose into `_read_data_from_sqlite()`, `_transform_row_to_json_record()`, `_write_json_records_to_file()`. Ensure existing tests continue to pass and cover the refactored logic.",
            "status": "pending",
            "testStrategy": "Verify that existing tests for the migration utility function pass. Create new focused unit tests for the extracted private methods if beneficial."
          },
          {
            "id": 14,
            "title": "Simplify `run` Method in `email_filter_node.py`",
            "description": "Refactor the `run` method in `src/backend/ai/email_filter_node.py` to reduce its cyclomatic complexity (from 16) by extracting distinct phases of execution into private helper methods.",
            "dependencies": [
              "4.1"
            ],
            "details": "Break down into `_ingest_email_messages()`, `_preprocess_message()`, `_apply_ai_filters()`, `_collate_results()`, `_store_or_dispatch_results()`. Ensure existing tests cover the refactored logic and system behavior remains consistent.",
            "status": "pending",
            "testStrategy": "Execute all relevant unit and integration tests for `email_filter_node.py`. Confirm end-to-end functionality of the AI filter node is unchanged."
          },
          {
            "id": 15,
            "title": "Perform initial analysis and characterization testing for smart_filters.py",
            "description": "Before refactoring, analyze the `smart_filters.py` module to understand its current behavior and establish a comprehensive suite of characterization tests. These tests will serve as a 'golden master' to ensure no regressions are introduced during refactoring.",
            "dependencies": [],
            "details": "Analyze `src/backend/smart_filters.py` (1598 lines). Identify all public interfaces and critical internal logic paths. Write characterization tests that capture the module's behavior. Ensure tests cover edge cases and typical workflows.",
            "status": "pending",
            "testStrategy": "Develop a suite of characterization tests that accurately reflect the current external and internal observable behavior of `smart_filters.py`. These tests will be run before and after each refactoring step."
          },
          {
            "id": 16,
            "title": "Extract rule parsing logic to src/backend/smart_filters/rule_parsers.py",
            "description": "Identify and extract all functions and classes related to parsing rule definitions from configurations within `smart_filters.py` into a new, dedicated module `src/backend/smart_filters/rule_parsers.py`.",
            "dependencies": [
              15
            ],
            "details": "Create the `src/backend/smart_filters/` package. Move relevant rule parsing functions/classes from `smart_filters.py` to `src/backend/smart_filters/rule_parsers.py`. Update all internal and external references to use the new module path.",
            "status": "pending",
            "testStrategy": "Run all characterization tests from task 15. Add new unit tests specifically for the extracted `rule_parsers.py` module to ensure its isolated functionality."
          },
          {
            "id": 17,
            "title": "Extract filter evaluation logic to src/backend/smart_filters/filter_evaluators.py",
            "description": "Isolate and extract all logic responsible for applying filter conditions and evaluating them against data within `smart_filters.py` into `src/backend/smart_filters/filter_evaluators.py`.",
            "dependencies": [
              16
            ],
            "details": "Identify filter application and evaluation functions/classes in `smart_filters.py`. Move them to `src/backend/smart_filters/filter_evaluators.py`. Adjust imports across the codebase to reflect this change. Ensure the module adheres to SRP.",
            "status": "pending",
            "testStrategy": "Run all characterization tests and unit tests. Add specific unit tests for the extracted `filter_evaluators.py` module to verify correct filter logic application."
          },
          {
            "id": 18,
            "title": "Extract data transformation logic to src/backend/smart_filters/data_transformers.py",
            "description": "Identify and extract all code responsible for preparing or transforming data prior to filtering operations from `smart_filters.py` into a new module, `src/backend/smart_filters/data_transformers.py`.",
            "dependencies": [
              17
            ],
            "details": "Locate data pre-processing and transformation logic within `smart_filters.py`. Relocate this code to `src/backend/smart_filters/data_transformers.py`. Update all affected import statements. Verify the module's cohesion.",
            "status": "pending",
            "testStrategy": "Execute all existing tests. Create dedicated unit tests for the `data_transformers.py` module to confirm data transformation accuracy and consistency."
          },
          {
            "id": 19,
            "title": "Refactor smart_filters package __init__.py and update remaining imports",
            "description": "Establish a clear public API for the new `smart_filters` package using its `__init__.py` and update any remaining direct imports from the original monolithic `smart_filters.py` to the new modular structure.",
            "dependencies": [
              18
            ],
            "details": "Create or modify `src/backend/smart_filters/__init__.py` to expose necessary functions/classes from `rule_parsers`, `filter_evaluators`, and `data_transformers`. Update all external imports that still reference the old `smart_filters.py` path.",
            "status": "pending",
            "testStrategy": "Run all characterization tests and unit tests for the `smart_filters` package to ensure the public API functions correctly and no import errors exist."
          },
          {
            "id": 20,
            "title": "Perform initial analysis and characterization testing for smart_retrieval.py",
            "description": "Before refactoring, thoroughly analyze the `smart_retrieval.py` module to understand its current behavior and establish a comprehensive suite of characterization tests, ensuring a 'golden master' for regression prevention.",
            "dependencies": [
              19
            ],
            "details": "Analyze `src/backend/smart_retrieval.py` (1198 lines). Identify all public interfaces and critical internal logic paths. Write characterization tests that capture the module's behavior, including data source integrations and ranking.",
            "status": "pending",
            "testStrategy": "Develop a suite of characterization tests that accurately reflect the current external and internal observable behavior of `smart_retrieval.py`. These tests will be run before and after each refactoring step."
          },
          {
            "id": 21,
            "title": "Extract query building logic to src/backend/smart_retrieval/query_builders.py",
            "description": "Identify and extract all functions and classes responsible for constructing retrieval queries from `smart_retrieval.py` into a new, dedicated module `src/backend/smart_retrieval/query_builders.py`.",
            "dependencies": [
              20
            ],
            "details": "Create the `src/backend/smart_retrieval/` package. Move relevant query building functions/classes from `smart_retrieval.py` to `src/backend/smart_retrieval/query_builders.py`. Update all internal and external references.",
            "status": "pending",
            "testStrategy": "Run all characterization tests from task 20. Add new unit tests specifically for the extracted `query_builders.py` module to ensure its isolated functionality and correct query generation."
          },
          {
            "id": 22,
            "title": "Extract ranking algorithms to src/backend/smart_retrieval/ranking_algorithms.py",
            "description": "Isolate and extract all logic related to scoring and ordering retrieval results within `smart_retrieval.py` into `src/backend/smart_retrieval/ranking_algorithms.py`.",
            "dependencies": [
              21
            ],
            "details": "Identify scoring and ranking functions/classes in `smart_retrieval.py`. Move them to `src/backend/smart_retrieval/ranking_algorithms.py`. Adjust imports across the codebase to reflect this change. Ensure the module adheres to SRP.",
            "status": "pending",
            "testStrategy": "Run all characterization tests and unit tests. Add specific unit tests for the extracted `ranking_algorithms.py` module to verify correct result ordering and scoring."
          },
          {
            "id": 23,
            "title": "Refactor smart_retrieval package __init__.py and update remaining imports",
            "description": "Establish a clear public API for the new `smart_retrieval` package using its `__init__.py` and update any remaining direct imports from the original monolithic `smart_retrieval.py` to the new modular structure.",
            "dependencies": [
              22
            ],
            "details": "Create or modify `src/backend/smart_retrieval/__init__.py` to expose necessary functions/classes from `query_builders` and `ranking_algorithms`. Update all external imports that still reference the old `smart_retrieval.py` path.",
            "status": "pending",
            "testStrategy": "Run all characterization tests and unit tests for the `smart_retrieval` package to ensure the public API functions correctly and no import errors exist."
          },
          {
            "id": 24,
            "title": "Identify and extract common AI engine utilities to src/backend/ai/utils.py",
            "description": "Analyze AI engine modules for duplicated code patterns specific to AI processing (e.g., tokenization, data normalization, AI-specific validation) and extract them into `src/backend/ai/utils.py`.",
            "dependencies": [
              23
            ],
            "details": "Scan AI engine modules (`src/backend/ai/email_filter_node.py` and others) for repeated AI-centric logic. Create `src/backend/ai/utils.py` and move common helper functions or classes there. Update all call sites to use the new utility module.",
            "status": "pending",
            "testStrategy": "Perform static analysis for duplication. Add unit tests for each extracted utility function in `src/backend/ai/utils.py`. Run existing integration tests for AI engine modules to ensure no behavioral changes."
          },
          {
            "id": 25,
            "title": "Identify and extract common cross-cutting utilities to src/backend/utils/common.py",
            "description": "Review the entire codebase for general, non-domain-specific duplicated patterns (e.g., generic input validation, error handling, logging boilerplate) and move them to `src/backend/utils/common.py`.",
            "dependencies": [
              24
            ],
            "details": "Perform a broader code scan for general utility functions or classes that appear in multiple, unrelated parts of the codebase. Create `src/backend/utils/common.py` and relocate these utilities. Update all references to the new module.",
            "status": "pending",
            "testStrategy": "Perform static analysis for duplication. Add unit tests for each extracted utility function in `src/backend/utils/common.py`. Run relevant integration tests across the backend to ensure stability."
          },
          {
            "id": 26,
            "title": "Simplify setup_dependencies function in src/backend/dependencies.py",
            "description": "Refactor the `setup_dependencies` function (complexity 21) in `src/backend/dependencies.py` by applying 'Extract Method' to reduce its cyclomatic complexity and improve readability.",
            "dependencies": [
              25
            ],
            "details": "Break down `setup_dependencies` into smaller, private helper methods (e.g., `_initialize_database_connection`, `_load_application_config`, `_setup_logging`). Each extracted method should encapsulate a single responsibility. Use descriptive names.",
            "status": "pending",
            "testStrategy": "Run all characterization tests that rely on `setup_dependencies`. Add new unit tests for the extracted private methods where applicable. Verify the application starts and dependencies are correctly initialized."
          },
          {
            "id": 27,
            "title": "Simplify migrate_sqlite_to_json in src/backend/utils/database.py",
            "description": "Refactor the `migrate_sqlite_to_json` function (complexity 17) in `src/backend/utils/database.py` using 'Extract Method' to reduce its cyclomatic complexity and enhance maintainability.",
            "dependencies": [
              26
            ],
            "details": "Decompose `migrate_sqlite_to_json` into distinct, smaller private methods such as `_read_data_from_sqlite(db_path)`, `_transform_row_to_json_record(row)`, and `_write_json_records_to_file(records, output_path)`. Ensure each method is focused.",
            "status": "pending",
            "testStrategy": "Run existing integration tests that involve database migration. Add unit tests for each extracted helper method to verify their specific logic (reading, transforming, writing). Verify migrated JSON data integrity."
          },
          {
            "id": 28,
            "title": "Simplify run method in src/backend/ai/email_filter_node.py",
            "description": "Refactor the `run` method (complexity 16) in `src/backend/ai/email_filter_node.py` by applying 'Extract Method' to break it into smaller, more manageable private functions, thereby improving its readability and testability.",
            "dependencies": [
              27
            ],
            "details": "Identify logical phases within the `run` method (e.g., `_ingest_email_messages()`, `_preprocess_message(message)`, `_apply_ai_filters(processed_message)`). Extract these phases into private helper methods. Ensure clear method responsibilities.",
            "status": "pending",
            "testStrategy": "Run characterization tests for `email_filter_node.py`. Add new unit tests for the extracted private methods to ensure each step of the email processing pipeline functions as expected in isolation. Verify end-to-end filtering behavior."
          },
          {
            "id": 29,
            "title": "Create characterization tests for `smart_filters.py`",
            "description": "Develop a comprehensive suite of characterization tests for the existing `src/backend/smart_filters.py` module to capture its current behavior as a golden master.",
            "dependencies": [],
            "details": "Analyze the module's public API and key internal interactions. Create tests that cover various input scenarios and expected outputs, ensuring no external behavior changes during refactoring. Use a testing framework like pytest.",
            "status": "pending",
            "testStrategy": "Golden Master testing approach; record current behavior before any changes."
          },
          {
            "id": 30,
            "title": "Create characterization tests for `smart_retrieval.py`",
            "description": "Develop a comprehensive suite of characterization tests for the existing `src/backend/smart_retrieval.py` module to capture its current behavior as a golden master.",
            "dependencies": [],
            "details": "Analyze the module's public API and key internal interactions. Create tests that cover various input scenarios and expected outputs, ensuring no external behavior changes during refactoring. Use a testing framework like pytest.",
            "status": "pending",
            "testStrategy": "Golden Master testing approach; record current behavior before any changes."
          },
          {
            "id": 31,
            "title": "Create characterization tests for `setup_dependencies` function",
            "description": "Develop characterization tests for the `setup_dependencies` function (likely in `src/backend/dependencies.py`) to capture its current behavior, ensuring the refactoring preserves its functionality.",
            "dependencies": [],
            "details": "Isolate the `setup_dependencies` function using mocks for external systems (e.g., database, config files) and assert on the side effects and returned dependencies.",
            "status": "pending",
            "testStrategy": "Golden Master testing; verify function's initial state and final state before/after refactoring."
          },
          {
            "id": 32,
            "title": "Create characterization tests for `migrate_sqlite_to_json` function",
            "description": "Develop characterization tests for the `migrate_sqlite_to_json` function (likely in `src/backend/utils/database.py`) to capture its current behavior and output format.",
            "dependencies": [],
            "details": "Create a sample SQLite database, run the function, and assert the structure and content of the generated JSON output. Ensure edge cases like empty database or specific data types are covered.",
            "status": "pending",
            "testStrategy": "Golden Master testing; compare output JSON to a predefined golden master file."
          },
          {
            "id": 33,
            "title": "Create characterization tests for `email_filter_node.run` method",
            "description": "Develop characterization tests for the `run` method in `src/backend/ai/email_filter_node.py` to capture its complex behavior and ensure no regression during simplification.",
            "dependencies": [],
            "details": "Use mocks for external dependencies such as email ingestion, AI models, and storage/dispatch mechanisms. Test various email inputs and verify the final processed results and any side effects.",
            "status": "pending",
            "testStrategy": "Golden Master testing; verify output and side effects against known good results."
          },
          {
            "id": 34,
            "title": "Establish package directory for `smart_filters`",
            "description": "Create the new package directory `src/backend/smart_filters/` and its `__init__.py` file to host the decomposed modules from the original `smart_filters.py`.",
            "dependencies": [
              29
            ],
            "details": "Ensure the `__init__.py` is properly configured to allow for controlled public API exposure later. No functional changes at this stage.",
            "status": "pending",
            "testStrategy": "Verify directory and `__init__.py` existence; no functional tests required at this stage."
          },
          {
            "id": 35,
            "title": "Extract rule parsing logic into `smart_filters/rule_parsers.py`",
            "description": "Analyze `src/backend/smart_filters.py` to identify all functions and classes related to parsing rule definitions and move them into `src/backend/smart_filters/rule_parsers.py`.",
            "dependencies": [
              34
            ],
            "details": "Update all internal and external references/imports to the newly created module. Run characterization tests (id 29) to ensure no behavior change and functionality remains identical.",
            "status": "pending",
            "testStrategy": "Run characterization tests for `smart_filters.py` (id 29) to ensure no regressions after extraction and import updates."
          },
          {
            "id": 36,
            "title": "Extract filter evaluation logic into `smart_filters/filter_evaluators.py`",
            "description": "Analyze `src/backend/smart_filters.py` to identify all functions and classes responsible for applying filter logic and move them into `src/backend/smart_filters/filter_evaluators.py`.",
            "dependencies": [
              35
            ],
            "details": "Update all internal and external references/imports to the newly created module. Run characterization tests (id 29) to ensure no behavior change and functionality remains identical.",
            "status": "pending",
            "testStrategy": "Run characterization tests for `smart_filters.py` (id 29) to ensure no regressions after extraction and import updates."
          },
          {
            "id": 37,
            "title": "Extract data transformation logic into `smart_filters/data_transformers.py`",
            "description": "Analyze `src/backend/smart_filters.py` to identify all functions and classes related to preparing data for filtering and move them into `src/backend/smart_filters/data_transformers.py`.",
            "dependencies": [
              36
            ],
            "details": "Update all internal and external references/imports to the newly created module. Run characterization tests (id 29) to ensure no behavior change and functionality remains identical.",
            "status": "pending",
            "testStrategy": "Run characterization tests for `smart_filters.py` (id 29) to ensure no regressions after extraction and import updates."
          },
          {
            "id": 38,
            "title": "Extract filter configuration persistence into `smart_filters/persistence.py`",
            "description": "Analyze `src/backend/smart_filters.py` to identify all functions and classes responsible for loading/saving filter configurations and move them into `src/backend/smart_filters/persistence.py`.",
            "dependencies": [
              37
            ],
            "details": "Update all internal and external references/imports to the newly created module. Run characterization tests (id 29) to ensure no behavior change and functionality remains identical.",
            "status": "pending",
            "testStrategy": "Run characterization tests for `smart_filters.py` (id 29) to ensure no regressions after extraction and import updates."
          },
          {
            "id": 39,
            "title": "Define `smart_filters` public API via `__init__.py`",
            "description": "Refactor `src/backend/smart_filters/__init__.py` to explicitly define and expose only the necessary functions/classes from its sub-modules, controlling the package's public API.",
            "dependencies": [
              38
            ],
            "details": "Use `from .sub_module import func_name` patterns. Ensure consumers of the `smart_filters` package are not tightly coupled to internal module structures. Verify external calls are updated.",
            "status": "pending",
            "testStrategy": "Run characterization tests for `smart_filters.py` (id 29) to ensure no regressions; verify public API access from external modules remains functional and correct."
          },
          {
            "id": 40,
            "title": "Establish package directory for `smart_retrieval`",
            "description": "Create the new package directory `src/backend/smart_retrieval/` and its `__init__.py` file to host the decomposed modules from the original `smart_retrieval.py`.",
            "dependencies": [
              30
            ],
            "details": "Ensure the `__init__.py` is properly configured to allow for controlled public API exposure later. No functional changes at this stage.",
            "status": "pending",
            "testStrategy": "Verify directory and `__init__.py` existence; no functional tests required at this stage."
          },
          {
            "id": 41,
            "title": "Extract query building logic into `smart_retrieval/query_builders.py`",
            "description": "Analyze `src/backend/smart_retrieval.py` to identify functions/classes for constructing retrieval queries and move them into `src/backend/smart_retrieval/query_builders.py`.",
            "dependencies": [
              40
            ],
            "details": "Update all internal and external references/imports to the newly created module. Run characterization tests (id 30) to ensure no behavior change and functionality remains identical.",
            "status": "pending",
            "testStrategy": "Run characterization tests for `smart_retrieval.py` (id 30) to ensure no regressions after extraction and import updates."
          },
          {
            "id": 42,
            "title": "Extract ranking algorithms into `smart_retrieval/ranking_algorithms.py`",
            "description": "Analyze `src/backend/smart_retrieval.py` to identify functions/classes for scoring and ordering results and move them into `src/backend/smart_retrieval/ranking_algorithms.py`.",
            "dependencies": [
              41
            ],
            "details": "Update all internal and external references/imports to the newly created module. Run characterization tests (id 30) to ensure no behavior change and functionality remains identical.",
            "status": "pending",
            "testStrategy": "Run characterization tests for `smart_retrieval.py` (id 30) to ensure no regressions after extraction and import updates."
          },
          {
            "id": 43,
            "title": "Extract data source integration logic into `smart_retrieval/source_integrations.py`",
            "description": "Analyze `src/backend/smart_retrieval.py` to identify functions/classes for interfacing with data sources and move them into `src/backend/smart_retrieval/source_integrations.py`.",
            "dependencies": [
              42
            ],
            "details": "Update all internal and external references/imports to the newly created module. Consider applying 'Ports and Adapters' pattern where appropriate. Run characterization tests (id 30).",
            "status": "pending",
            "testStrategy": "Run characterization tests for `smart_retrieval.py` (id 30) to ensure no regressions after extraction and import updates."
          },
          {
            "id": 44,
            "title": "Extract result processing logic into `smart_retrieval/result_processors.py`",
            "description": "Analyze `src/backend/smart_retrieval.py` to identify functions/classes for post-processing retrieved items and move them into `src/backend/smart_retrieval/result_processors.py`.",
            "dependencies": [
              43
            ],
            "details": "Update all internal and external references/imports to the newly created module. Run characterization tests (id 30) to ensure no behavior change and functionality remains identical.",
            "status": "pending",
            "testStrategy": "Run characterization tests for `smart_retrieval.py` (id 30) to ensure no regressions after extraction and import updates."
          },
          {
            "id": 45,
            "title": "Define `smart_retrieval` public API via `__init__.py`",
            "description": "Refactor `src/backend/smart_retrieval/__init__.py` to explicitly define and expose only the necessary functions/classes from its sub-modules, controlling the package's public API.",
            "dependencies": [
              44
            ],
            "details": "Use `from .sub_module import func_name` patterns. Ensure consumers of the `smart_retrieval` package are not tightly coupled to internal module structures. Verify external calls are updated.",
            "status": "pending",
            "testStrategy": "Run characterization tests for `smart_retrieval.py` (id 30) to ensure no regressions; verify public API access from external modules remains functional and correct."
          },
          {
            "id": 46,
            "title": "Conduct duplication analysis across AI engine modules",
            "description": "Systematically analyze AI engine modules (e.g., `src/backend/ai/email_filter_node.py` and related components) to identify recurring code blocks, patterns, and boilerplate.",
            "dependencies": [
              33
            ],
            "details": "Use static analysis tools (e.g., pylint, prospector, dedicated duplication detectors) to identify concrete duplication occurrences and categorize them (e.g., data validation, pre-processing, error handling).",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 47,
            "title": "Establish `ai/utils.py` for AI-specific helper functions",
            "description": "Create the new utility module `src/backend/ai/utils.py` to house general AI-related helper functions specific to the AI domain, abstracting common logic.",
            "dependencies": [
              46
            ],
            "details": "This module will be the target for extracting duplicated code related to AI components, ensuring AI-specific helper functions are centralized.",
            "status": "pending",
            "testStrategy": "Verify file creation and initial module structure."
          },
          {
            "id": 48,
            "title": "Consolidate AI data validation into `ai/utils.py`",
            "description": "Identify and extract recurring AI-specific data validation logic (e.g., using Pydantic models for input validation in AI processing) from various AI modules into `src/backend/ai/utils.py`.",
            "dependencies": [
              47
            ],
            "details": "Replace duplicated validation blocks with calls to the new utility functions. Update imports as needed. Run relevant characterization tests to verify no change in validation behavior.",
            "status": "pending",
            "testStrategy": "Run characterization tests for affected AI modules (e.g., 33) to ensure validation behavior remains unchanged."
          },
          {
            "id": 49,
            "title": "Consolidate AI pre-processing into `ai/utils.py`",
            "description": "Identify and extract common pre-processing steps for text/email content (e.g., tokenization, normalization specific to AI models) from AI modules into `src/backend/ai/utils.py`.",
            "dependencies": [
              48
            ],
            "details": "Replace duplicated pre-processing blocks with calls to the new utility functions. Update imports as needed. Run relevant characterization tests to verify no change in pre-processing behavior.",
            "status": "pending",
            "testStrategy": "Run characterization tests for affected AI modules (e.g., 33) to ensure pre-processing behavior remains unchanged."
          },
          {
            "id": 50,
            "title": "Establish `utils/common.py` for broad common utilities",
            "description": "Create the new module `src/backend/utils/common.py` to house broader, cross-cutting utilities that are not specific to the AI domain but are general to the backend.",
            "dependencies": [
              46
            ],
            "details": "This module will be the target for extracting duplicated code common across different parts of the backend, like logging or general data manipulation. Verify file creation.",
            "status": "pending",
            "testStrategy": "Verify file creation and initial module structure."
          },
          {
            "id": 51,
            "title": "Consolidate error handling patterns into `utils/common.py`",
            "description": "Identify and extract standardized error handling patterns from various backend modules into `src/backend/utils/common.py`, promoting consistency and reducing duplication.",
            "dependencies": [
              50
            ],
            "details": "Replace duplicated error handling blocks with calls to new utility functions/classes. Update imports as needed. Run relevant characterization tests to ensure error handling behavior is consistent and correct.",
            "status": "pending",
            "testStrategy": "Run relevant characterization tests to ensure error handling behavior is consistent and correct."
          },
          {
            "id": 52,
            "title": "Consolidate logging boilerplate into `utils/common.py`",
            "description": "Identify and extract recurring logging boilerplate code (e.g., logger instantiation, common log message formatting) from various backend modules into `src/backend/utils/common.py`.",
            "dependencies": [
              51
            ],
            "details": "Replace duplicated logging setup with calls to new utility functions. Update imports as needed. Run relevant characterization tests to confirm logging behavior.",
            "status": "pending",
            "testStrategy": "Verify logs are generated correctly and consistently after refactoring; use characterization tests where applicable."
          },
          {
            "id": 53,
            "title": "Refactor `setup_dependencies` by extracting helper methods",
            "description": "Apply 'Extract Method' refactoring to `setup_dependencies` (in `src/backend/dependencies.py`) to reduce its cyclomatic complexity (from 21) by creating smaller, private helper functions.",
            "dependencies": [
              31
            ],
            "details": "Extract methods like `_initialize_database_connection()`, `_load_application_config()`, `_setup_logging()`, `_register_event_handlers()`. Ensure descriptive names and update calls within the main function.",
            "status": "pending",
            "testStrategy": "Run characterization tests for `setup_dependencies` (id 31) to ensure no regressions and functionality remains intact."
          },
          {
            "id": 54,
            "title": "Refactor `migrate_sqlite_to_json` by extracting helper methods",
            "description": "Apply 'Extract Method' refactoring to `migrate_sqlite_to_json` (in `src/backend/utils/database.py`) to reduce its cyclomatic complexity (from 17) by creating smaller, private helper functions.",
            "dependencies": [
              32
            ],
            "details": "Decompose into methods such as `_read_data_from_sqlite(db_path)`, `_transform_row_to_json_record(row)`, `_write_json_records_to_file(records, output_path)`. Ensure descriptive method names.",
            "status": "pending",
            "testStrategy": "Run characterization tests for `migrate_sqlite_to_json` (id 32) to ensure no regressions and the output remains identical."
          },
          {
            "id": 55,
            "title": "Refactor `email_filter_node.run` by extracting helper methods",
            "description": "Apply 'Extract Method' refactoring to the `run` method in `src/backend/ai/email_filter_node.py` to reduce its cyclomatic complexity (from 16) by creating smaller, private helper functions.",
            "dependencies": [
              33,
              49
            ],
            "details": "Decompose into methods such as `_ingest_email_messages()`, `_preprocess_message()`, `_apply_ai_filters()`, `_collate_results()`, `_store_or_dispatch_results()`. Ensure descriptive names for clarity.",
            "status": "pending",
            "testStrategy": "Run characterization tests for `email_filter_node.run` (id 33) to ensure no regressions and the end-to-end functionality remains intact."
          },
          {
            "id": 56,
            "title": "Conduct final review and quality check of refactored code",
            "description": "Perform a comprehensive code review of all refactored modules and functions, ensuring adherence to coding standards, refactoring best practices, and overall code quality.",
            "dependencies": [
              39,
              45,
              52,
              53,
              54,
              55
            ],
            "details": "Check for clean code principles, test coverage, adherence to SRP, DRY, high cohesion, low coupling, and ensure all characterization tests pass without regression. Utilize static analysis tools.",
            "status": "pending",
            "testStrategy": "Manual code review, static analysis tools (linter, cyclomatic complexity checker), and running all existing test suites to confirm no regressions."
          }
        ]
      },
      {
        "id": 5,
        "title": "Align Feature Branches with Scientific Branch",
        "description": "Systematically align multiple feature branches with the scientific branch to ensure code consistency, reduce future merge conflicts, and propagate the latest changes. This process will carefully preserve the key business logic and unique features of each feature branch, focusing updates primarily on architectural and common files from the scientific branch to minimize unnecessary conflicts, while ensuring feature-specific work is maintained.",
        "status": "pending",
        "dependencies": [
          "4"
        ],
        "priority": "high",
        "details": "This high-priority task involves a coordinated effort to bring all active feature branches up-to-date with the stable 'scientific' branch. The process will be executed in three main parts.\n\nPart 1: Feature Branch Assessment\n- Use `git branch --remote` and `git log` to identify all active branches diverging from 'scientific'.\n- Create a shared checklist (e.g., a repository markdown file) of branches to be aligned, including: feature/backlog-ac-updates, fix/import-error-corrections, docs-cleanup, feature/search-in-category, feature/merge-clean, feature/merge-setup-improvements.\n- Prioritize the branches based on importance and complexity of divergence.\n\nPart 2: Alignment Execution\n- For each branch, create a local backup first (e.g., `git branch <branch-name>-backup-pre-align`).\n- Check out the feature branch (`git checkout <feature-branch>`).\n- For public or shared branches, use a merge strategy to preserve history: `git merge scientific`.\n- Systematically resolve any merge conflicts, using visual merge tools for clarity. Document complex resolutions in the merge commit message.\n\nPart 3: Validation and Documentation\n- After each successful alignment, push the updated branch to the remote repository.\n- Create a Pull Request for each aligned branch to allow for code review and automated CI checks.\n- Update the central alignment checklist to reflect the status of each branch.\n<info added on 2025-11-21T14:11:59.973Z>\nResearch Session Findings: Granular Branch Alignment Tasks. A recent research session highlighted the importance of creating dedicated tasks for each feature branch alignment to ensure granular tracking, resource allocation, and comprehensive integration. While several branches already have dedicated subtasks for their alignment (e.g., 'feature/backlog-ac-updates', 'fix/import-error-corrections', 'feature/search-in-category'), the session specifically identified 'docs-cleanup' as a branch that would benefit from its own dedicated alignment task. Recommendation: To enhance project tracking and risk management, it is recommended to create a dedicated subtask for the alignment of the 'docs-cleanup' branch with 'scientific'. This would involve extracting 'docs-cleanup' from the existing 'Align Remaining Minor Branches' subtask (Subtask 6) and establishing it as a standalone subtask. The new subtask should include details focusing on integrating documentation updates, resolving formatting/structural conflicts, and verifying documentation build processes, with a test strategy involving review of changes and build verification. This approach aligns with the benefits of granular tasks, such as improved project tracking, clearer scope, and isolated risk management.\n</info added on 2025-11-21T14:11:59.973Z>\n<info added on 2025-11-21T14:14:56.135Z>\n<info added on 2025-11-22T01:14:33.000Z>\nFurther Research Session Findings:\n\nGranular Task Breakdown:\nReinforcing the importance of granular task tracking, it is recommended that dedicated subtasks be created for the alignment of all remaining grouped branches. Specifically, 'feature/merge-clean' and 'feature/merge-setup-improvements' (currently part of Subtask 6: 'Align Remaining Minor Branches') should be extracted and established as standalone subtasks. This aligns with the existing recommendation for 'docs-cleanup' and ensures consistent, detailed tracking for each branch alignment.\n\nCross-Feature Branch Dependency Management:\nTo proactively manage and verify dependencies between feature branches, the following actions are recommended:\n1.  **Update Task Scope (Part 1: Feature Branch Assessment)**: Enhance the 'Feature Branch Assessment' phase to explicitly include a dependency analysis step for each identified feature branch. This analysis should confirm independence from other *feature* branches or explicitly document any specific, intentional dependencies. The `ALIGNMENT_CHECKLIST.md` should be augmented with a 'Dependencies' column.\n2.  **Developer Guidelines**: Establish clear guidelines for developers to minimize unintended cross-feature branch dependencies. Any unavoidable dependencies must be communicated early and thoroughly documented.\n3.  **CI/CD Integration (Task 7)**: Leverage the CI/CD pipeline (as part of Task 7) to run integration tests for feature branches against the `scientific` branch *only*, without other unmerged feature branches. This will help expose unintended dependencies early.\n4.  **Regular Sync-ups**: Implement regular technical sync-ups to discuss active feature branches, potential overlaps, and planned integration orders, especially for branches with known or suspected dependencies.\n</info added on 2025-11-22T01:14:33.000Z>\n</info added on 2025-11-21T14:14:56.135Z>",
        "testStrategy": "Validation will focus on ensuring no regressions are introduced during the alignment process.\n1. Before beginning alignment on a branch, run its existing test suite to establish a baseline.\n2. After the merge/rebase is completed and all conflicts are resolved locally, run the full project test suite. All tests must pass.\n3. Perform a manual smoke test on the core functionality provided by the feature branch to ensure it has not been broken by the merge.\n4. Upon pushing the aligned branch and opening a PR, the continuous integration (CI) pipeline must pass all checks, including linting, unit tests, and integration tests. This CI pass is a mandatory success criterion for each branch.",
        "subtasks": [
          {
            "id": 1,
            "title": "Identify Divergent Branches and Create Alignment Plan",
            "description": "Analyze all remote feature branches to identify which have diverged from the 'scientific' branch. Document these branches in a new markdown file to serve as a master checklist for the alignment process.",
            "dependencies": [],
            "details": "Use 'git branch --remote' and 'git log' to compare each feature branch with 'scientific'. Create a new file named `ALIGNMENT_CHECKLIST.md` in the project root. List the branches to be aligned, including `feature/backlog-ac-updates`, `fix/import-error-corrections`, `docs-cleanup`, `feature/search-in-category`, `feature/merge-clean`, and `feature/merge-setup-improvements`. Add columns for status and notes.",
            "status": "pending",
            "testStrategy": "Verify the generated `ALIGNMENT_CHECKLIST.md` file is present in the repository and accurately lists all the feature branches mentioned in the task description."
          },
          {
            "id": 2,
            "title": "Create Local Backups of All Targeted Feature Branches",
            "description": "Before initiating any merge operations, create local backup copies of all feature branches listed in the alignment checklist. This provides a critical safety net and a simple rollback point.",
            "dependencies": [
              "5.1"
            ],
            "details": "For each branch identified in `ALIGNMENT_CHECKLIST.md`, execute the command `git branch <branch-name>-backup-pre-align`. For example, for 'feature/backlog-ac-updates', run `git branch feature/backlog-ac-updates-backup-pre-align`. Verify backup creation with `git branch`.",
            "status": "pending",
            "testStrategy": "Run `git branch | grep -- '-backup-pre-align'` locally to confirm that a backup has been created for every branch listed in the alignment checklist."
          },
          {
            "id": 3,
            "title": "Align 'feature/backlog-ac-updates' with 'scientific' Branch",
            "description": "Perform a merge of the 'scientific' branch into the 'feature/backlog-ac-updates' branch. This will propagate the latest stable changes into the feature branch.",
            "dependencies": [],
            "details": "First, checkout the feature branch using `git checkout feature/backlog-ac-updates`. Then, run `git merge scientific`. Systematically resolve any merge conflicts that arise using a visual merge tool and document complex resolutions in the merge commit message.",
            "status": "pending",
            "testStrategy": "Run the project's test suite on the branch before the merge to establish a baseline. After the merge and conflict resolution, run the full test suite again to ensure no regressions have been introduced."
          },
          {
            "id": 4,
            "title": "Align 'fix/import-error-corrections' with 'scientific' Branch",
            "description": "Perform a merge of the 'scientific' branch into the 'fix/import-error-corrections' branch to ensure the fix is compatible with the latest codebase.",
            "dependencies": [],
            "details": "Checkout the fix branch using `git checkout fix/import-error-corrections`. Execute `git merge scientific`. Carefully resolve any merge conflicts, paying special attention to file path and import statement changes given the nature of this branch.",
            "status": "pending",
            "testStrategy": "Before merging, run tests to confirm the fix works as intended. After merging, run the full test suite, especially integration tests that might fail due to incorrect module paths or circular dependencies."
          },
          {
            "id": 5,
            "title": "Align 'feature/search-in-category' with 'scientific' Branch",
            "description": "Merge the latest changes from the 'scientific' branch into the 'feature/search-in-category' branch, resolving any resulting conflicts.",
            "dependencies": [],
            "details": "Switch to the feature branch with `git checkout feature/search-in-category`. Run `git merge scientific` to start the alignment. Resolve all merge conflicts and ensure the new search functionality integrates correctly with the latest base code from 'scientific'.",
            "status": "pending",
            "testStrategy": "Run existing tests on the branch to get a baseline. After the merge, run the full test suite. Additionally, perform a manual smoke test of the search-in-category feature to confirm its behavior remains correct."
          },
          {
            "id": 6,
            "title": "Align Remaining Minor Branches with 'scientific'",
            "description": "Perform the alignment process for the remaining, lower-priority branches: 'docs-cleanup', 'feature/merge-clean', and 'feature/merge-setup-improvements'.",
            "dependencies": [],
            "details": "Sequentially process each branch. For each one, use `git checkout <branch-name>`, then `git merge scientific`. Resolve any conflicts that may arise. Given their scope, these branches are expected to have fewer conflicts.",
            "status": "pending",
            "testStrategy": "For 'docs-cleanup', a visual inspection of rendered documentation changes is sufficient. For 'feature/merge-clean' and 'feature/merge-setup-improvements', the full project test suite must be run post-merge to validate integrity."
          },
          {
            "id": 7,
            "title": "Push Aligned Branches and Create Pull Requests",
            "description": "Push all the locally merged and validated feature branches to the remote repository. Create a corresponding Pull Request for each aligned branch to enable code review and trigger CI/CD pipelines.",
            "dependencies": [
              "5.4",
              "5.5",
              "5.6"
            ],
            "details": "For each successfully aligned branch, run `git push origin <branch-name> --force-with-lease` if rebase was used, or `git push` if merge was used. Subsequently, navigate to the repository's web UI and create a Pull Request for each branch, targeting 'scientific'.",
            "status": "pending",
            "testStrategy": "Confirm that a new Pull Request exists for each aligned branch. Check that the PR's 'Files changed' tab correctly reflects the merge from 'scientific' and that CI checks have been automatically triggered."
          },
          {
            "id": 8,
            "title": "Final Validation, Review, and Checklist Completion",
            "description": "Monitor the CI pipelines for all created Pull Requests, address any feedback from code reviews, and update the master checklist to reflect the completion of each branch alignment.",
            "dependencies": [
              "5.1",
              "5.7"
            ],
            "details": "Review the CI build logs for each PR to ensure all tests pass. Address any code review comments by pushing new commits to the respective branches. After a PR is fully validated and approved, update its status to 'Completed' in the `ALIGNMENT_CHECKLIST.md` file.",
            "status": "pending",
            "testStrategy": "Final validation is successful when all created PRs have a 'green' build status, have been approved by at least one reviewer, and the `ALIGNMENT_CHECKLIST.md` file is updated to show all branches as 'Completed'."
          }
        ]
      },
      {
        "id": 6,
        "title": "Deep Integration & Refactoring: Align feature-notmuch-tagging-1 with Scientific, Enhance Data Platform, and Introduce Scalable AI Capabilities",
        "description": "Execute a comprehensive alignment of the `feature-notmuch-tagging-1` branch with the `scientific` branch, focusing on robust integration of the enhanced NotmuchDataSource, AI analysis, smart tagging, and full CRUD operations. This task not only involves updating architectural and common files from the `scientific` branch but also requires significant refactoring to ensure scalability, fault tolerance, and deep compatibility with existing data structures and the analytics ecosystem, while preserving and optimizing the feature branch's distinct functionalities. Special attention should be paid to data model evolution, performance under load, and comprehensive security hardening.",
        "status": "pending",
        "dependencies": [
          "4"
        ],
        "priority": "high",
        "details": "This task involves a multi-faceted integration, including architectural adjustments and new feature development:\n1.  **Strategic Branch Alignment & Conflict Resolution:** Initiate by rebasing `feature-notmuch-tagging-1` onto the latest `scientific` branch. Resolve all merge conflicts meticulously, especially in core data access and AI-related modules, ensuring `scientific`'s foundational stability while integrating feature-specific changes. Document all non-trivial conflict resolutions.\n2.  **Extended NotmuchDataSource Implementation & Data Model Evolution:** In `src/core/notmuch_data_source.py`, ensure the full DataSource interface is not only implemented (`delete_email`, `get_dashboard_aggregates`, `get_category_breakdown`) but also optimized for performance and large datasets. Review and propose necessary schema migrations or adaptations in the underlying data store to accommodate new Notmuch tagging and AI analysis metadata. Ensure `get_dashboard_aggregates` supports complex filtering and time-series aggregation efficiently.\n3.  **Scalable AI Analysis & Categorization Pipeline:** Integrate the AI-powered sentiment analysis and categorization logic as a highly scalable and fault-tolerant background process. This requires implementing a message queue (e.g., Kafka/RabbitMQ) for processing, robust retry mechanisms, and error queues for failed AI jobs. The AI model should support versioning and be configurable for different analysis types (e.g., multilingual). Consider model retraining and deployment strategies.\n4.  **Advanced SmartFilterManager Integration & Rule Engine Extension:** Beyond simple connection, extend `SmartFilterManager` to allow dynamic rule creation and persistence based on AI-derived tags and user-defined criteria. This may require a new domain-specific language (DSL) or configuration interface for defining and applying complex filtering rules that leverage Notmuch tags and AI insights.\n5.  **Hierarchical Tagging & Category Management System:** Implement comprehensive logic for tag-based category mapping, including support for hierarchical tags, tag synonyms, and user-defined categorization rules. This system should provide CRUD operations for tags and categories themselves, allowing for flexible taxonomy management.\n6.  **Distributed Observability & Performance Monitoring:** Integrate comprehensive error reporting, distributed tracing (e.g., OpenTelemetry), and advanced performance monitoring hooks for all new components. Define custom metrics for AI processing latency, accuracy, and background job throughput. Ensure all logs are structured and integrated with a centralized logging solution.\n7.  **End-to-End Security Hardening & Compliance:** Expand security enhancements beyond robust path validation to include input sanitization across all new data entry points, API authentication/authorization for any new internal/external endpoints, data encryption at rest and in transit for sensitive Notmuch data, and adherence to relevant data privacy regulations (e.g., GDPR, CCPA). Conduct a security review of the entire integration.\n8.  **Database Layer Adaptation & Optimization:** Update `src/core/database.py` with any necessary adjustments for compatibility with the enhanced data source, including new data types (e.g., JSONB for flexible metadata, vector embeddings for AI), optimized query patterns for the new aggregation methods, and ensuring transactional integrity for all tagging and AI-related operations. Implement database connection pooling and query caching where beneficial.\n9.  **API Exposure & Documentation:** If new capabilities are exposed via an API, ensure robust, versioned API endpoints are created and fully documented using OpenAPI/Swagger, including security considerations and usage examples.\n10. **Comprehensive Documentation:** Update all relevant design documents, architectural diagrams, and user guides to reflect the new NotmuchDataSource capabilities, AI integration, and tagging system.",
        "testStrategy": "A rigorous and multi-layered testing strategy is required to ensure the stability, correctness, performance, and security of this complex integration:\n1.  **Expanded Regression Test Suite:** Execute a full regression test suite across the entire application to ensure no existing functionality is broken. This must include performance baselining before and after integration to identify any regressions in response times or resource consumption.\n2.  **Detailed Unit & Integration Tests for NotmuchDataSource:** Develop comprehensive unit tests covering all methods of the `NotmuchDataSource`, including edge cases for each (`delete_email` with non-existent IDs, `get_dashboard_aggregates` with various date ranges and filters, `get_category_breakdown` with empty data). Integration tests should thoroughly verify the `NotmuchDataSource`'s interaction with the actual database and `SmartFilterManager`.\n3.  **Advanced AI/Background Process Validation:** Create specific integration tests to verify the end-to-end flow of the background AI analysis and tagging processes. This includes testing message queue interactions, error handling for failed AI jobs (e.g., malformed input, AI model errors), retry mechanisms, and the persistence of AI-derived metadata. Validate AI model accuracy with a diverse test dataset, tracking false positives and negatives.\n4.  **Smart Filtering & Tagging System Validation:** Develop comprehensive tests for the `SmartFilterManager` and the new hierarchical tagging system. This includes validating dynamic rule creation, application, and persistence, as well as complex category mapping scenarios (e.g., overlapping tags, nested categories, tag deprecation).\n5.  **Observability System Verification:** Confirm that new performance metrics, distributed traces, and structured error logs are correctly generated, captured, and accessible through the centralized monitoring system. Validate alert configurations for critical errors and performance thresholds.\n6.  **Comprehensive Security Testing:** Beyond functional security tests, conduct vulnerability scanning, penetration testing simulations (e.g., SQL injection, XSS for new inputs if applicable, broken access control for new endpoints), and data privacy compliance checks. Verify all input sanitization and authorization rules.\n7.  **Performance, Load, and Stress Testing:** Conduct extensive performance testing, including load tests (simulating expected peak usage), stress tests (exceeding peak usage to find breaking points), and soak tests (long-duration tests to detect memory leaks or resource exhaustion). Focus on the `NotmuchDataSource` aggregation methods and AI background processing under high concurrency.\n8.  **Data Migration & Schema Evolution Testing:** Rigorously test any proposed database schema changes or data migrations. This includes testing forward compatibility (new code with old schema) and backward compatibility (old code with new schema, if necessary for rollback plans). Ensure data integrity and consistency throughout the migration process.\n9.  **User Acceptance Testing (UAT):** Plan and execute UAT sessions with key business stakeholders to validate that the new smart tagging, AI analysis results, and filtering capabilities meet business requirements and provide an intuitive user experience.\n10. **Continuous Integration/Continuous Deployment (CI/CD) Integration:** Ensure all newly developed tests, including unit, integration, and security checks, are integrated into the CI/CD pipeline for automated execution on every code change.",
        "subtasks": [
          {
            "id": 1,
            "title": "Rebase feature branch and resolve core conflicts",
            "description": "Rebase `feature-notmuch-tagging-1` onto the latest `scientific` branch. Meticulously resolve all merge conflicts, especially in core data access and AI-related modules. This ensures foundational stability while integrating feature-specific changes.",
            "dependencies": [],
            "details": "Use `git rebase --onto scientific feature-notmuch-tagging-1` and carefully resolve conflicts in files such as `src/core/notmuch_data_source.py`, `ai_model_interface.py`, and any common utility or configuration files.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 2,
            "title": "Document non-trivial conflict resolutions",
            "description": "Document all non-trivial conflict resolutions encountered during the rebase process, outlining the specific files, changes made, and the rationale behind the chosen resolution strategy. This ensures maintainability and understanding.",
            "dependencies": [
              "6.1"
            ],
            "details": "Create a dedicated `docs/merge_resolutions.md` file or update existing design documents to clearly explain complex merges, including before/after snippets where necessary for clarity.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 3,
            "title": "Implement and optimize NotmuchDataSource interface methods",
            "description": "Fully implement and optimize the `delete_email`, `get_dashboard_aggregates`, and `get_category_breakdown` methods within `src/core/notmuch_data_source.py` to handle large datasets efficiently.",
            "dependencies": [],
            "details": "Focus on implementing efficient database queries and data processing logic to ensure optimal performance for high-volume data operations and complex aggregation requests. Performance benchmarks are required.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 4,
            "title": "Design and implement data model adaptations for AI metadata",
            "description": "Review and propose necessary schema migrations or adaptations in the underlying data store to robustly accommodate new Notmuch tagging and AI analysis metadata. This includes evolving existing data models.",
            "dependencies": [],
            "details": "Evaluate the use of JSONB fields for flexible metadata storage or creating new dedicated tables for specific AI-derived insights and tag hierarchies, ensuring backward compatibility where required.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 5,
            "title": "Optimize `get_dashboard_aggregates` for complex filtering",
            "description": "Optimize the `get_dashboard_aggregates` method to efficiently support complex filtering criteria and time-series aggregation, ensuring responsiveness under various query loads.",
            "dependencies": [
              "6.4"
            ],
            "details": "This involves leveraging database-specific features like indexing, materialized views, or pre-computed aggregates, and benchmarking performance under various filter combinations and time ranges.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 6,
            "title": "Design and set up message queue for AI processing",
            "description": "Design and set up a robust, scalable message queue system, such as Kafka or RabbitMQ, for asynchronous processing of AI analysis and categorization tasks in a fault-tolerant manner.",
            "dependencies": [],
            "details": "Configure topics/queues, producers, consumers, and ensure message durability, ordering guarantees, and high availability for the AI processing pipeline. Consider auto-scaling groups.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 7,
            "title": "Implement robust AI job retry and error handling mechanisms",
            "description": "Develop and implement robust retry mechanisms for transient failures in AI processing jobs and establish dedicated error queues for unrecoverable or dead-letter messages.",
            "dependencies": [
              "6.6"
            ],
            "details": "Implement exponential backoff strategies for retries, configure dead-letter queues, and integrate failure reporting into the observability stack for quick issue detection and resolution.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 8,
            "title": "Integrate AI model with pipeline and define deployment strategy",
            "description": "Integrate the AI model into the new message queue-driven pipeline, ensuring comprehensive support for model versioning, flexible configuration for different analysis types, and defining clear model retraining and deployment strategies.",
            "dependencies": [
              "6.7"
            ],
            "details": "Create a dedicated microservice or component for model inference, manage model artifacts securely, and establish CI/CD pipelines for automated model updates and A/B testing capabilities.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 9,
            "title": "Design DSL/configuration for dynamic filtering rules",
            "description": "Design a flexible domain-specific language (DSL) or a robust, user-friendly configuration interface for dynamically creating and persisting complex filtering rules that leverage Notmuch tags and AI insights.",
            "dependencies": [
              "6.4"
            ],
            "details": "Define the grammar and syntax for the DSL, or design a schema for a configuration-based rule engine, ensuring it is intuitive for defining advanced filtering logic and extensible for future needs.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 10,
            "title": "Implement dynamic rule creation and persistence",
            "description": "Implement the core logic within `SmartFilterManager` to allow dynamic rule creation, persistence, and efficient application based on the designed DSL/configuration.",
            "dependencies": [
              "6.9"
            ],
            "details": "Develop a rule parsing and evaluation engine, implement database storage for rules, and ensure rules can be dynamically applied to incoming and existing data, leveraging Notmuch tags and AI-derived categories effectively.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 11,
            "title": "Implement hierarchical tagging and category mapping logic",
            "description": "Develop and implement comprehensive logic to support hierarchical tags, tag synonyms, and user-defined categorization rules within the Notmuch tagging system.",
            "dependencies": [
              "6.4",
              "6.5"
            ],
            "details": "Design the data structures for representing tag relationships (parent-child, synonyms), implement algorithms for tag normalization and resolution, and expose APIs for managing these relationships effectively.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 12,
            "title": "Implement CRUD operations for tags and categories",
            "description": "Implement full CRUD (Create, Read, Update, Delete) operations for managing tags and categories within the system, enabling flexible and user-driven taxonomy management.",
            "dependencies": [
              "6.11"
            ],
            "details": "Design and implement API endpoints and database interactions for adding, retrieving, modifying, and deleting tags and categories, ensuring referential integrity and permission checks are robust.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 13,
            "title": "Integrate OpenTelemetry for distributed tracing",
            "description": "Integrate OpenTelemetry for end-to-end distributed tracing across all new components, especially the AI pipeline, NotmuchDataSource, and SmartFilterManager, to provide deep visibility into system behavior.",
            "dependencies": [
              "6.8",
              "6.10"
            ],
            "details": "Instrument service calls, database queries, message queue interactions, and internal processing steps to capture and correlate trace data for performance and error analysis in production.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 14,
            "title": "Implement structured logging and centralized integration",
            "description": "Ensure all newly developed components emit structured logs in a consistent format and integrate them seamlessly with a centralized logging solution for efficient aggregation and analysis.",
            "dependencies": [],
            "details": "Configure logging libraries to output JSON-formatted logs, set up log shippers (e.g., Filebeat, Fluentd), and integrate with a central log management system (e.g., ELK stack, Grafana Loki, Splunk).",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 15,
            "title": "Define and implement custom performance metrics",
            "description": "Define and implement custom performance metrics for key aspects like AI processing latency, model accuracy, background job throughput, and SmartFilterManager rule evaluation time.",
            "dependencies": [
              "6.14"
            ],
            "details": "Expose these metrics via a Prometheus exporter or similar mechanism, design comprehensive dashboards in Grafana or other monitoring tools, and set up alerts for critical thresholds and anomalies.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 16,
            "title": "Implement input sanitization and API authentication/authorization",
            "description": "Implement robust input sanitization across all new data entry points and enforce strong API authentication and authorization mechanisms for any new internal or external endpoints.",
            "dependencies": [
              "6.10"
            ],
            "details": "Utilize libraries for input validation and sanitization (e.g., Marshmallow, Pydantic), implement OAuth2 or JWT-based authentication for APIs, and role-based access control (RBAC) for authorization checks.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 17,
            "title": "Design and implement data encryption for Notmuch data",
            "description": "Design and implement comprehensive data encryption strategies, both at rest and in transit, for sensitive Notmuch data and AI-derived metadata, ensuring compliance with data privacy regulations.",
            "dependencies": [
              "6.4",
              "6.16"
            ],
            "details": "Implement TLS/SSL for data in transit, utilize disk encryption or application-level encryption for data at rest, and manage encryption keys securely using a KMS (Key Management System).",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 18,
            "title": "Adapt database layer for new data types and optimize queries",
            "description": "Adapt `src/core/database.py` to support new data types, including JSONB for flexible metadata and vector embeddings for AI, and optimize query patterns for new aggregation methods.",
            "dependencies": [
              "6.4",
              "6.5"
            ],
            "details": "Integrate database connection pooling and query caching where beneficial to enhance performance and ensure transactional integrity for all new tagging and AI-related operations, handling large datasets.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 19,
            "title": "Design, implement, and document new API endpoints",
            "description": "Design and implement robust, versioned API endpoints to expose the new NotmuchDataSource capabilities, AI insights, and tagging system features. Thoroughly document these endpoints using OpenAPI/Swagger.",
            "dependencies": [
              "6.10",
              "6.17"
            ],
            "details": "Define clear API contracts, incorporate security considerations, provide detailed usage examples, and ensure proper versioning for future extensibility and maintainability. Conduct API testing.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 20,
            "title": "Update comprehensive system documentation",
            "description": "Conduct a comprehensive update of all relevant design documents, architectural diagrams, and user guides to accurately reflect the newly integrated NotmuchDataSource capabilities, AI integration, and tagging system.",
            "dependencies": [
              "6.19"
            ],
            "details": "Update system architecture diagrams, data flow diagrams, API specifications, and create or revise user-facing documentation to explain new features and functionalities, ensuring clarity and completeness.",
            "status": "pending",
            "testStrategy": null
          }
        ]
      },
      {
        "id": 7,
        "title": "Create Comprehensive Merge Validation Framework",
        "description": "Create a comprehensive validation framework to ensure all architectural updates have been properly implemented before merging scientific branch to main. This framework will leverage research-backed CI/CD practices to validate consistency, functionality, performance, and security across all components, specifically tailored for our Python/FastAPI application. This task is directly linked to the backlog item: `backlog/tasks/alignment/create-merge-validation-framework.md`.",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "details": "This framework will integrate into the GitHub Actions CI/CD pipeline, triggered on pull requests to the `main` branch from `scientific`. It will encompass several layers of automated checks, including:\n1.  **Architectural Enforcement**: Static analysis to ensure `src/backend` adheres to defined module boundaries and import rules.\n2.  **Functional Correctness**: Execution of full unit/integration test suites for `src/backend` and end-to-end smoke tests against a deployed instance of the FastAPI application.\n3.  **Performance Benchmarking**: Automated checks for performance regressions on critical FastAPI endpoints.\n4.  **Security Validation**: Dependency vulnerability scanning and Static Application Security Testing (SAST) for the Python/FastAPI codebase.\nThe ultimate goal is to automatically block merges if any of these validation layers fail, ensuring a robust and secure `main` branch. The framework will be configured to analyze the `src/backend` directory primarily.",
        "testStrategy": "The overall test strategy involves validating each component of the framework individually, then ensuring their integrated operation correctly blocks or permits merges based on the validation outcomes. This will include creating test PRs with deliberate failures (architectural, functional, performance, security) and successes to verify the framework's decision-making.",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Validation Scope and Acceptance Criteria",
            "description": "Document the specific architectural rules, functional requirements, performance benchmarks, and security requirements that will be validated by the framework before any code is written.",
            "dependencies": [],
            "details": "Create a markdown document (e.g., `docs/ci-cd/VALIDATION_CRITERIA.md`) in the repository. It must list: 1) Architectural rules like module boundaries and import restrictions for the `src/backend` structure. 2) Key functional user flows for smoke testing of the FastAPI application. 3) Performance baseline metrics (e.g., API response times, memory usage) for critical FastAPI endpoints. 4) Security requirements and vulnerability thresholds (e.g., OWASP Top 10, dependency vulnerability scanning thresholds). This document will serve as the single source of truth for the validation framework's goals.",
            "status": "pending",
            "testStrategy": "Review the generated document with the lead architect and senior developers to ensure consensus, completeness, and feasibility of the defined criteria."
          },
          {
            "id": 2,
            "title": "Configure CI Pipeline Trigger for 'scientific' Branch Pull Requests",
            "description": "Set up a dedicated CI/CD workflow that triggers automatically when a pull request is opened from the 'scientific' branch to 'main'.",
            "dependencies": [
              "7.1"
            ],
            "details": "Create a new GitHub Actions workflow file (`.github/workflows/merge_validation.yml`). Configure it to trigger on pull requests targeting the 'main' branch. This initial version will only define the trigger and placeholder jobs for subsequent validation steps, ensuring it correctly checks out the PR code from the `src/backend` context.",
            "status": "pending",
            "testStrategy": "Create a test pull request from a temporary branch to 'main' and verify that the new CI pipeline is triggered automatically and completes its initial placeholder steps successfully."
          },
          {
            "id": 3,
            "title": "Implement Architectural Consistency Checks",
            "description": "Integrate static analysis tools into the CI pipeline to automatically enforce the architectural rules defined in the scope for the Python/FastAPI application.",
            "dependencies": [],
            "details": "Utilize a tool like `Pylint` (for module import rules), `Adept` (for dependency graph analysis), or a custom Python script leveraging `ast` module parsing to enforce architectural rules defined in `docs/ci-cd/VALIDATION_CRITERIA.md`. This will check module boundaries and import restrictions specific to the `src/backend` structure. Add a new step in the `.github/workflows/merge_validation.yml` CI workflow that runs this analysis. The step must fail the build if any architectural violations are detected, such as a feature module importing from another feature module within `src/backend`.",
            "status": "pending",
            "testStrategy": "Create a test branch with a deliberate architectural violation (e.g., an illegal import statement within `src/backend`). Open a PR and verify that the CI pipeline fails at the static analysis step with a clear, actionable error message highlighting the Python module violation."
          },
          {
            "id": 4,
            "title": "Integrate Full Regression Test Suite Execution",
            "description": "Add a job to the validation pipeline that executes the complete backend test suite (unit and integration tests) to ensure no existing functionality is broken in the FastAPI application.",
            "dependencies": [],
            "details": "Add a new job to the `.github/workflows/merge_validation.yml` file. This job will install all project dependencies (e.g., from `requirements.txt` in the root or `src/backend/requirements.txt`) and then execute the full test suite using `pytest` from the `src/backend` directory. The job must be configured to fail the entire pipeline run if any single test fails. Ensure the test environment is correctly configured with all necessary services (e.g., database, cache) required for the FastAPI application.",
            "status": "pending",
            "testStrategy": "Manually introduce a failing test in a new PR within `src/backend` to ensure the pipeline correctly identifies the failure, reports it, and blocks the merge. Verify that the logs show the complete test suite was executed."
          },
          {
            "id": 5,
            "title": "Develop and Integrate E2E Smoke Tests",
            "description": "Create and run a suite of automated end-to-end (E2E) smoke tests against a live instance of the FastAPI application to verify critical user flows.",
            "dependencies": [
              "7.4"
            ],
            "details": "Using `pytest` with `httpx` (or `requests`), write tests that simulate core FastAPI application workflows (e.g., user authentication, data submission, primary resource retrieval) against the `src/backend` service. The CI pipeline will need a step to build a Docker container for the PR's code (e.g., from `src/backend/Dockerfile`), run it, expose the FastAPI application, and then execute these smoke tests against the running container's API.",
            "status": "pending",
            "testStrategy": "Run the smoke tests locally against a developer environment to validate the test logic. Then, trigger the CI pipeline and confirm it correctly builds the container, runs the application, and executes the smoke tests, failing the build if a test fails."
          },
          {
            "id": 6,
            "title": "Implement Automated Performance Benchmark Testing",
            "description": "Create and integrate performance tests for critical FastAPI API endpoints to automatically detect and flag performance regressions.",
            "dependencies": [
              "7.5"
            ],
            "details": "Use a tool like `locust` or `pytest-benchmark` within `pytest` to measure key performance indicators (KPIs) like p95 response time for 5-10 critical FastAPI endpoints from `src/backend`. Establish a baseline by running these tests against the 'main' branch. The CI script will compare the PR's performance against this baseline and fail if it exceeds a defined threshold (e.g., 15% degradation). The performance test environment should closely mimic production conditions for the FastAPI service.",
            "status": "pending",
            "testStrategy": "Establish the performance baseline. Then, create a PR with an intentionally inefficient code change (e.g., adding a `time.sleep()` in a hot path within `src/backend`) and verify that the performance test detects the regression, reports the deviation, and fails the pipeline."
          },
          {
            "id": 7,
            "title": "Implement Automated Security Vulnerability Scanning",
            "description": "Integrate automated security scanning tools into the CI pipeline to identify known vulnerabilities in dependencies and potential security weaknesses in the Python/FastAPI application code.",
            "dependencies": [],
            "details": "Add a new job to the `.github/workflows/merge_validation.yml` file. This job will perform two types of scans: 1) Dependency vulnerability scanning using tools like `safety` or `pip-audit` against `requirements.txt` (or similar dependency files in `src/backend`). 2) Static Application Security Testing (SAST) using tools like `Bandit` or `Pylint` with security checks configured, targeting the Python code in `src/backend`. Configure the job to fail the pipeline if critical or high-severity vulnerabilities are found, or if SAST rules are violated.",
            "status": "pending",
            "testStrategy": "Introduce a known vulnerable dependency (e.g., an old version of a common library) or a simple security flaw (e.g., hardcoded credentials) into a test branch in `src/backend`. Verify the CI pipeline correctly identifies and flags these issues, failing the build as appropriate and providing clear output."
          },
          {
            "id": 8,
            "title": "Generate and Publish a Unified Validation Report",
            "description": "Configure the CI pipeline to aggregate results from all validation steps into a single report and publish it as a comment on the pull request.",
            "dependencies": [
              "7.4",
              "7.6"
            ],
            "details": "Use a GitHub Action (like 'actions/github-script') in a final pipeline job to generate a markdown summary of the results. The report should include the status of architectural checks, test suite results (e.g., '1573/1573 passed'), performance benchmark deltas, and security scan findings. This job will post the markdown summary as a comment on the associated PR.",
            "status": "pending",
            "testStrategy": "Trigger the pipeline on a test PR and check that a comment is successfully posted. Verify the comment's content accurately reflects the success or failure of the preceding steps. Check both a fully successful run and a run with a failure."
          },
          {
            "id": 9,
            "title": "Enforce Merge Blocking via Branch Protection Rules",
            "description": "Configure repository branch protection rules to make the merge validation pipeline a required check, preventing merges to 'main' if any validation fails.",
            "dependencies": [
              "7.7"
            ],
            "details": "In the GitHub repository settings, navigate to 'Branches' and edit the protection rule for the 'main' branch. Under 'Require status checks to pass before merging', add the merge validation CI job. This will physically disable the merge button on pull requests until the check passes.",
            "status": "pending",
            "testStrategy": "Open a new PR from 'scientific' to 'main' with a change that causes the validation pipeline to fail. Verify that the GitHub UI blocks the merge. Then, push a fix, wait for the pipeline to pass, and confirm the merge button becomes active again."
          }
        ]
      },
      {
        "id": 8,
        "title": "Update Setup Subtree Integration in Scientific Branch",
        "description": "Update the scientific branch to use the new setup subtree methodology that has been implemented in the main branch. This will align the architecture for easier merging and future maintenance, as detailed in the original backlog task: backlog/tasks/alignment/update-setup-subtree-integration.md.",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "details": "",
        "testStrategy": "",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze 'main' branch for setup subtree methodology",
            "description": "Investigate the 'main' branch's Git history and current filesystem to understand the precise implementation details of the 'new setup subtree methodology'. This includes identifying the subtree's remote URL, its local path within the 'main' branch, and the specific `git subtree` commands used for its integration and updates. Pay close attention to any associated scripts or configuration files.",
            "dependencies": [],
            "details": "Use `git log --grep='git subtree'` on the 'main' branch to find relevant commit messages. Examine the `.git/config` file in 'main' for subtree-related entries. Identify the exact path where the subtree is mounted (e.g., 'src/setup' or 'shared/config') and the remote repository it points to. Check for any automation scripts in `scripts/` or `tools/` that manage the subtree.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 2,
            "title": "Identify and isolate target directory in 'scientific' branch",
            "description": "Pinpoint the specific directory or set of files within the 'scientific' branch that corresponds to the 'setup' functionality now managed by the new subtree methodology in 'main'. This content will be replaced or integrated with the subtree.",
            "dependencies": [
              "8.1"
            ],
            "details": "Based on the findings from subtask 1 (e.g., if the main branch uses 'src/setup' for the subtree), identify the equivalent directory in the 'scientific' branch. Verify its content aligns with what is expected from the 'setup' component. Create a temporary `temp_backup_scientific_setup` directory.",
            "status": "pending",
            "testStrategy": "Verify the identified directory path and its contents against the expected setup component. Confirm this is the correct target for subtree replacement."
          },
          {
            "id": 3,
            "title": "Backup and remove existing setup code in 'scientific' branch",
            "description": "Before integrating the new setup subtree, create a complete backup of the existing 'setup' related code found in the 'scientific' branch. After backup, safely remove this existing code to ensure a clean slate for the subtree integration and prevent potential conflicts.",
            "dependencies": [],
            "details": "Copy the identified 'setup' directory (e.g., 'src/setup') from the 'scientific' branch to a temporary, version-controlled backup location (e.g., a new branch or a dedicated commit). Then, use `git rm -r <path_to_setup_directory>` to remove the existing directory from the 'scientific' branch's working tree and stage the deletion. Commit these changes.",
            "status": "pending",
            "testStrategy": "After removal, run `git status` to confirm the directory is gone. Attempt to build the project (if applicable) to ensure no immediate, critical build failures due to missing files (expect failures, but ensure it's clean)."
          },
          {
            "id": 4,
            "title": "Integrate the new setup subtree into 'scientific' branch",
            "description": "Add the 'setup' component from its designated remote repository as a `git subtree` into the 'scientific' branch, following the methodology identified in the 'main' branch. This step establishes the official link and pulls in the latest 'setup' code.",
            "dependencies": [],
            "details": "Using the remote URL and path identified in subtask 1, execute the `git subtree add --prefix=<path_in_scientific> <remote_url> <branch_name> --squash` command. For example, `git subtree add --prefix=src/setup https://github.com/org/setup-repo.git main --squash`. Commit the changes after successful addition. If the 'main' branch uses `git subtree pull` from an existing remote, ensure that remote is added first.",
            "status": "pending",
            "testStrategy": "Verify the new directory structure for the subtree appears at the correct path. Check `git log` to confirm the subtree addition commit. Attempt a basic project build to ensure the newly added files are recognized."
          },
          {
            "id": 5,
            "title": "Reconcile 'scientific' branch-specific modifications",
            "description": "Carefully identify and re-apply any unique modifications, configurations, or extensions that were present in the 'scientific' branch's original 'setup' code (backed up in subtask 3) but are not part of the newly integrated subtree. This ensures feature parity for the 'scientific' branch.",
            "dependencies": [
              "8.4"
            ],
            "details": "Using the backup created in subtask 3, perform a diff between the backup and the newly integrated subtree content. Manually merge or re-apply any custom logic, configuration changes, or additional files that are specific to the 'scientific' branch's requirements. Resolve any merge conflicts that arise during this process. Commit these reconciliation changes.",
            "status": "pending",
            "testStrategy": "Perform detailed code review of re-applied changes. Run unit tests specific to the 'setup' component, if available, to ensure custom logic behaves as expected. Manually verify configurations are correct."
          },
          {
            "id": 6,
            "title": "Verify and test setup subtree functionality",
            "description": "Thoroughly test the newly integrated setup subtree to ensure all functionalities related to the 'setup' component work correctly within the 'scientific' branch. This includes validating configurations, dependencies, and any interactions with other parts of the system.",
            "dependencies": [
              "8.5"
            ],
            "details": "Execute the full suite of unit, integration, and end-to-end tests related to the 'setup' component. If the 'setup' component is responsible for environment configuration or build processes, run full build processes and test environment startup. Pay special attention to areas where 'scientific' branch had specific modifications. Ensure proper loading of configurations and dependencies.",
            "status": "pending",
            "testStrategy": "Run all existing test suites. Develop new integration tests for critical 'setup' functionalities if existing coverage is insufficient. Deploy to a staging environment for full system integration testing to ensure no regressions."
          }
        ]
      },
      {
        "id": 9,
        "title": "Align import-error-corrections Branch with Main Branch",
        "description": "Systematically align feature branch `fix/import-error-corrections` with the main branch. This alignment will carefully preserve the key business logic and unique features of the `fix/import-error-corrections` branch, focusing updates primarily on architectural and common files from the main branch to minimize unnecessary conflicts, while ensuring feature-specific work is maintained. This task is linked to the original backlog item: `backlog/tasks/alignment/task-feature-branch-alignment-import-error-corrections-main.md`.",
        "status": "pending",
        "dependencies": [],
        "priority": "low",
        "details": "",
        "testStrategy": "",
        "subtasks": [
          {
            "id": 1,
            "title": "Ensure clean working directory and checkout feature branch",
            "description": "Before starting the alignment process, ensure there are no uncommitted changes in the local repository and checkout the target feature branch `fix/import-error-corrections`.",
            "dependencies": [],
            "details": "Use `git status` to confirm a clean working directory. If necessary, stash or commit any local changes. Then, execute `git checkout fix/import-error-corrections`.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 2,
            "title": "Update local 'main' branch to latest remote",
            "description": "Synchronize the local 'main' branch with the upstream 'main' branch to ensure the feature branch will be rebased against the most recent changes.",
            "dependencies": [
              "9.1"
            ],
            "details": "First, switch to the main branch (`git checkout main`), then fetch and pull the latest changes from the remote (`git pull origin main`).",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 3,
            "title": "Run baseline tests on 'fix/import-error-corrections'",
            "description": "Execute the existing test suite on the `fix/import-error-corrections` branch to establish a functional baseline before applying changes from 'main'.",
            "dependencies": [],
            "details": "Switch back to the feature branch (`git checkout fix/import-error-corrections`). Run all relevant unit, integration, and end-to-end tests for the project. Document any existing failures or unexpected behaviors.",
            "status": "pending",
            "testStrategy": "Capture test results (e.g., screenshots of test runner output, log files) to compare against post-rebase results."
          },
          {
            "id": 4,
            "title": "Rebase 'fix/import-error-corrections' onto 'main'",
            "description": "Perform the Git rebase operation to integrate the `main` branch's history into `fix/import-error-corrections`, ensuring a linear commit history.",
            "dependencies": [],
            "details": "While on the `fix/import-error-corrections` branch, execute `git rebase main`. This will apply the commits from the feature branch on top of the latest 'main' commit.",
            "status": "pending",
            "testStrategy": null
          },
          {
            "id": 5,
            "title": "Resolve rebase conflicts",
            "description": "Address and resolve any merge conflicts that arise during the rebase process to ensure code integrity and functionality.",
            "dependencies": [
              "9.4"
            ],
            "details": "Upon encountering conflicts, use `git status` to identify conflicting files. Edit these files to resolve conflicts, then stage them (`git add <file>`) and continue the rebase (`git rebase --continue`). Repeat until the rebase is complete.",
            "status": "pending",
            "testStrategy": "Thoroughly review each conflict resolution to ensure correct code merging and no introduction of new bugs."
          },
          {
            "id": 6,
            "title": "Run full test suite post-rebase",
            "description": "After successfully rebasing and resolving conflicts, run the entire project's test suite to verify that no regressions or new issues were introduced.",
            "dependencies": [
              "9.5"
            ],
            "details": "Execute all unit, integration, and end-to-end tests on the rebased `fix/import-error-corrections` branch. Compare results with the baseline tests from subtask 3.",
            "status": "pending",
            "testStrategy": "All tests must pass. If any tests fail, debug and fix the issues introduced by the rebase, then re-run tests. Document any new issues."
          },
          {
            "id": 7,
            "title": "Perform local functional validation and code review",
            "description": "Conduct a manual review of the aligned code and perform functional tests specific to the `import-error-corrections` feature to ensure it still works as expected.",
            "dependencies": [
              "9.6"
            ],
            "details": "Manually test the `import-error-corrections` functionality. Review the changes introduced from 'main' to ensure they integrate logically with the feature branch's code. Pay attention to any areas that had conflicts.",
            "status": "pending",
            "testStrategy": "Verify the core functionality addressed by `fix/import-error-corrections` still works correctly. Check for unexpected side effects from the `main` branch's changes."
          },
          {
            "id": 8,
            "title": "Force push rebased branch and notify team",
            "description": "Once validation is complete, force push the rebased `fix/import-error-corrections` branch to the remote repository and inform relevant team members.",
            "dependencies": [
              "9.7"
            ],
            "details": "Execute `git push --force-with-lease origin fix/import-error-corrections`. Notify the development team, especially those who might be working on or depending on this branch, about the successful rebase and updated remote branch.",
            "status": "pending",
            "testStrategy": null
          }
        ]
      },
      {
        "id": 10,
        "title": "Improve Task Management with Advanced Testing Integration",
        "description": "Enhance Task Master AI with automated testing validation, branch-specific task tracking, and integration with scientific branch's advanced testing frameworks. This will ensure tasks include proper testing requirements and validation before completion.",
        "status": "pending",
        "dependencies": [],
        "priority": "medium",
        "details": "This task focuses on integrating robust testing validation and tracking into the Task Master AI system. The implementation will leverage the existing CI/CD infrastructure (`.github/workflows/main.yml`) which already uses `pytest` for testing and Codecov for coverage reporting across `src/backend/tests` and `src/data_platform/tests`. Specific attention will be given to the unique testing requirements of the `scientific` branch, as indicated by the `scientific_branch_specific_checks` job in the workflow. The goal is to enforce testing requirements at task creation and automate validation checks during the task completion workflow, ensuring high code quality and reliability.",
        "testStrategy": "Testing will involve a multi-faceted approach. Modifications to the CI/CD workflow (`.github/workflows/main.yml`) will be validated through creation of draft pull requests and observation of workflow runs for various branch types (e.g., `main`, `scientific`, `feature/`, `feature/scientific-`). Unit tests will be developed for any new backend services or logic interacting with the task management system, located within `src/backend/tests`. Integration tests will ensure seamless communication between the CI/CD pipeline and the Task Master system, verifying correct status updates and enforcement of validation rules. Specific test cases will target the unique validation needs of the `scientific` branch.",
        "subtasks": [
          {
            "id": 2,
            "title": "Add branch-specific task tracking and validation",
            "description": "Extend the task management system to associate tasks with specific Git branches (e.g., `feature/task-10-validation`) and enforce validation rules based on the branch type. This will leverage the existing branch-aware logic in `.github/workflows/main.yml`.",
            "dependencies": [],
            "details": "Update the Task Master system to store and display the associated Git branch or pull request for each task. Enhance the `.github/workflows/main.yml` to utilize `github.head_ref` or `github.ref_name` to identify the current branch. Implement conditional steps within the CI/CD (similar to the `scientific_branch_specific_checks` job) to apply different validation criteria or invoke specialized tests based on branch naming conventions (e.g., `feature/scientific-*` branches may trigger additional checks). Ensure Task Master correctly interprets and reacts to these branch-specific validation outcomes received from the CI/CD.",
            "status": "pending",
            "testStrategy": "Create multiple test branches with different naming conventions (e.g., `feature/standard-task-A`, `feature/scientific-task-B`). Create pull requests for these branches and verify that the CI/CD pipeline applies the correct branch-specific validation rules. Confirm Task Master accurately reflects the validation status for each task based on its associated branch type."
          },
          {
            "id": 3,
            "title": "Incorporate scientific branch testing frameworks into Task Master",
            "description": "Integrate the advanced testing frameworks and specific test suites used in the `scientific` branch into Task Master's validation process. This directly builds upon the `scientific_branch_specific_checks` job in `.github/workflows/main.yml`.",
            "dependencies": [],
            "details": "Enhance the `scientific_branch_specific_checks` job in `.github/workflows/main.yml`. Currently, this job explicitly runs `pytest tests/data_sources/test_notmuch_data_source.py` within `scientific_branch/src/data_platform`. Expand this job to include any other 'additional scientific-specific testing frameworks' or more comprehensive `pytest` suites located in `src/data_platform/tests` that are critical for `scientific` branch validation. Ensure that the results of these specialized scientific tests are captured and reported back to Task Master, potentially using the same mechanism developed for general automated testing validation. Configure Task Master to distinctly recognize and display these 'scientific branch' test outcomes as a required validation stage for relevant tasks.",
            "status": "pending",
            "testStrategy": "Develop a feature branch with a `feature/scientific-` prefix. Introduce a small, new scientific-specific test file in `src/data_platform/tests` or modify an existing `scientific` test. Create a pull request and verify that the `scientific_branch_specific_checks` job executes the scientific tests, and their results are accurately reported back to Task Master."
          },
          {
            "id": 4,
            "title": "Implement testing requirement enforcement in task creation",
            "description": "Modify the task creation process within Task Master to enforce the definition of testing requirements (e.g., type of tests, coverage goals) from the outset.",
            "dependencies": [],
            "details": "Update the data model or schema for tasks within the Task Master system to include mandatory fields such as `required_test_types` (e.g., 'unit', 'integration', 'scientific-specific') and `minimum_coverage_percentage`. If Task Master has a user interface or API for creating tasks, modify these interfaces to enforce the input of these testing requirements. Provide updated documentation for developers on how to specify these requirements when creating new tasks. Consider optional integration of pre-commit hooks or static analysis tools to encourage adherence to these testing requirements during code development, possibly referencing existing test directories like `src/backend/tests` and `src/data_platform/tests`.",
            "status": "pending",
            "testStrategy": "Attempt to create tasks through the Task Master API/UI without specifying the new mandatory testing requirement fields, and verify that the creation fails with an appropriate error. Successfully create tasks with valid testing requirements specified, and confirm these requirements are correctly stored and displayed. Develop and run unit tests for the task creation logic."
          }
        ]
      },
      {
        "id": 11,
        "title": "Project Management: Oversee Backend Migration to src/",
        "description": "This task involves the comprehensive oversight and tracking of the `backend` package's complete migration to the new `src/backend` modular architecture, ensuring successful transition of all core components including database management, AI engine, workflow systems, and API routes.",
        "details": "1.  **Define Migration Scope & Success Criteria**: Collaborate with the team performing Task 2 to explicitly define the \"complete\" migration. This includes identifying all sub-modules (e.g., `src/backend/db`, `src/backend/ai`, `src/backend/workflows`, `src/backend/api`, `src/backend/services`) within the legacy `backend` directory that need to be moved and refactored. Document clear success criteria for each component's transition.\n2.  **Progress Monitoring & Reporting**: Establish a schedule for regular check-ins (e.g., daily stand-ups, weekly syncs) with the team working on Task 2. Monitor progress against the defined sub-tasks of Task 2. Track metrics such as file migration completion, import statement updates, and resolution of breaking changes. Report status to stakeholders, highlighting any blockers or deviations from the plan.\n3.  **Coordination with Related Tasks**:\n    *   **Task 2 (Execution of Migration)**: Actively monitor the execution of Task 2, ensuring all defined subtasks are progressing as planned.\n    *   **Task 3 (Enhanced Security)** & **Task 4 (Refactor High-Complexity Modules)**: Ensure that the migration in Task 2 does not impede or create conflicts for these dependent tasks, which will build upon the `src/backend` structure. Verify that any decisions made during the migration align with the future state planned for security and refactoring.\n    *   **Task 7 (Merge Validation Framework)**: Ensure the output of Task 2 is compatible with, and fully validated by, the framework established in Task 7. Coordinate with the Task 7 team to define and execute validation checks specifically for the migrated backend, utilizing the `scientific_branch_specific_checks` from the CI/CD pipeline (`.github/workflows/main.yml`).\n    *   **Task 10 (Advanced Testing Integration)**: Leverage the improved task management and testing integration from Task 10 to better track testing efforts within Task 2 and ensure comprehensive test coverage for the migrated components.\n4.  **Risk Management & Issue Resolution**: Proactively identify potential risks (e.g., missed imports, dependency conflicts, performance degradation) during the migration. Facilitate the resolution of critical issues by escalating to appropriate teams or resources. Maintain a log of issues and their resolution.\n5.  **Documentation & Knowledge Transfer**: Ensure that any significant architectural changes, decisions, or new patterns introduced during the migration are well-documented. Prepare for knowledge transfer sessions to bring other teams up to speed on the new `src/backend` structure.",
        "testStrategy": "1.  **Task 2 Completion Verification**: Confirm that Task 2 (the actual migration execution) is marked as complete, with all its sub-tasks successfully verified according to its own test strategy.\n2.  **Validation Framework Execution Report Review**: Review the results from the Merge Validation Framework (Task 7). Specifically, verify that all architectural, functional, performance, and security checks related to the `src/backend` migration pass successfully on the `scientific` branch. This includes reviewing output from `pytest` runs and static analysis tools.\n3.  **Smoke Test and Functionality Check**: Conduct high-level, manual smoke tests or request reports from the QA team to ensure critical API endpoints (e.g., `/api/v1/health`, core AI services, database interactions) are fully functional on the `scientific` branch after migration.\n4.  **Dependency Review**: Verify that tasks dependent on the migration (e.g., Task 3 and Task 4) can proceed without encountering foundational issues related to the `src/backend` structure.\n5.  **Stakeholder Sign-off**: Obtain formal sign-off from relevant stakeholders (e.g., project lead, architecture review board) confirming satisfaction with the completed migration and its adherence to architectural standards.",
        "status": "pending",
        "dependencies": [
          "7"
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Move Backend Files to src/",
            "description": "Move all source code from the 'backend' directory to the new 'src/backend' directory. This is the first step in the migration process.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 11
          },
          {
            "id": 2,
            "title": "Update Imports and References",
            "description": "Update all internal imports and external references throughout the codebase to reflect the new 'src/backend' path. This includes imports in both backend and frontend code.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 11
          },
          {
            "id": 3,
            "title": "Update Configuration Files",
            "description": "Update all relevant configuration files (e.g., Docker, tsconfig, build scripts) to support the new backend structure. This ensures that all services and build processes work correctly after the migration.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 11
          },
          {
            "id": 4,
            "title": "Run and Fix Tests",
            "description": "Run the full test suite to ensure that the migration has not introduced any regressions. Fix any failing tests.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 11
          },
          {
            "id": 5,
            "title": "Final Cleanup",
            "description": "Remove the original 'backend' directory from the project. This is the final step in the migration process.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 11
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-11-06T04:11:47.735Z",
      "updated": "2025-11-07T17:12:34.439Z",
      "description": "Tasks for master context"
    }
  }
}