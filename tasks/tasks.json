{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Recover Lost Backend Modules and Features",
        "description": "Investigate Git history to identify, recover, and reintegrate any backend modules or features that were inadvertently lost, ensuring a complete and correct codebase for subsequent development.",
        "details": "1.  **Git History Investigation and Identification**:\n    *   Utilize `git log --graph --oneline --decorate --all` to visualize the repository's commit history, focusing on the `main` branch and any recent feature merges.\n    *   Identify commits or merge points where specific backend modules, files, or functionalities might have been inadvertently removed or overwritten.\n    *   Collaborate with developers involved in recent significant merges to gather context on potential losses.\n    *   Use `git diff <commit_hash1> <commit_hash2>` or `git diff <branch1> <branch2>` to compare states and pinpoint missing code.\n    *   If specific files are suspected, `git blame <file_path>` can help track changes and removals.\n    *   Document the precise files, directories, and code snippets identified as lost, along with their expected location and functionality.\n2.  **Recovery Strategy and Implementation**:\n    *   Create a new, dedicated feature branch (e.g., `feature/recover-lost-modules`) for the recovery work to avoid direct manipulation of the `main` branch.\n    *   For entire missing files or directories, use `git checkout <commit_hash_where_it_existed> -- <path/to/file_or_directory>` to restore them.\n    *   For specific code blocks or functionalities, consider `git cherry-pick <commit_hash_introducing_feature>` if the feature was introduced cleanly in a single commit, or manually re-apply the code from older versions if granular recovery is needed.\n    *   In cases of complex merge conflicts that led to loss, analyze the possibility of reverting the problematic merge and re-merging with care, or rebasing.\n    *   Ensure all recovered code conforms to current coding standards and project conventions.\n3.  **Reintegration and Configuration**:\n    *   Resolve any merge conflicts that arise during the reintegration of recovered code, carefully preserving both the recovered functionality and existing valid code.\n    *   Verify that all recovered modules are correctly referenced in application configuration files, build scripts, routing definitions, and dependency injection containers.\n    *   Update any necessary documentation (e.g., API docs, READMEs) to reflect the restored modules and features.",
        "testStrategy": "1.  **Automated Test Execution**:\n    *   Run the complete suite of existing unit, integration, and end-to-end tests to ensure that the recovery process has not introduced any regressions or broken existing functionalities.\n    *   If the recovered modules/features had dedicated tests in their original form, ensure these tests are also recovered, pass, and are integrated into the current test suite.\n    *   Develop new unit and integration tests specifically for the recovered components if insufficient test coverage exists or if the recovered code introduces new complexities.\n2.  **Manual Functional Verification**:\n    *   Deploy the codebase with the recovered modules to a dedicated staging or development environment.\n    *   Perform comprehensive manual testing of all identified and recovered backend functionalities, endpoints, and services.\n    *   Verify data integrity and persistence for features that interact with the database.\n    *   Compare the behavior and output of the recovered features against historical documentation or a known good version of the application.\n3.  **Code Review and Architectural Compliance**:\n    *   Conduct a thorough code review of all changes made during the recovery process to ensure correctness, maintainability, and adherence to architectural guidelines.\n    *   Confirm that the recovered modules integrate seamlessly into the existing backend architecture.\n4.  **Performance Check (if relevant)**:\n    *   For critical or performance-sensitive recovered modules, perform basic load or performance tests to ensure that their reintegration does not negatively impact overall system performance.",
        "status": "pending",
        "dependencies": [],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Recovery Branch and Compile Lost Feature Checklist",
            "description": "Prepare the workspace for code recovery by creating a new Git branch. Review `docs/PRD.md` to create a comprehensive checklist of all features and modules that are expected to be present in the backend.",
            "details": "Create a new branch named `feature/recover-lost-modules`. Create a new file `docs/recovery_log.md` and populate it with a 'Lost Features Checklist' section based on the analysis of `docs/PRD.md`, including 'Smart Filtering Engine', 'Smart Retrieval Engine', and 'Email Summarization AI'.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 1
          },
          {
            "id": 2,
            "title": "Audit Git History to Identify Commits Containing Lost Modules",
            "description": "Use Git forensics commands to search the repository's history for commits related to the lost features identified in the checklist. Document the findings in the recovery log.",
            "details": "Use commands like `git log --diff-filter=D --summary`, `git log -S'SmartFilteringEngine'`, `git log -S'SmartRetrievalEngine'`, and `git reflog` to find commits where files like `smart_filters.py` and `smart_retrieval.py` were deleted or last existed. Record the relevant commit hashes and file paths in `docs/recovery_log.md`.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 1
          },
          {
            "id": 3,
            "title": "Restore Core AI Modules: 'smart_filters' and 'smart_retrieval'",
            "description": "Restore the `smart_filters.py` and `smart_retrieval.py` modules from the commits identified during the Git audit into the recovery branch.",
            "details": "Using the commit hashes documented in `docs/recovery_log.md`, execute `git checkout &lt;commit_hash&gt; -- &lt;path/to/file&gt;` for both `smart_filters.py` and `smart_retrieval.py` to restore them to the `backend/` directory (or their original location).",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 1
          },
          {
            "id": 4,
            "title": "Restore Additional Lost Modules (e.g., Email Summarization)",
            "description": "Restore any other identified lost modules, such as the email summarization feature and related utility functions, using the same Git recovery process.",
            "details": "Based on the git audit, restore other missing files, such as a module for the 'Email Summarization AI' (e.g., `backend/summarizer.py`) and any helper utilities they depended on. Use `git checkout &lt;commit_hash&gt; -- &lt;path/to/file&gt;` and update `docs/recovery_log.md` with the status of each restored item.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 1
          },
          {
            "id": 5,
            "title": "Integrate and Verify All Recovered Backend Modules",
            "description": "Ensure all restored modules are correctly integrated into the application. This includes fixing imports, running existing tests, and adding minimal new tests for uncovered restored code to confirm basic functionality.",
            "details": "Update `backend/main.py` and other necessary files to import and wire up endpoints for the restored modules. Execute the full existing test suite (`pytest tests/`). If restored features like `smart_filters` lack tests, add a new test file (e.g., `tests/test_smart_filters.py`) with a single test case to confirm a basic function call succeeds.",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 1
          }
        ]
      },
      {
        "id": 2,
        "title": "Migrate Backend from Node.js to Python",
        "description": "Migrate the entire existing Node.js backend to Python, ensuring full feature parity, data integrity, and comparable or improved performance for all API endpoints, database interactions, and authentication mechanisms.",
        "details": "1.  **Project Setup and Framework Selection**: Initialize a new Python project. Select an appropriate web framework (e.g., FastAPI, Flask, Django) that aligns with project requirements and team expertise. Set up virtual environment and basic project structure.\n2.  **Feature Inventory and API Documentation**: Comprehensive inventory of all existing Node.js backend features, API endpoints, request/response schemas, business logic, and database interactions. Document the current API using tools like Swagger/OpenAPI specifications if not already available.\n3.  **Database Migration Strategy**: Identify the existing database type and connection methods. Choose a suitable Python ORM/ODM (e.g., SQLAlchemy, Django ORM) and design Python models that mirror the existing database schema. Implement connection pooling and query translation from Node.js to Python.\n4.  **API Endpoint Implementation**: Recreate each API endpoint from the Node.js backend in the chosen Python framework. Ensure identical URL paths, HTTP methods, request body/query parameter validation, response structures, and status codes. Translate all business logic accurately.\n5.  **Authentication and Authorization Conversion**: Analyze the current Node.js authentication (e.g., JWT, OAuth, session-based) and authorization schemes. Implement an equivalent, secure system in Python, integrating with the chosen framework's security features and best practices.\n6.  **Dependency Management and Equivalents**: Identify all external Node.js packages and libraries used. Find suitable Python equivalents or develop custom solutions for missing functionalities.\n7.  **Error Handling and Logging**: Implement robust and consistent error handling, exception management, and logging mechanisms across the Python backend, matching or improving upon the existing Node.js system.\n8.  **Configuration Management**: Establish a clear and secure configuration management system for database credentials, API keys, and environment-specific settings.\n9.  **Performance Optimization (Initial Pass)**: During development, pay attention to potential performance bottlenecks. Use asynchronous programming where appropriate (e.g., with FastAPI/asyncio) and optimize database queries to maintain or improve performance.\n10. **Documentation Update**: Update all relevant backend documentation, including API specifications, setup guides, and deployment instructions, to reflect the Python implementation.",
        "testStrategy": "1.  **Unit Testing**: Develop comprehensive unit tests for all new Python models, services, controllers, and utility functions to ensure individual components work as expected.\n2.  **API Contract Testing**: Use automated tools (e.g., Postman collections, OpenAPI/Swagger tools, `pytest-httpx`) to run tests against each Python API endpoint, verifying request/response schemas, status codes, and data integrity against the documented Node.js API contracts.\n3.  **Integration Testing**: Implement integration tests to verify the interactions between the Python backend components (e.g., database interactions, external service calls, authentication flow).\n4.  **End-to-End Testing**: Leverage existing end-to-end test suites (if any) or create new ones to test complete user flows against the new Python backend, ensuring all features function correctly from the client's perspective.\n5.  **Performance Benchmarking**: Conduct load and stress testing using tools like JMeter or k6 to compare the performance (response times, throughput, resource utilization) of the new Python backend against the original Node.js backend under similar conditions. Establish performance baselines and targets.\n6.  **Security Testing**: Perform security audits and vulnerability scanning (e.g., OWASP ZAP, manual penetration testing) on the Python backend, specifically focusing on authentication, authorization, input validation, and data handling.\n7.  **Data Integrity Verification**: After migration, perform extensive data integrity checks to ensure no data loss or corruption occurred during the database interactions and that all data is correctly retrieved and stored by the Python application.\n8.  **Feature Parity Check**: Maintain a detailed checklist of all features identified from the Node.js backend and systematically verify their correct and equivalent implementation in the Python backend. This can involve manual verification alongside automated tests.\n9.  **Error Handling and Logging Verification**: Test various error scenarios (e.g., invalid input, database errors, external service failures) to ensure the Python backend handles them gracefully and logs appropriate information.",
        "status": "pending",
        "dependencies": [
          1
        ],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Create migration branch from scientific",
            "description": "Create feature/backend-to-src-migration branch from scientific branch",
            "details": "Execute `git checkout scientific && git pull && git checkout -b feature/backend-to-src-migration`",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 2
          },
          {
            "id": 2,
            "title": "Move backend directory to src/backend",
            "description": "Physically relocate backend directory to src/backend structure",
            "details": "Execute `mkdir -p src && mv backend src/backend && touch src/__init__.py src/backend/__init__.py`",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 2
          },
          {
            "id": 3,
            "title": "Update all Python import statements",
            "description": "Replace all `from backend.` imports with `from src.backend.`",
            "details": "Use sed/grep to find and replace import statements across the codebase",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 2
          },
          {
            "id": 4,
            "title": "Update configuration files",
            "description": "Update PYTHONPATH and other references in config files",
            "details": "Update Dockerfiles, docker-compose.yml, CI/CD configs, and pyproject.toml",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 2
          },
          {
            "id": 5,
            "title": "Run full test suite and smoke tests",
            "description": "Execute comprehensive testing to ensure migration success",
            "details": "Run pytest, start application, test API endpoints",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 2
          },
          {
            "id": 6,
            "title": "Clean up original backend directory",
            "description": "Remove obsolete top-level backend directory",
            "details": "Execute `rm -rf backend` after validation",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 2
          }
        ]
      },
      {
        "id": 3,
        "title": "Recover Lost Backend Modules and Features",
        "description": "Investigate the Git history to find, recover, and reintegrate backend modules and features that were lost during recent commits and merges. This is a prerequisite to ensure the subsequent migration and refactoring work is performed on a complete and correct codebase.",
        "details": "Begin by creating a comprehensive list of expected features and modules based on the PRD and any existing design documents. Use Git commands to audit the repository's history for lost code. Key commands include `git reflog` to find dangling commits, `git log --diff-filter=D --summary` to find deleted files, and `git log -S'&lt;unique_code_string&gt;'` to search for code that has disappeared. Once found, use `git checkout &lt;commit_hash&gt; -- &lt;file_path&gt;` or `git cherry-pick &lt;commit_id&gt;` to restore the code into a dedicated recovery branch. All recovered code must be documented in a new file, e.g., `docs/recovery_log.md`.",
        "testStrategy": "Create a new branch for recovery. After restoring files/code, run the existing test suite to check for immediate breakages. For any recovered feature that lacks test coverage, write minimal unit or integration tests to confirm its basic functionality is restored. The primary goal is to reintegrate the code and ensure the application remains stable. A peer review of the recovered code against the recovery log is mandatory before merging.",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Backend Migration from 'backend' to 'src/backend'",
        "description": "Execute the complete structural migration of the legacy 'backend' package to the new modular architecture under 'src/backend'. This is a critical architectural improvement to modernize the codebase and enable further development.",
        "details": "1. Create a new branch from the scientific branch (which contains the most advanced backend implementation). 2. Physically move the entire `backend` directory into `src/` and rename it to `src/backend`. 3. Perform a project-wide find-and-replace to update all Python import statements from `from backend...` to `from src.backend...`. Use a tool like `sed`, `grep`, or an IDE's refactoring capabilities. 4. Update any configuration files that reference the old path. This includes `PYTHONPATH` definitions in Dockerfiles (`ENV PYTHONPATH=\"/app/src:$PYTHONPATH\"`), `docker-compose.yml`, CI/CD pipeline scripts (e.g., `.github/workflows/ci.yml`), and potentially `pyproject.toml` if using editable installs. 5. After all tests pass, delete the original top-level `backend` directory. 6. Ensure `src/__init__.py` and `src/backend/__init__.py` exist to make them valid packages.",
        "testStrategy": "The primary validation method is comprehensive regression testing. Run the full existing test suite and ensure 100% of tests pass. Perform a manual smoke test of the application by starting it and hitting key API endpoints using Postman or cURL to confirm it is fully functional. The application's behavior must be identical to the pre-migration state. No new feature tests are required for this task.",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Implement Enhanced Security: RBAC, MFA, Session Management, and Auditing",
        "description": "Upgrade the existing JWT-based authentication system to include Role-Based Access Control (RBAC), Multi-Factor Authentication (MFA), secure session management, and audit logging for sensitive operations, following modern security best practices.",
        "details": "This task will be implemented on the new `src/backend` structure. 1. **RBAC**: Extend the User model/schema with a `role: str` field. Create a FastAPI dependency that checks roles. Example: `def require_role(required_roles: list[str]): def dependency(current_user: User = Depends(get_current_user)): if current_user.role not in required_roles: raise HTTPException(403); return current_user; return dependency`. Protect routes with `Depends(require_role(['admin']))`. 2. **MFA**: Integrate `pyotp~=2.9.0`. Add `mfa_secret: str` and `mfa_enabled: bool` to the User model. Create endpoints: `POST /auth/mfa/generate` (returns a `pyotp.totp.TOTP(...).provisioning_uri(...)`) and `POST /auth/mfa/verify` (checks code). Update the login flow to require a second step if MFA is enabled. 3. **Session Management**: Use `redis-py~=5.0.1`. On login, store a session token in Redis with a TTL (e.g., 1h). The JWT issued to the client can be short-lived (e.g., 15m). A middleware will validate the session token against Redis on each request, allowing for immediate session revocation. 4. **Audit Logging**: Use Python's `logging` module to create a dedicated 'audit' logger. Create a middleware or decorator to apply to sensitive endpoints. The log should capture `timestamp`, `user_id`, `source_ip`, `action`, and `outcome` in a structured format (JSON is recommended).",
        "testStrategy": "Develop unit tests for each new security feature. For RBAC, test that users with incorrect roles are denied access (403). For MFA, test the TOTP generation and validation logic. For Session Management, test session expiry and revocation. For Audit Logging, verify that sensitive actions are logged correctly to the specified output. Create integration tests that simulate a full login flow with MFA and subsequent access to a role-protected endpoint.",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Identify Divergent Branches and Create Alignment Plan",
            "description": "Analyze all remote feature branches to identify which have diverged from the 'scientific' branch. Document these branches in a new markdown file to serve as a master checklist for the alignment process.",
            "details": "Use 'git branch --remote' and 'git log' to compare each feature branch with 'scientific'. Create a new file named `ALIGNMENT_CHECKLIST.md` in the project root. List the branches to be aligned, including `feature/backlog-ac-updates`, `fix/import-error-corrections`, `docs-cleanup`, `feature/search-in-category`, `feature/merge-clean`, and `feature/merge-setup-improvements`. Add columns for status and notes.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 5
          },
          {
            "id": 2,
            "title": "Create Local Backups of All Targeted Feature Branches",
            "description": "Before initiating any merge operations, create local backup copies of all feature branches listed in the alignment checklist. This provides a critical safety net and a simple rollback point.",
            "details": "For each branch identified in `ALIGNMENT_CHECKLIST.md`, execute the command `git branch &lt;branch-name&gt;-backup-pre-align`. For example, for 'feature/backlog-ac-updates', run `git branch feature/backlog-ac-updates-backup-pre-align`. Verify backup creation with `git branch`.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 5
          },
          {
            "id": 3,
            "title": "Align 'feature/backlog-ac-updates' with 'scientific' Branch",
            "description": "Perform a merge of the 'scientific' branch into the 'feature/backlog-ac-updates' branch. This will propagate the latest stable changes into the feature branch.",
            "details": "First, checkout the feature branch using `git checkout feature/backlog-ac-updates`. Then, run `git merge scientific`. Systematically resolve any merge conflicts that arise using a visual merge tool and document complex resolutions in the merge commit message.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 5
          },
          {
            "id": 4,
            "title": "Align 'fix/import-error-corrections' with 'scientific' Branch",
            "description": "Perform a merge of the 'scientific' branch into the 'fix/import-error-corrections' branch to ensure the fix is compatible with the latest codebase.",
            "details": "Checkout the fix branch using `git checkout fix/import-error-corrections`. Execute `git merge scientific`. Carefully resolve any merge conflicts, paying special attention to file path and import statement changes given the nature of this branch.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 5
          },
          {
            "id": 5,
            "title": "Align 'feature/search-in-category' with 'scientific' Branch",
            "description": "Merge the latest changes from the 'scientific' branch into the 'feature/search-in-category' branch, resolving any resulting conflicts.",
            "details": "Switch to the feature branch with `git checkout feature/search-in-category`. Run `git merge scientific` to start the alignment. Resolve all merge conflicts and ensure the new search functionality integrates correctly with the latest base code from 'scientific'.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 5
          },
          {
            "id": 6,
            "title": "Align Remaining Minor Branches with 'scientific'",
            "description": "Perform the alignment process for the remaining, lower-priority branches: 'docs-cleanup', 'feature/merge-clean', and 'feature/merge-setup-improvements'.",
            "details": "Sequentially process each branch. For each one, use `git checkout &lt;branch-name&gt;`, then `git merge scientific`. Resolve any conflicts that may arise. Given their scope, these branches are expected to have fewer conflicts.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 5
          },
          {
            "id": 7,
            "title": "Push Aligned Branches and Create Pull Requests",
            "description": "Push all the locally merged and validated feature branches to the remote repository. Create a corresponding Pull Request for each aligned branch to enable code review and trigger CI/CD pipelines.",
            "details": "For each successfully aligned branch, run `git push origin &lt;branch-name&gt; --force-with-lease` if rebase was used, or `git push` if merge was used. Subsequently, navigate to the repository's web UI and create a Pull Request for each branch, targeting 'scientific'.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 5
          },
          {
            "id": 8,
            "title": "Final Validation, Review, and Checklist Completion",
            "description": "Monitor the CI pipelines for all created Pull Requests, address any feedback from code reviews, and update the master checklist to reflect the completion of each branch alignment.",
            "details": "Review the CI build logs for each PR to ensure all tests pass. Address any code review comments by pushing new commits to the respective branches. After a PR is fully validated and approved, update its status to 'Completed' in the `ALIGNMENT_CHECKLIST.md` file.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 5
          }
        ]
      },
      {
        "id": 6,
        "title": "Refactor High-Complexity Modules and Duplicated Code",
        "description": "Improve code quality by refactoring large modules (`smart_filters.py`, `smart_retrieval.py`), reducing code duplication in AI engine modules, and simplifying high-complexity functions as identified in the PRD, applying research-backed refactoring best practices.",
        "details": "This refactoring will be performed on the new `src/backend` structure, incorporating modern refactoring best practices and an incremental, test-driven approach.\n\n1.  **Module Splitting & Incremental Refactoring for `smart_filters.py` and `smart_retrieval.py`**:\n    *   **Goal**: Decompose these large, monolithic modules (`src/backend/smart_filters.py` at 1598 lines and `src/backend/smart_retrieval.py` at 1198 lines) into smaller, more cohesive, and single-responsibility modules, strictly adhering to the Single Responsibility Principle (SRP) and aiming for high cohesion and low coupling.\n    *   **Strategy**: Employ a phased 'Extract Module' or 'Extract Class' refactoring pattern. For each large module:\n        *   **Identify Cohesive Units**: Systematically analyze the existing files to identify distinct logical concerns, architectural layers, or sets of closely related functions/data that exhibit 'Feature Envy' (methods using data from other objects more than their own) or form 'Data Clumps'. These are prime candidates for extraction.\n        *   **Create Dedicated Packages**: Create a dedicated package directory, e.g., `src/backend/smart_filters/` and `src/backend/smart_retrieval/`. This clearly delineates the new module boundaries.\n        *   **Module Breakdown Examples**:\n            *   `smart_filters.py`: Potential modules include `rule_parsers.py` (for parsing rule definitions from configuration, e.g., `src/backend/smart_filters/rule_parsers.py`), `filter_evaluators.py` (for applying filter logic), `data_transformers.py` (for preparing data for filtering), `persistence.py` (for loading/saving filter configurations).\n            *   `smart_retrieval.py`: Potential modules include `query_builders.py` (for constructing retrieval queries), `ranking_algorithms.py` (for scoring and ordering results), `source_integrations.py` (for interfacing with data sources, potentially using 'Ports and Adapters' pattern), `result_processors.py` (for...",
        "testStrategy": "This is a pure refactoring task; no new functionality should be introduced, and the external behavior of the system must remain unchanged. A robust test-driven refactoring strategy will be employed:\n1.  **Pre-refactoring Characterization Tests (Golden Master)**: Before initiating any changes, ensure all target modules (`src/backend/smart_filters.py`, `src/backend/smart_retrieval.py`, `src/backend/ai/email_filter_node.py`, `src/backend/dependencies.py`, `src/backend/utils/database.py`, and any relevant AI engine modules) have comprehensive test coverage. If coverage is lacking, write 'characterization tests' (also known as 'golden master tests') to rigorously capture and lock in the current observable behavior, even if it's imperfect. These tests are critical to ensure functional equivalence post-refactoring.\n2.  **Incremental Testing and Verification**: After *each small, atomic refactoring step* (e.g., extracting a single method, moving a set of functions to a new module, changing an import), immediately run the relevant test suite (including characterization tests) to catch regressions early. This iterative testing approach is fundamental to safe and effective refactoring. The goal is to always have a green test suite.\n3.  **Behavioral Preservation**: All existing tests for the modified code must pass without any alterations to the tests themselves. The external behavior of the system should remain unchanged. New tests should only be added to cover newly extracted units or to improve coverage where it was initially insufficient for safe refactoring.\n4.  **Static Analysis Integration and Monitoring**: \n    *   **Baseline**: Run a static analysis tool like `radon` (`radon cc -a .` for cyclomatic complexity and `radon mi -a .` for maintainability index) across the codebase (or targeted modules) *before* starting the refactoring to establish a quantitative baseline for complexity and maintainability.\n    *   **Continuous Monitoring**: In...",
        "status": "pending",
        "dependencies": [],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Align Feature Branches with Scientific Branch",
        "description": "Systematically align multiple feature branches with the scientific branch to ensure code consistency, reduce future merge conflicts, and propagate the latest changes. This process will carefully preserve the key business logic and unique features of each feature branch, focusing updates primarily on architectural and common files from the scientific branch to minimize unnecessary conflicts, while ensuring feature-specific work is maintained.",
        "details": "This high-priority task involves a coordinated effort to bring all active feature branches up-to-date with the stable 'scientific' branch. The process will be executed in three main parts.\n\nPart 1: Feature Branch Assessment\n- Use `git branch --remote` and `git log` to identify all active branches diverging from 'scientific'.\n- Create a shared checklist (e.g., a repository markdown file) of branches to be aligned, including: feature/backlog-ac-updates, fix/import-error-corrections, docs-cleanup, feature/search-in-category, feature/merge-clean, feature/merge-setup-improvements.\n- Prioritize the branches based on importance and complexity of divergence.\n\nPart 2: Alignment Execution\n- For each branch, create a local backup first (e.g., `git branch &lt;branch-name&gt;-backup-pre-align`).\n- Check out the feature branch (`git checkout &lt;feature-branch&gt;`).\n- For public or shared branches, use a merge strategy to preserve history: `git merge scientific`.\n- Systematically resolve any merge conflicts, using visual merge tools for clarity. Document complex resolutions in the merge commit message.\n\nPart 3: Validation and Documentation\n- After each successful alignment, push the updated branch to the remote repository.\n- Create a Pull Request for each aligned branch to allow for code review and automated CI checks.\n- Update the central alignment checklist to reflect the status of each branch.",
        "testStrategy": "Validation will focus on ensuring no regressions are introduced during the alignment process.\n1. Before beginning alignment on a branch, run its existing test suite to establish a baseline.\n2. After the merge/rebase is completed and all conflicts are resolved locally, run the full project test suite. All tests must pass.\n3. Perform a manual smoke test on the core functionality provided by the feature branch to ensure it has not been broken by the merge.\n4. Upon pushing the aligned branch and opening a PR, the continuous integration (CI) pipeline must pass all checks, including linting, unit tests, and integration tests. This CI pass is a mandatory success criterion for each branch.",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Validation Scope and Acceptance Criteria",
            "description": "Document the specific architectural rules, functional requirements, performance benchmarks, and security requirements that will be validated by the framework before any code is written.",
            "details": "Create a markdown document (e.g., `docs/ci-cd/VALIDATION_CRITERIA.md`) in the repository. It must list: 1) Architectural rules like module boundaries and import restrictions for the `src/backend` structure. 2) Key functional user flows for smoke testing of the FastAPI application. 3) Performance baseline metrics (e.g., API response times, memory usage) for critical FastAPI endpoints. 4) Security requirements and vulnerability thresholds (e.g., OWASP Top 10, dependency vulnerability scanning thresholds). This document will serve as the single source of truth for the validation framework's goals.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 2,
            "title": "Configure CI Pipeline Trigger for 'scientific' Branch Pull Requests",
            "description": "Set up a dedicated CI/CD workflow that triggers automatically when a pull request is opened from the 'scientific' branch to 'main'.",
            "details": "Create a new GitHub Actions workflow file (`.github/workflows/merge_validation.yml`). Configure it to trigger on pull requests targeting the 'main' branch. This initial version will only define the trigger and placeholder jobs for subsequent validation steps, ensuring it correctly checks out the PR code from the `src/backend` context.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 3,
            "title": "Implement Architectural Consistency Checks",
            "description": "Integrate static analysis tools into the CI pipeline to automatically enforce the architectural rules defined in the scope for the Python/FastAPI application.",
            "details": "Utilize a tool like `Pylint` (for module import rules), `Adept` (for dependency graph analysis), or a custom Python script leveraging `ast` module parsing to enforce architectural rules defined in `docs/ci-cd/VALIDATION_CRITERIA.md`. This will check module boundaries and import restrictions specific to the `src/backend` structure. Add a new step in the `.github/workflows/merge_validation.yml` CI workflow that runs this analysis. The step must fail the build if any architectural violations are detected, such as a feature module importing from another feature module within `src/backend`.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 4,
            "title": "Integrate Full Regression Test Suite Execution",
            "description": "Add a job to the validation pipeline that executes the complete backend test suite (unit and integration tests) to ensure no existing functionality is broken in the FastAPI application.",
            "details": "Add a new job to the `.github/workflows/merge_validation.yml` file. This job will install all project dependencies (e.g., from `requirements.txt` in the root or `src/backend/requirements.txt`) and then execute the full test suite using `pytest` from the `src/backend` directory. The job must be configured to fail the entire pipeline run if any single test fails. Ensure the test environment is correctly configured with all necessary services (e.g., database, cache) required for the FastAPI application.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 5,
            "title": "Develop and Integrate E2E Smoke Tests",
            "description": "Create and run a suite of automated end-to-end (E2E) smoke tests against a live instance of the FastAPI application to verify critical user flows.",
            "details": "Using `pytest` with `httpx` (or `requests`), write tests that simulate core FastAPI application workflows (e.g., user authentication, data submission, primary resource retrieval) against the `src/backend` service. The CI pipeline will need a step to build a Docker container for the PR's code (e.g., from `src/backend/Dockerfile`), run it, expose the FastAPI application, and then execute these smoke tests against the running container's API.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 6,
            "title": "Implement Automated Performance Benchmark Testing",
            "description": "Create and integrate performance tests for critical FastAPI API endpoints to automatically detect and flag performance regressions.",
            "details": "Use a tool like `locust` or `pytest-benchmark` within `pytest` to measure key performance indicators (KPIs) like p95 response time for 5-10 critical FastAPI endpoints from `src/backend`. Establish a baseline by running these tests against the 'main' branch. The CI script will compare the PR's performance against this baseline and fail if it exceeds a defined threshold (e.g., 15% degradation). The performance test environment should closely mimic production conditions for the FastAPI service.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 7,
            "title": "Implement Automated Security Vulnerability Scanning",
            "description": "Integrate automated security scanning tools into the CI pipeline to identify known vulnerabilities in dependencies and potential security weaknesses in the Python/FastAPI application code.",
            "details": "Add a new job to the `.github/workflows/merge_validation.yml` file. This job will perform two types of scans: 1) Dependency vulnerability scanning using tools like `safety` or `pip-audit` against `requirements.txt` (or similar dependency files in `src/backend`). 2) Static Application Security Testing (SAST) using tools like `Bandit` or `Pylint` with security checks configured, targeting the Python code in `src/backend`. Configure the job to fail the pipeline if critical or high-severity vulnerabilities are found, or if SAST rules are violated.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 8,
            "title": "Generate and Publish a Unified Validation Report",
            "description": "Configure the CI pipeline to aggregate results from all validation steps into a single report and publish it as a comment on the pull request.",
            "details": "Use a GitHub Action (like 'actions/github-script') in a final pipeline job to generate a markdown summary of the results. The report should include the status of architectural checks, test suite results (e.g., '1573/1573 passed'), performance benchmark deltas, and security scan findings. This job will post the markdown summary as a comment on the associated PR.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 9,
            "title": "Enforce Merge Blocking via Branch Protection Rules",
            "description": "Configure repository branch protection rules to make the merge validation pipeline a required check, preventing merges to 'main' if any validation fails.",
            "details": "In the GitHub repository settings, navigate to 'Branches' and edit the protection rule for the 'main' branch. Under 'Require status checks to pass before merging', add the merge validation CI job. This will physically disable the merge button on pull requests until the check passes.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 7
          }
        ]
      },
      {
        "id": 8,
        "title": "Deep Integration & Refactoring: Align feature-notmuch-tagging-1 with Scientific, Enhance Data Platform, and Introduce Scalable AI Capabilities",
        "description": "Execute a comprehensive alignment of the `feature-notmuch-tagging-1` branch with the `scientific` branch, focusing on robust integration of the enhanced NotmuchDataSource, AI analysis, smart tagging, and full CRUD operations. This task not only involves updating architectural and common files from the `scientific` branch but also requires significant refactoring to ensure scalability, fault tolerance, and deep compatibility with existing data structures and the analytics ecosystem, while preserving and optimizing the feature branch's distinct functionalities. Special attention should be paid to data model evolution, performance under load, and comprehensive security hardening.",
        "details": "This task involves a multi-faceted integration, including architectural adjustments and new feature development:\n1.  **Strategic Branch Alignment & Conflict Resolution:** Initiate by rebasing `feature-notmuch-tagging-1` onto the latest `scientific` branch. Resolve all merge conflicts meticulously, especially in core data access and AI-related modules, ensuring `scientific`'s foundational stability while integrating feature-specific changes. Document all non-trivial conflict resolutions.\n2.  **Extended NotmuchDataSource Implementation & Data Model Evolution:** In `src/core/notmuch_data_source.py`, ensure the full DataSource interface is not only implemented (`delete_email`, `get_dashboard_aggregates`, `get_category_breakdown`) but also optimized for performance and large datasets. Review and propose necessary schema migrations or adaptations in the underlying data store to accommodate new Notmuch tagging and AI analysis metadata. Ensure `get_dashboard_aggregates` supports complex filtering and time-series aggregation efficiently.\n3.  **Scalable AI Analysis & Categorization Pipeline:** Integrate the AI-powered sentiment analysis and categorization logic as a highly scalable and fault-tolerant background process. This requires implementing a message queue (e.g., Kafka/RabbitMQ) for processing, robust retry mechanisms, and error queues for failed AI jobs. The AI model should support versioning and be configurable for different analysis types (e.g., multilingual). Consider model retraining and deployment strategies.\n4.  **Advanced SmartFilterManager Integration & Rule Engine Extension:** Beyond simple connection, extend `SmartFilterManager` to allow dynamic rule creation and persistence based on AI-derived tags and user-defined criteria. This may require a new domain-specific language (DSL) or configuration interface for defining and applying complex filtering rules that leverage Notmuch tags and AI insights.\n5.  **Hierarchical Tagging & Category...",
        "testStrategy": "A rigorous and multi-layered testing strategy is required to ensure the stability, correctness, performance, and security of this complex integration:\n1.  **Expanded Regression Test Suite:** Execute a full regression test suite across the entire application to ensure no existing functionality is broken. This must include performance baselining before and after integration to identify any regressions in response times or resource consumption.\n2.  **Detailed Unit & Integration Tests for NotmuchDataSource:** Develop comprehensive unit tests covering all methods of the `NotmuchDataSource`, including edge cases for each (`delete_email` with non-existent IDs, `get_dashboard_aggregates` with various date ranges and filters, `get_category_breakdown` with empty data). Integration tests should thoroughly verify the `NotmuchDataSource`'s interaction with the actual database and `SmartFilterManager`.\n3.  **Advanced AI/Background Process Validation:** Create specific integration tests to verify the end-to-end flow of the background AI analysis and tagging processes. This includes testing message queue interactions, error handling for failed AI jobs (e.g., malformed input, AI model errors), retry mechanisms, and the persistence of AI-derived metadata. Validate AI model accuracy with a diverse test dataset, tracking false positives and negatives.\n4.  **Smart Filtering & Tagging System Validation:** Develop comprehensive tests for the `SmartFilterManager` and the new hierarchical tagging system. This includes validating dynamic rule creation, application, and persistence, as well as complex category mapping scenarios (e.g., overlapping tags, nested categories, tag deprecation).\n5.  **Observability System Verification:** Confirm that new performance metrics, distributed traces, and structured error logs are correctly generated, captured, and accessible through the centralized monitoring system. Validate alert configurations for critical errors and perfo...",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze 'main' branch for setup subtree methodology",
            "description": "Investigate the 'main' branch's Git history and current filesystem to understand the precise implementation details of the 'new setup subtree methodology'. This includes identifying the subtree's remote URL, its local path within the 'main' branch, and the specific `git subtree` commands used for its integration and updates. Pay close attention to any associated scripts or configuration files.",
            "details": "Use `git log --grep='git subtree'` on the 'main' branch to find relevant commit messages. Examine the `.git/config` file in 'main' for subtree-related entries. Identify the exact path where the subtree is mounted (e.g., 'src/setup' or 'shared/config') and the remote repository it points to. Check for any automation scripts in `scripts/` or `tools/` that manage the subtree.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 8
          },
          {
            "id": 2,
            "title": "Identify and isolate target directory in 'scientific' branch",
            "description": "Pinpoint the specific directory or set of files within the 'scientific' branch that corresponds to the 'setup' functionality now managed by the new subtree methodology in 'main'. This content will be replaced or integrated with the subtree.",
            "details": "Based on the findings from subtask 1 (e.g., if the main branch uses 'src/setup' for the subtree), identify the equivalent directory in the 'scientific' branch. Verify its content aligns with what is expected from the 'setup' component. Create a temporary `temp_backup_scientific_setup` directory.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 8
          },
          {
            "id": 3,
            "title": "Backup and remove existing setup code in 'scientific' branch",
            "description": "Before integrating the new setup subtree, create a complete backup of the existing 'setup' related code found in the 'scientific' branch. After backup, safely remove this existing code to ensure a clean slate for the subtree integration and prevent potential conflicts.",
            "details": "Copy the identified 'setup' directory (e.g., 'src/setup') from the 'scientific' branch to a temporary, version-controlled backup location (e.g., a new branch or a dedicated commit). Then, use `git rm -r &lt;path_to_setup_directory&gt;` to remove the existing directory from the 'scientific' branch's working tree and stage the deletion. Commit these changes.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 8
          },
          {
            "id": 4,
            "title": "Integrate the new setup subtree into 'scientific' branch",
            "description": "Add the 'setup' component from its designated remote repository as a `git subtree` into the 'scientific' branch, following the methodology identified in the 'main' branch. This step establishes the official link and pulls in the latest 'setup' code.",
            "details": "Using the remote URL and path identified in subtask 1, execute the `git subtree add --prefix=&lt;path_in_scientific&gt; &lt;remote_url&gt; &lt;branch_name&gt; --squash` command. For example, `git subtree add --prefix=src/setup https://github.com/org/setup-repo.git main --squash`. Commit the changes after successful addition. If the 'main' branch uses `git subtree pull` from an existing remote, ensure that remote is added first.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 8
          },
          {
            "id": 5,
            "title": "Reconcile 'scientific' branch-specific modifications",
            "description": "Carefully identify and re-apply any unique modifications, configurations, or extensions that were present in the 'scientific' branch's original 'setup' code (backed up in subtask 3) but are not part of the newly integrated subtree. This ensures feature parity for the 'scientific' branch.",
            "details": "Using the backup created in subtask 3, perform a diff between the backup and the newly integrated subtree content. Manually merge or re-apply any custom logic, configuration changes, or additional files that are specific to the 'scientific' branch's requirements. Resolve any merge conflicts that arise during this process. Commit these reconciliation changes.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 8
          },
          {
            "id": 6,
            "title": "Verify and test setup subtree functionality",
            "description": "Thoroughly test the newly integrated setup subtree to ensure all functionalities related to the 'setup' component work correctly within the 'scientific' branch. This includes validating configurations, dependencies, and any interactions with other parts of the system.",
            "details": "Execute the full suite of unit, integration, and end-to-end tests related to the 'setup' component. If the 'setup' component is responsible for environment configuration or build processes, run full build processes and test environment startup. Pay special attention to areas where 'scientific' branch had specific modifications. Ensure proper loading of configurations and dependencies.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 8
          }
        ]
      },
      {
        "id": 9,
        "title": "Create Comprehensive Merge Validation Framework",
        "description": "Create a comprehensive validation framework to ensure all architectural updates have been properly implemented before merging scientific branch to main. This framework will leverage research-backed CI/CD practices to validate consistency, functionality, performance, and security across all components, specifically tailored for our Python/FastAPI application. This task is directly linked to the backlog item: `backlog/tasks/alignment/create-merge-validation-framework.md`.",
        "details": "This framework will integrate into the GitHub Actions CI/CD pipeline, triggered on pull requests to the `main` branch from `scientific`. It will encompass several layers of automated checks, including:\n1.  **Architectural Enforcement**: Static analysis to ensure `src/backend` adheres to defined module boundaries and import rules.\n2.  **Functional Correctness**: Execution of full unit/integration test suites for `src/backend` and end-to-end smoke tests against a deployed instance of the FastAPI application.\n3.  **Performance Benchmarking**: Automated checks for performance regressions on critical FastAPI endpoints.\n4.  **Security Validation**: Dependency vulnerability scanning and Static Application Security Testing (SAST) for the Python/FastAPI codebase.\nThe ultimate goal is to automatically block merges if any of these validation layers fail, ensuring a robust and secure `main` branch. The framework will be configured to analyze the `src/backend` directory primarily.",
        "testStrategy": "The overall test strategy involves validating each component of the framework individually, then ensuring their integrated operation correctly blocks or permits merges based on the validation outcomes. This will include creating test PRs with deliberate failures (architectural, functional, performance, security) and successes to verify the framework's decision-making.",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Ensure clean working directory and checkout feature branch",
            "description": "Before starting the alignment process, ensure there are no uncommitted changes in the local repository and checkout the target feature branch `fix/import-error-corrections`.",
            "details": "Use `git status` to confirm a clean working directory. If necessary, stash or commit any local changes. Then, execute `git checkout fix/import-error-corrections`.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 9
          },
          {
            "id": 2,
            "title": "Update local 'main' branch to latest remote",
            "description": "Synchronize the local 'main' branch with the upstream 'main' branch to ensure the feature branch will be rebased against the most recent changes.",
            "details": "First, switch to the main branch (`git checkout main`), then fetch and pull the latest changes from the remote (`git pull origin main`).",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 9
          },
          {
            "id": 3,
            "title": "Run baseline tests on 'fix/import-error-corrections'",
            "description": "Execute the existing test suite on the `fix/import-error-corrections` branch to establish a functional baseline before applying changes from 'main'.",
            "details": "Switch back to the feature branch (`git checkout fix/import-error-corrections`). Run all relevant unit, integration, and end-to-end tests for the project. Document any existing failures or unexpected behaviors.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 9
          },
          {
            "id": 4,
            "title": "Rebase 'fix/import-error-corrections' onto 'main'",
            "description": "Perform the Git rebase operation to integrate the `main` branch's history into `fix/import-error-corrections`, ensuring a linear commit history.",
            "details": "While on the `fix/import-error-corrections` branch, execute `git rebase main`. This will apply the commits from the feature branch on top of the latest 'main' commit.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 9
          },
          {
            "id": 5,
            "title": "Resolve rebase conflicts",
            "description": "Address and resolve any merge conflicts that arise during the rebase process to ensure code integrity and functionality.",
            "details": "Upon encountering conflicts, use `git status` to identify conflicting files. Edit these files to resolve conflicts, then stage them (`git add &lt;file&gt;`) and continue the rebase (`git rebase --continue`). Repeat until the rebase is complete.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 9
          },
          {
            "id": 6,
            "title": "Run full test suite post-rebase",
            "description": "After successfully rebasing and resolving conflicts, run the entire project's test suite to verify that no regressions or new issues were introduced.",
            "details": "Execute all unit, integration, and end-to-end tests on the rebased `fix/import-error-corrections` branch. Compare results with the baseline tests from subtask 3.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 9
          },
          {
            "id": 7,
            "title": "Perform local functional validation and code review",
            "description": "Conduct a manual review of the aligned code and perform functional tests specific to the `import-error-corrections` feature to ensure it still works as expected.",
            "details": "Manually test the `import-error-corrections` functionality. Review the changes introduced from 'main' to ensure they integrate logically with the feature branch's code. Pay attention to any areas that had conflicts.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 9
          },
          {
            "id": 8,
            "title": "Force push rebased branch and notify team",
            "description": "Once validation is complete, force push the rebased `fix/import-error-corrections` branch to the remote repository and inform relevant team members.",
            "details": "Execute `git push --force-with-lease origin fix/import-error-corrections`. Notify the development team, especially those who might be working on or depending on this branch, about the successful rebase and updated remote branch.",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 9
          }
        ]
      },
      {
        "id": 10,
        "title": "Update Setup Subtree Integration in Scientific Branch",
        "description": "Update the scientific branch to use the new setup subtree methodology that has been implemented in the main branch. This will align the architecture for easier merging and future maintenance, as detailed in the original backlog task: backlog/tasks/alignment/update-setup-subtree-integration.md.",
        "details": "TBD",
        "testStrategy": "TBD",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Automated testing validation into task completion workflow",
            "description": "Integrate automated testing validation into the task completion workflow to ensure tasks cannot be marked complete without proper testing validation.",
            "details": "",
            "status": "completed",
            "dependencies": [],
            "parentTaskId": 10
          },
          {
            "id": 2,
            "title": "Adding branch-specific task tracking and validation",
            "description": "Implement branch-aware validation rules in CI/CD service with conditional branch-specific testing for scientific vs standard branches, and task-branch association validation.",
            "details": "",
            "status": "completed",
            "dependencies": [],
            "parentTaskId": 10
          },
          {
            "id": 3,
            "title": "Incorporate scientific branch testing frameworks into Task Master",
            "description": "Integrate the advanced testing frameworks and practices from the scientific branch into the Task Master system, including enhanced test coverage requirements, scientific-specific test suites, and advanced testing methodologies.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 10
          },
          {
            "id": 4,
            "title": "Add testing requirement enforcement in task creation",
            "description": "Implement validation rules during task creation to ensure that tasks include appropriate testing requirements, acceptance criteria, and testing strategies based on their complexity and branch context.",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 10
          }
        ]
      },
      {
        "id": 11,
        "title": "Align import-error-corrections Branch with Main Branch",
        "description": "Systematically align feature branch `fix/import-error-corrections` with the main branch. This alignment will carefully preserve the key business logic and unique features of the `fix/import-error-corrections` branch, focusing updates primarily on architectural and common files from the main branch to minimize unnecessary conflicts, while ensuring feature-specific work is maintained. This task is linked to the original backlog item: `backlog/tasks/alignment/task-feature-branch-alignment-import-error-corrections-main.md`.",
        "details": "TBD",
        "testStrategy": "TBD",
        "status": "pending",
        "dependencies": [],
        "priority": "low",
        "subtasks": []
      },
      {
        "id": 12,
        "title": "Improve Task Management with Advanced Testing Integration",
        "description": "Enhance the Task Master system with advanced testing integration capabilities, including automated testing validation in task completion workflows, branch-specific task tracking and validation, incorporation of scientific branch testing frameworks, and testing requirement enforcement in task creation.",
        "details": "This task focuses on deeply integrating testing and validation processes into our task management and CI/CD pipelines, building upon the foundational 'Comprehensive Merge Validation Framework' established in Task 7. The goal is to enforce quality and consistency from task creation through to completion.\n\n1.  **Automated Testing Validation into Task Completion Workflow:** Enhance existing GitHub Actions workflows (as defined in Task 7 for `scientific` to `main` merges) to automatically validate test results upon pull request merges or task completion markers. This includes integrating coverage checks, static analysis results, and functional test suite outcomes directly into the task status reporting (e.g., via GitHub status checks or a custom webhook to a task tracking system).\n\n2.  **Adding Branch-Specific Task Tracking and Validation:** Implement mechanisms to differentiate validation rules and required checks based on the target branch (e.g., `main`, `scientific`, feature branches). This could involve conditional GitHub Actions workflows or specific branch protection rules that vary per branch type, ensuring that `scientific` branch merges, for instance, trigger specific 'scientific branch testing frameworks'.\n\n3.  **Incorporate Scientific Branch Testing Frameworks:** Develop and integrate specialized testing frameworks for the `scientific` branch within the CI/CD pipeline. This may involve setting up dedicated test environments, using specialized data sets for model validation, or incorporating statistical testing and reproducibility checks tailored for scientific computing or AI model evaluation within `src/backend`'s AI components. Leverage existing Python testing frameworks like `pytest` and extend them with custom plugins or reporting.\n\n4.  **Add Testing Requirement Enforcement in Task Creation:** Implement checks at the pull request creation or task definition stage to ensure testing requirements are met. This could include:\n    *   Pr...",
        "testStrategy": "A multi-faceted test strategy will be employed to ensure the robust implementation of advanced testing integrations:\n1.  **Unit and Integration Tests:** Develop comprehensive unit tests for any new scripts or code that facilitate these integrations (e.g., custom GitHub Actions, webhook handlers).\n2.  **End-to-End Workflow Tests:** Create specific test scenarios for each integration point:\n    *   **Task Completion Validation:** Create a feature branch, add code, add tests (passing and failing), open a PR, and observe how the CI/CD pipeline (leveraging Task 7's framework) validates and reports task status upon merge attempts.\n    *   **Branch-Specific Validation:** Test PRs targeting `main` vs. `scientific` to ensure different sets of required checks or workflows are correctly triggered and enforced.\n    *   **Scientific Framework Integration:** Run PRs with changes to scientific components (e.g., in `src/backend/ai_engine`) and verify that the specialized scientific testing frameworks execute correctly and report results. This includes testing with expected data variations.\n    *   **Requirement Enforcement:** Attempt to create PRs that intentionally violate testing requirements (e.g., no new tests for new features, low coverage) and confirm that the enforcement mechanisms (CI checks, pre-commit) correctly block the PR or flag the issue.\n3.  **Regression Testing:** Ensure that these new integrations do not adversely affect existing CI/CD pipelines or merge validation processes defined in Task 7.\n4.  **Documentation & User Acceptance:** Verify that the new requirements and workflows are clearly documented and understandable for developers, and conduct internal UAT to confirm usability and effectiveness.",
        "status": "pending",
        "dependencies": [],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 13,
        "title": "Oversee & Track Complete Backend Migration to src/",
        "description": "Establish a comprehensive framework to oversee and track the full migration of the deprecated 'backend' package to the new modular architecture under 'src/', ensuring all components including database, AI, workflow, and API routes are successfully transitioned.",
        "details": "This task focuses on the strategic planning, coordination, and validation aspects of the complete backend migration. It is not about the direct execution of code migration, but rather the management and assurance that all migration sub-tasks are completed comprehensively and correctly.\n\n1.  **Develop Comprehensive Migration Scope Checklist**: Create a detailed checklist itemizing every component and sub-system within the deprecated 'backend' package that requires migration to 'src/'. This must explicitly cover:\n    *   Structural code relocation and import path updates.\n    *   Database management interfaces (ORM models, connection logic, migration scripts).\n    *   AI engine integrations (model loading, inference pipelines, feature stores).\n    *   Workflow system components (task definitions, queues, event handling, schedulers).\n    *   All associated API routes and services (FastAPI endpoints, Pydantic models, middleware, authentication).\n    *   Configuration files, environment variables, and deployment scripts.\n    *   Existing unit, integration, and E2E test suites.\n2.  **Define Success Criteria**: For each item in the checklist, clearly define objective success criteria that indicate a successful migration of that component (e.g., all tests pass, specific API endpoints return expected data, performance metrics are met).\n3.  **Establish Tracking and Reporting Mechanism**: Implement a dedicated system (e.g., a GitHub Project board, JIRA epic, or a central documentation page with statuses) to track the progress of each migration item against its success criteria. This system should provide clear visibility into overall migration status.\n4.  **Stakeholder Coordination Plan**: Develop a plan for regular communication and coordination with relevant teams (e.g., Data Science for AI engine, DevOps for deployment, Frontend for API consumers) to ensure their components are correctly migrated and integrated.\n5.  **Validation Strategy Integration**: Design a specific validation strategy that leverages the 'Comprehensive Merge Validation Framework' (Task 9) to automatically verify the correctness, performance, and integrity of the migrated 'src/backend' at key stages, particularly before final merges to main.",
        "testStrategy": "The verification of this task will focus on the completeness and effectiveness of the established oversight and tracking processes.\n\n1.  **Migration Scope Checklist Review**: Conduct a formal peer review or stakeholder review of the developed 'Comprehensive Migration Scope Checklist' to ensure it is exhaustive, accurate, and aligns with all project requirements. All key stakeholders must sign off on the checklist's completeness.\n2.  **Success Criteria Validation**: Individually review and validate that the defined 'Success Criteria' for each checklist item are measurable, achievable, relevant, and time-bound. Perform a sanity check by applying them to a hypothetical migrated component.\n3.  **Tracking System Audit**: Verify the established 'Tracking and Reporting Mechanism' is fully functional, correctly configured, and provides the necessary granularity and visibility for all migration components. Simulate updates to ensure accurate status reflection.\n4.  **Coordination Plan Dry Run**: Conduct a dry run or simulated coordination meeting with relevant (or representative) stakeholders to confirm the 'Stakeholder Coordination Plan' is effective and facilitates clear communication channels.\n5.  **Validation Strategy Efficacy Review**: Review the 'Validation Strategy Integration' plan to confirm it comprehensively utilizes the capabilities of the 'Comprehensive Merge Validation Framework' (Task 9) for automated and manual verification, including defining specific test cases or validation scripts where applicable.",
        "status": "pending",
        "dependencies": [
          1,
          2,
          3,
          9
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Move Backend Files to src/",
            "description": "Move all source code from the 'backend' directory to the new 'src/backend' directory. This is the first step in the migration process.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 13
          },
          {
            "id": 2,
            "title": "Update Imports and References",
            "description": "Update all internal imports and external references throughout the codebase to reflect the new 'src/backend' path. This includes imports in both backend and frontend code.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 13
          },
          {
            "id": 3,
            "title": "Update Configuration Files",
            "description": "Update all relevant configuration files (e.g., Docker, tsconfig, build scripts) to support the new backend structure. This ensures that all services and build processes work correctly after the migration.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 13
          },
          {
            "id": 4,
            "title": "Run and Fix Tests",
            "description": "Run the full test suite to ensure that the migration has not introduced any regressions. Fix any failing tests.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 13
          },
          {
            "id": 5,
            "title": "Final Cleanup",
            "description": "Remove the original 'backend' directory from the project. This is the final step in the migration process.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 13
          }
        ]
      },
      {
        "id": 14,
        "title": "Production Deployment Infrastructure Setup",
        "description": "Establish the complete production deployment infrastructure for stable application releases, encompassing Docker containerization, robust CI/CD pipelines, and comprehensive monitoring solutions.",
        "details": "This task involves architecting and implementing the foundational elements for production-ready deployment. Key steps include:\n1.  **Environment Definition**: Define the target production environment (e.g., cloud provider, specific services like AWS EKS, GCP GKE, or a self-hosted Kubernetes cluster).\n2.  **Dockerization**: Develop optimized Dockerfiles for all application components, ensuring efficient image sizes, security best practices, and production-ready configurations.\n3.  **Container Orchestration**: Implement and configure the chosen container orchestration platform (e.g., Kubernetes deployments, services, ingress controllers).\n4.  **CI/CD Pipeline Implementation**: Extend the existing CI/CD framework (established in Task 9) to create a dedicated production pipeline. This pipeline will automate building, testing, and deploying to staging and production environments. It should include gates for manual approval for production releases.\n5.  **Configuration Management**: Implement secure management of production secrets and environment-specific configurations (e.g., using AWS Secrets Manager, HashiCorp Vault, Kubernetes Secrets).\n6.  **Logging Setup**: Integrate centralized logging solutions (e.g., Elasticsearch/Kibana, Grafana Loki, cloud-native logging services) to collect and analyze application and infrastructure logs.\n7.  **Monitoring & Alerting**: Set up comprehensive monitoring for application performance (APM), infrastructure health, and error tracking using tools like Prometheus, Grafana, Datadog, or cloud-specific services. Define critical alerts and notification channels.\n8.  **Scalability & Resiliency**: Design and implement auto-scaling mechanisms and ensure high availability for critical services.\n9.  **Rollback Strategy**: Define and implement procedures for quick and reliable rollbacks in case of deployment failures or critical issues.\n10. **Documentation**: Create thorough documentation for the entire production deployment process, infrastructure architecture, monitoring dashboards, and incident response procedures.",
        "testStrategy": "A multi-stage testing and validation approach will be used:\n1.  **Staging Environment Validation**: Deploy a full instance of the application to a dedicated staging environment configured identically to production. Conduct thorough functional, performance, load, and security testing against this staging instance.\n2.  **CI/CD Pipeline End-to-End Test**: Trigger the newly built CI/CD pipeline with a release candidate to ensure that all stages (build, test, deploy to staging, approval, deploy to production) execute correctly and automatically transition the application through the environments.\n3.  **Monitoring and Alerting Simulation**: Intentionally introduce faults (e.g., stop a service, generate high error rates, simulate high CPU usage) in the staging environment to verify that monitoring systems detect these issues and appropriate alerts are triggered and routed correctly.\n4.  **Rollback Procedure Test**: Perform a simulated production deployment, then execute a full rollback to ensure the procedure is functional and the application returns to its previous stable state without data loss or prolonged downtime.\n5.  **Infrastructure Security Scan**: Conduct an automated security scan of the deployed infrastructure and Docker images to identify and remediate potential vulnerabilities.\n6.  **Documentation Review**: Have an independent reviewer or team member audit all deployment and infrastructure documentation for clarity, accuracy, and completeness.",
        "status": "pending",
        "dependencies": [
          9,
          13
        ],
        "priority": "low",
        "subtasks": []
      },
      {
        "id": 15,
        "title": "Comprehensive Security Audit and Hardening for Production",
        "description": "Conduct a comprehensive security audit and implement hardening measures for production deployment, encompassing dependency vulnerability scanning, secure configuration, secrets management, API security, rate limiting, and security monitoring.",
        "details": "This task involves a multi-faceted approach to secure the application for production deployment, building upon the established `src/backend` structure and deployment infrastructure.\n\n1.  **Dependency Vulnerability Scanning**: \n    *   Integrate automated dependency scanning into the existing GitHub Actions CI/CD pipeline (e.g., `.github/workflows/main.yml`).\n    *   Utilize tools like `pip-audit` or `safety` for Python dependencies listed in `pyproject.toml` or `requirements.txt`.\n    *   Configure `Dependabot` or a similar service for continuous monitoring and alerting on new vulnerabilities in `pyproject.toml`.\n\n2.  **Secure Configuration Management**: \n    *   Review `src/backend` code for hardcoded credentials or sensitive information. \n    *   Enforce the use of environment variables for all configuration parameters, especially sensitive ones, leveraging libraries like `python-dotenv` or Pydantic's `BaseSettings`.\n    *   Document all production configuration settings and ensure they follow least privilege principles.\n\n3.  **Secrets Management**: \n    *   Define a clear strategy for managing secrets in production. This should include using a dedicated secrets manager (e.g., AWS Secrets Manager, GCP Secret Manager, HashiCorp Vault) rather than environment variables directly for highly sensitive data.\n    *   Ensure CI/CD processes utilize GitHub Actions secrets for build-time credentials securely.\n    *   Implement secure retrieval of secrets at runtime within `src/backend`, avoiding local storage of keys.\n\n4.  **API Security (FastAPI Specific)**:\n    *   **Authentication & Authorization**: Verify that all sensitive API endpoints in `src/backend` are protected with appropriate authentication (e.g., OAuth2 with JWT using `fastapi.security`). Implement granular role-based access control where necessary.\n    *   **Input Validation**: Leverage FastAPI's Pydantic models extensively to ensure strict input validation on all incoming API requests to prevent injection attacks and malformed data.\n    *   **Output Sanitization**: Ensure no sensitive data is inadvertently exposed in API responses.\n    *   **CORS Configuration**: Properly configure CORS headers in `src/backend/main.py` (or similar entry point) to allow only trusted origins.\n    *   **HTTPS Enforcement**: Ensure all production traffic is exclusively over HTTPS.\n\n5.  **Rate Limiting**: \n    *   Implement a rate-limiting mechanism for key API endpoints within `src/backend` to prevent abuse and denial-of-service attacks. Consider using `fastapi-limiter` or a similar solution integrated with Redis.\n    *   Configure appropriate limits per IP address or authenticated user.\n\n6.  **Security Monitoring & Logging**: \n    *   Enhance existing logging in `src/backend` to capture security-relevant events (e.g., failed login attempts, unauthorized access attempts, critical configuration changes).\n    *   Integrate security logs with a centralized logging system (e.g., ELK stack, Splunk) for real-time analysis and alerting.\n    *   Establish metrics and alerts for unusual API activity, error rates, and resource consumption that could indicate an attack.",
        "testStrategy": "A comprehensive testing approach will be used to validate the security hardening measures:\n\n1.  **Automated Security Scans**: \n    *   Verify that dependency vulnerability scans run successfully in CI/CD and report no critical or high vulnerabilities.\n    *   Run static application security testing (SAST) tools against `src/backend` to identify common security flaws (e.g., Bandit for Python).\n\n2.  **Configuration Review**: \n    *   Manually review production configuration files and environment variables to ensure no sensitive data is exposed and all settings adhere to security best practices.\n    *   Perform a peer review of the secrets management implementation.\n\n3.  **Penetration Testing (Ethical Hacking)**: \n    *   Conduct internal or external penetration testing against the deployed application in a staging environment to identify vulnerabilities in API endpoints, authentication, and authorization.\n\n4.  **API Security Testing**: \n    *   Develop specific integration tests to confirm that API endpoints enforce correct authentication and authorization rules.\n    *   Test rate-limiting functionality by making excessive requests to protected endpoints and verifying that requests are correctly blocked.\n    *   Test input validation by sending malformed or malicious payloads to `src/backend` API endpoints and ensuring they are rejected.\n\n5.  **Monitoring and Alerting Validation**: \n    *   Trigger simulated security events (e.g., multiple failed logins, attempts to access unauthorized resources) and verify that these events are correctly logged and trigger appropriate alerts in the monitoring system.",
        "status": "pending",
        "dependencies": [
          9,
          14
        ],
        "priority": "medium",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-11-08T02:20:00.000Z",
      "updated": "2025-11-07T16:20:09.358Z",
      "description": "Tasks for master context"
    }
  }
}