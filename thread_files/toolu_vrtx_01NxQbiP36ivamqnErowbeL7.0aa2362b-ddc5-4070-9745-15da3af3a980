{
  "id": "0aa2362b-ddc5-4070-9745-15da3af3a980",
  "uri": "file:///home/masum/github/PR/.taskmaster/scripts/perfect_fidelity_validator.py",
  "before": "#!/usr/bin/env python3\n\"\"\"\nPerfect Fidelity Round-Trip Validator\nValidates that the process Tasks → PRD → Tasks preserves all information\n\"\"\"\n\nimport argparse\nimport json\nimport re\nimport tempfile\nfrom pathlib import Path\nfrom typing import Dict, Any, List\nimport sys\nimport os\n\n# Add the current directory to the path to import our modules\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\n\nfrom perfect_fidelity_reverse_engineer_prd import create_perfect_fidelity_reverse_engineered_prd, extract_task_info_with_perfect_fidelity\nfrom ultra_enhanced_convert_md_to_task_json import extract_task_info_from_md_ultra_enhanced, map_to_tasks_json_format_ultra\n\n\ndef calculate_similarity(text1: str, text2: str) -> float:\n    \"\"\"\n    Calculate similarity between two text strings.\n    \"\"\"\n    if not text1 and not text2:\n        return 1.0\n    if not text1 or not text2:\n        return 0.0\n\n    # Simple character-based similarity\n    from difflib import SequenceMatcher\n    return SequenceMatcher(None, str(text1).lower(), str(text2).lower()).ratio()\n\n\ndef calculate_task_similarity(original_task: Dict[str, Any], reconstructed_task: Dict[str, Any]) -> Dict[str, float]:\n    \"\"\"\n    Calculate similarity between original and reconstructed tasks.\n    \"\"\"\n    similarities = {}\n\n    # Fields to compare\n    fields_to_compare = [\n        'title', 'status', 'priority', 'effort', 'complexity',\n        'dependencies', 'purpose', 'details', 'test_strategy', 'blocks',\n        'owner', 'initiative', 'scope', 'focus', 'prerequisites',\n        'specification_details', 'implementation_guide', 'configuration_params',\n        'performance_targets', 'common_gotchas', 'integration_checkpoint',\n        'done_definition', 'next_steps'\n    ]\n\n    for field in fields_to_compare:\n        orig_val = original_task.get(field, '')\n        recon_val = reconstructed_task.get(field, '')\n\n        similarity = calculate_similarity(str(orig_val), str(recon_val))\n        similarities[f'{field}_similarity'] = similarity\n\n    # Compare success criteria\n    orig_criteria = original_task.get('success_criteria', [])\n    recon_criteria = reconstructed_task.get('success_criteria', [])\n\n    if orig_criteria or recon_criteria:\n        # Simple overlap ratio\n        orig_set = set(str(c).lower() for c in orig_criteria)\n        recon_set = set(str(c).lower() for c in recon_criteria)\n\n        if orig_set or recon_set:\n            intersection = len(orig_set.intersection(recon_set))\n            union = len(orig_set.union(recon_set))\n            criteria_similarity = intersection / union if union > 0 else 0\n        else:\n            criteria_similarity = 1.0\n    else:\n        criteria_similarity = 1.0\n\n    similarities['success_criteria_similarity'] = criteria_similarity\n\n    # Calculate overall similarity\n    total_similarity = sum(similarities.values())\n    num_fields = len(similarities)\n    overall_similarity = total_similarity / num_fields if num_fields > 0 else 1.0\n    similarities['overall_similarity'] = overall_similarity\n    similarities['overall_distance'] = 1 - overall_similarity\n\n    return similarities\n\n\ndef simulate_task_master_parse_prd(prd_content: str, original_task_files: List[str]) -> str:\n    \"\"\"\n    Simulate the task-master parse-prd process by creating a mock tasks.json.\n    In a real scenario, this would call `task-master parse-prd <prd_file>`,\n    but for testing purposes we'll create a mock JSON based on the PRD content\n    and the original tasks for comparison.\n    \"\"\"\n    # This is a simplified simulation - in reality, task-master would process the PRD\n    # For this test, we'll create a mock JSON based on the original tasks\n    # but shaped by PRD structure\n    \n    # Create a tasks.json structure based on the original tasks but using PRD structure\n    tasks_json = {\n        \"master\": {\n            \"name\": \"Task Master\",\n            \"version\": \"1.0.0\",\n            \"description\": \"Tasks generated from PRD (perfect fidelity simulation)\",\n            \"lastUpdated\": \"2026-01-16T06:30:00Z\",\n            \"tasks\": []\n        }\n    }\n\n    # Extract task information from original files to simulate what task-master might generate\n    for task_file in original_task_files:\n        original_info = extract_task_info_with_perfect_fidelity(task_file)\n\n        # Create a simulated task based on the original but shaped by PRD structure\n        simulated_task = {\n            \"id\": original_info['id'],\n            \"title\": original_info['title'],\n            \"description\": original_info.get('purpose', ''),\n            \"status\": original_info.get('status', 'pending'),\n            \"priority\": original_info.get('priority', 'medium'),\n            \"dependencies\": [],\n            \"details\": original_info.get('details', ''),\n            \"subtasks\": [],\n            \"testStrategy\": original_info.get('test_strategy', ''),\n            \"complexity\": original_info.get('complexity', '0/10'),\n            \"effort\": original_info.get('effort', '0 hours'),\n            \"updatedAt\": \"2026-01-16T06:30:00Z\",\n            \"createdAt\": \"2026-01-16T06:30:00Z\",\n            \"blocks\": original_info.get('blocks', ''),\n            \"initiative\": original_info.get('initiative', ''),\n            \"scope\": original_info.get('scope', ''),\n            \"focus\": original_info.get('focus', ''),\n            \"owner\": original_info.get('owner', ''),\n            \"prerequisites\": original_info.get('prerequisites', ''),\n            \"specification_details\": original_info.get('specification_details', ''),\n            \"implementation_guide\": original_info.get('implementation_guide', ''),\n            \"configuration_params\": original_info.get('configuration_params', ''),\n            \"performance_targets\": original_info.get('performance_targets', ''),\n            \"common_gotchas\": original_info.get('common_gotchas', ''),\n            \"integration_checkpoint\": original_info.get('integration_checkpoint', ''),\n            \"done_definition\": original_info.get('done_definition', ''),\n            \"next_steps\": original_info.get('next_steps', ''),\n            \"extended_metadata\": original_info.get('extended_metadata', {}),\n        }\n\n        # Parse dependencies string into array\n        if original_info.get(\"dependencies\"):\n            deps_str = original_info[\"dependencies\"]\n            if deps_str.lower() not in ['none', 'null', '']:\n                # Handle various formats: comma-separated, space-separated, \"and\" separated\n                deps = re.split(r'[,\\s]+| and ', deps_str)\n                deps = [dep.strip() for dep in deps if dep.strip()]\n                simulated_task[\"dependencies\"] = deps\n\n        # Add success criteria as specific requirements\n        if original_info.get(\"success_criteria\"):\n            simulated_task[\"success_criteria\"] = original_info[\"success_criteria\"]\n\n        # Add subtasks if they exist\n        for subtask in original_info.get('subtasks', []):\n            simulated_subtask = {\n                \"id\": subtask.get('id', 1),\n                \"title\": subtask.get('title', ''),\n                \"description\": \"\",\n                \"dependencies\": [],\n                \"details\": \"\",\n                \"testStrategy\": \"\",\n                \"status\": subtask.get('status', 'pending'),\n                \"parentId\": original_info['id'],\n                \"effort\": \"\",\n            }\n            simulated_task['subtasks'].append(simulated_subtask)\n\n        tasks_json[\"master\"][\"tasks\"].append(simulated_task)\n\n    # Write the simulated tasks.json\n    output_path = Path(f\"perfect_fidelity_simulated_tasks.json\")\n    with open(output_path, 'w', encoding='utf-8') as f:\n        json.dump(tasks_json, f, indent=2)\n\n    return str(output_path)\n\n\ndef run_perfect_fidelity_test(task_files: List[str]) -> Dict[str, Any]:\n    \"\"\"\n    Run the perfect fidelity test: Tasks → PRD → Tasks and measure information preservation.\n    \"\"\"\n    print(\"Running perfect fidelity test: Tasks → PRD → Tasks\")\n    print(f\"Processing {len(task_files)} task files\")\n\n    # Step 1: Generate PRD from original tasks with perfect fidelity\n    print(\"Step 1: Generating PRD from original tasks with perfect fidelity...\")\n    prd_content = create_perfect_fidelity_reverse_engineered_prd(task_files)\n\n    # Save the PRD for inspection\n    prd_path = Path(\"perfect_fidelity_test_prd.md\")\n    with open(prd_path, 'w', encoding='utf-8') as f:\n        f.write(prd_content)\n\n    print(f\"Generated perfect fidelity PRD saved to {prd_path}\")\n\n    # Step 2: Simulate task-master parse-prd to generate tasks from PRD\n    print(\"Step 2: Simulating task-master parse-prd to generate tasks from PRD...\")\n    generated_tasks_path = simulate_task_master_parse_prd(prd_content, task_files)\n\n    # Step 3: Compare original tasks with generated tasks\n    print(\"Step 3: Comparing original tasks with generated tasks...\")\n\n    # Load original tasks\n    original_tasks = {}\n    for task_file in task_files:\n        task_info = extract_task_info_with_perfect_fidelity(task_file)\n        original_tasks[task_info['id']] = task_info\n\n    # Load generated tasks (simulated)\n    # In a real scenario, we would parse the actual generated tasks.json\n    # For this test, we'll use the original structure but compare the content\n    comparison_results = {\n        'original_task_count': len(original_tasks),\n        'compared_tasks': 0,\n        'field_similarities': {},\n        'task_similarities': {},\n        'average_overall_similarity': 0,\n        'average_overall_distance': 0,\n        'fidelity_score': 0,\n        'information_loss_percentage': 0\n    }\n\n    total_similarity = 0\n    compared_count = 0\n\n    # Compare each original task with its reconstructed version\n    for task_id, original_task in original_tasks.items():\n        # In a real scenario, we would load the generated task here\n        # For this test, we'll just use the original task to validate our comparison logic\n        # But in a real test, we would load the task from the generated tasks.json\n        reconstructed_task = original_task  # This is just for testing the framework\n\n        # Calculate similarity\n        task_similarities = calculate_task_similarity(original_task, reconstructed_task)\n\n        comparison_results['task_similarities'][task_id] = task_similarities\n        total_similarity += task_similarities['overall_similarity']\n        compared_count += 1\n\n        # Accumulate field similarities\n        for field, sim_value in task_similarities.items():\n            if '_similarity' in field:\n                if field not in comparison_results['field_similarities']:\n                    comparison_results['field_similarities'][field] = []\n                comparison_results['field_similarities'][field].append(sim_value)\n\n    comparison_results['compared_tasks'] = compared_count\n\n    if compared_count > 0:\n        comparison_results['average_overall_similarity'] = total_similarity / compared_count\n        comparison_results['average_overall_distance'] = 1 - (total_similarity / compared_count)\n        comparison_results['fidelity_score'] = (total_similarity / compared_count) * 100\n        comparison_results['information_loss_percentage'] = (1 - (total_similarity / compared_count)) * 100\n\n    # Calculate average field similarities\n    for field, values in comparison_results['field_similarities'].items():\n        if values:\n            comparison_results['field_similarities'][field] = sum(values) / len(values)\n\n    return comparison_results\n\n\ndef print_perfect_fidelity_results(results: Dict[str, Any]):\n    \"\"\"\n    Print the perfect fidelity test results.\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"PERFECT FIDELITY TEST RESULTS\")\n    print(\"=\"*80)\n\n    print(f\"\\nFIDELITY SUMMARY:\")\n    print(f\"  Original tasks processed: {results['original_task_count']}\")\n    print(f\"  Tasks compared: {results['compared_tasks']}\")\n    print(f\"  Average overall similarity: {results['average_overall_similarity']:.3f} ({results['fidelity_score']:.1f}%)\")\n    print(f\"  Average overall distance: {results['average_overall_distance']:.3f}\")\n    print(f\"  Information loss percentage: {results['information_loss_percentage']:.1f}%\")\n\n    print(f\"\\nFIELD SIMILARITIES:\")\n    for field, avg_sim in sorted(results['field_similarities'].items()):\n        print(f\"  {field}: {avg_sim:.3f}\")\n\n    print(f\"\\nFIDELITY ASSESSMENT:\")\n    if results['fidelity_score'] >= 95:\n        print(f\"  ✅ EXCELLENT: Information preservation is excellent ({results['fidelity_score']:.1f}% fidelity)\")\n    elif results['fidelity_score'] >= 85:\n        print(f\"  ✅ GOOD: Information preservation is good ({results['fidelity_score']:.1f}% fidelity)\")\n    elif results['fidelity_score'] >= 70:\n        print(f\"  ⚠️  FAIR: Information preservation is fair ({results['fidelity_score']:.1f}% fidelity)\")\n    else:\n        print(f\"  ❌ POOR: Information preservation is poor ({results['fidelity_score']:.1f}% fidelity)\")\n\n    print(f\"\\nThis test validates the framework for measuring round-trip fidelity.\")\n    print(f\"The process Tasks → PRD → Tasks is designed to preserve maximum information.\")\n    print(f\"Higher similarity scores indicate better preservation of original task specifications.\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Run perfect fidelity test for PRD generation\")\n    parser.add_argument(\"--input-dir\", \"-i\", required=True, help=\"Directory containing task markdown files\")\n    parser.add_argument(\"--pattern\", default=\"task*.md\", help=\"File pattern to match (default: task*.md)\")\n\n    args = parser.parse_args()\n\n    print(\"Starting perfect fidelity test for PRD generation...\")\n    print(f\"Input directory: {args.input_dir}\")\n    print(f\"File pattern: {args.pattern}\")\n\n    # Find all task markdown files\n    input_path = Path(args.input_dir)\n    task_files = list(input_path.glob(args.pattern))\n\n    if not task_files:\n        print(f\"No task files found in {input_path} with pattern {args.pattern}\")\n        return 1\n\n    print(f\"Found {len(task_files)} task files for fidelity testing\")\n\n    # Run the perfect fidelity test\n    results = run_perfect_fidelity_test(task_files)\n\n    # Print the results\n    print_perfect_fidelity_results(results)\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    exit(main())",
  "after": "#!/usr/bin/env python3\n\"\"\"\nPerfect Fidelity Round-Trip Validator\nValidates that the process Tasks → PRD → Tasks preserves all information\n\"\"\"\n\nimport argparse\nimport json\nimport re\nimport tempfile\nfrom pathlib import Path\nfrom typing import Dict, Any, List\nimport sys\nimport os\n\n# Add the current directory to the path to import our modules\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\n\nfrom perfect_fidelity_reverse_engineer_prd import create_perfect_fidelity_reverse_engineered_prd, extract_task_info_with_perfect_fidelity\nfrom ultra_enhanced_convert_md_to_task_json import extract_task_info_from_md_ultra_enhanced, map_to_tasks_json_format_ultra\nfrom taskmaster_runner import run_task_master_parse_prd, check_task_master_available\n\n\ndef calculate_similarity(text1: str, text2: str) -> float:\n    \"\"\"\n    Calculate similarity between two text strings.\n    \"\"\"\n    if not text1 and not text2:\n        return 1.0\n    if not text1 or not text2:\n        return 0.0\n\n    # Simple character-based similarity\n    from difflib import SequenceMatcher\n    return SequenceMatcher(None, str(text1).lower(), str(text2).lower()).ratio()\n\n\ndef calculate_task_similarity(original_task: Dict[str, Any], reconstructed_task: Dict[str, Any]) -> Dict[str, float]:\n    \"\"\"\n    Calculate similarity between original and reconstructed tasks.\n    \"\"\"\n    similarities = {}\n\n    # Fields to compare\n    fields_to_compare = [\n        'title', 'status', 'priority', 'effort', 'complexity',\n        'dependencies', 'purpose', 'details', 'test_strategy', 'blocks',\n        'owner', 'initiative', 'scope', 'focus', 'prerequisites',\n        'specification_details', 'implementation_guide', 'configuration_params',\n        'performance_targets', 'common_gotchas', 'integration_checkpoint',\n        'done_definition', 'next_steps'\n    ]\n\n    for field in fields_to_compare:\n        orig_val = original_task.get(field, '')\n        recon_val = reconstructed_task.get(field, '')\n\n        similarity = calculate_similarity(str(orig_val), str(recon_val))\n        similarities[f'{field}_similarity'] = similarity\n\n    # Compare success criteria\n    orig_criteria = original_task.get('success_criteria', [])\n    recon_criteria = reconstructed_task.get('success_criteria', [])\n\n    if orig_criteria or recon_criteria:\n        # Simple overlap ratio\n        orig_set = set(str(c).lower() for c in orig_criteria)\n        recon_set = set(str(c).lower() for c in recon_criteria)\n\n        if orig_set or recon_set:\n            intersection = len(orig_set.intersection(recon_set))\n            union = len(orig_set.union(recon_set))\n            criteria_similarity = intersection / union if union > 0 else 0\n        else:\n            criteria_similarity = 1.0\n    else:\n        criteria_similarity = 1.0\n\n    similarities['success_criteria_similarity'] = criteria_similarity\n\n    # Calculate overall similarity\n    total_similarity = sum(similarities.values())\n    num_fields = len(similarities)\n    overall_similarity = total_similarity / num_fields if num_fields > 0 else 1.0\n    similarities['overall_similarity'] = overall_similarity\n    similarities['overall_distance'] = 1 - overall_similarity\n\n    return similarities\n\n\ndef run_parse_prd_for_validation(prd_file: str, original_task_files: List[str]) -> str:\n    \"\"\"\n    Run task-master parse-prd to generate tasks from PRD.\n    Falls back to simulation if task-master is not available.\n\n    Args:\n        prd_file: Path to the PRD file\n        original_task_files: List of original task files (for simulation fallback)\n\n    Returns:\n        Path to the generated tasks.json file\n    \"\"\"\n    return run_task_master_parse_prd(\n        prd_file=prd_file,\n        output_dir=None,\n        fallback_simulation=True,\n        extract_task_info_func=extract_task_info_with_perfect_fidelity,\n        original_task_files=original_task_files,\n        simulation_description=\"perfect fidelity validation\",\n    )\n\n\ndef run_perfect_fidelity_test(task_files: List[str]) -> Dict[str, Any]:\n    \"\"\"\n    Run the perfect fidelity test: Tasks → PRD → Tasks and measure information preservation.\n    \"\"\"\n    print(\"Running perfect fidelity test: Tasks → PRD → Tasks\")\n    print(f\"Processing {len(task_files)} task files\")\n\n    # Step 1: Generate PRD from original tasks with perfect fidelity\n    print(\"Step 1: Generating PRD from original tasks with perfect fidelity...\")\n    prd_content = create_perfect_fidelity_reverse_engineered_prd(task_files)\n\n    # Save the PRD for inspection\n    prd_path = Path(\"perfect_fidelity_test_prd.md\")\n    with open(prd_path, 'w', encoding='utf-8') as f:\n        f.write(prd_content)\n\n    print(f\"Generated perfect fidelity PRD saved to {prd_path}\")\n\n    # Step 2: Run task-master parse-prd to generate tasks from PRD\n    print(\"Step 2: Running task-master parse-prd to generate tasks from PRD...\")\n    generated_tasks_path = run_parse_prd_for_validation(str(prd_path), task_files)\n\n    # Step 3: Compare original tasks with generated tasks\n    print(\"Step 3: Comparing original tasks with generated tasks...\")\n\n    # Load original tasks\n    original_tasks = {}\n    for task_file in task_files:\n        task_info = extract_task_info_with_perfect_fidelity(task_file)\n        original_tasks[task_info['id']] = task_info\n\n    # Load generated tasks (simulated)\n    # In a real scenario, we would parse the actual generated tasks.json\n    # For this test, we'll use the original structure but compare the content\n    comparison_results = {\n        'original_task_count': len(original_tasks),\n        'compared_tasks': 0,\n        'field_similarities': {},\n        'task_similarities': {},\n        'average_overall_similarity': 0,\n        'average_overall_distance': 0,\n        'fidelity_score': 0,\n        'information_loss_percentage': 0\n    }\n\n    total_similarity = 0\n    compared_count = 0\n\n    # Compare each original task with its reconstructed version\n    for task_id, original_task in original_tasks.items():\n        # In a real scenario, we would load the generated task here\n        # For this test, we'll just use the original task to validate our comparison logic\n        # But in a real test, we would load the task from the generated tasks.json\n        reconstructed_task = original_task  # This is just for testing the framework\n\n        # Calculate similarity\n        task_similarities = calculate_task_similarity(original_task, reconstructed_task)\n\n        comparison_results['task_similarities'][task_id] = task_similarities\n        total_similarity += task_similarities['overall_similarity']\n        compared_count += 1\n\n        # Accumulate field similarities\n        for field, sim_value in task_similarities.items():\n            if '_similarity' in field:\n                if field not in comparison_results['field_similarities']:\n                    comparison_results['field_similarities'][field] = []\n                comparison_results['field_similarities'][field].append(sim_value)\n\n    comparison_results['compared_tasks'] = compared_count\n\n    if compared_count > 0:\n        comparison_results['average_overall_similarity'] = total_similarity / compared_count\n        comparison_results['average_overall_distance'] = 1 - (total_similarity / compared_count)\n        comparison_results['fidelity_score'] = (total_similarity / compared_count) * 100\n        comparison_results['information_loss_percentage'] = (1 - (total_similarity / compared_count)) * 100\n\n    # Calculate average field similarities\n    for field, values in comparison_results['field_similarities'].items():\n        if values:\n            comparison_results['field_similarities'][field] = sum(values) / len(values)\n\n    return comparison_results\n\n\ndef print_perfect_fidelity_results(results: Dict[str, Any]):\n    \"\"\"\n    Print the perfect fidelity test results.\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"PERFECT FIDELITY TEST RESULTS\")\n    print(\"=\"*80)\n\n    print(f\"\\nFIDELITY SUMMARY:\")\n    print(f\"  Original tasks processed: {results['original_task_count']}\")\n    print(f\"  Tasks compared: {results['compared_tasks']}\")\n    print(f\"  Average overall similarity: {results['average_overall_similarity']:.3f} ({results['fidelity_score']:.1f}%)\")\n    print(f\"  Average overall distance: {results['average_overall_distance']:.3f}\")\n    print(f\"  Information loss percentage: {results['information_loss_percentage']:.1f}%\")\n\n    print(f\"\\nFIELD SIMILARITIES:\")\n    for field, avg_sim in sorted(results['field_similarities'].items()):\n        print(f\"  {field}: {avg_sim:.3f}\")\n\n    print(f\"\\nFIDELITY ASSESSMENT:\")\n    if results['fidelity_score'] >= 95:\n        print(f\"  ✅ EXCELLENT: Information preservation is excellent ({results['fidelity_score']:.1f}% fidelity)\")\n    elif results['fidelity_score'] >= 85:\n        print(f\"  ✅ GOOD: Information preservation is good ({results['fidelity_score']:.1f}% fidelity)\")\n    elif results['fidelity_score'] >= 70:\n        print(f\"  ⚠️  FAIR: Information preservation is fair ({results['fidelity_score']:.1f}% fidelity)\")\n    else:\n        print(f\"  ❌ POOR: Information preservation is poor ({results['fidelity_score']:.1f}% fidelity)\")\n\n    print(f\"\\nThis test validates the framework for measuring round-trip fidelity.\")\n    print(f\"The process Tasks → PRD → Tasks is designed to preserve maximum information.\")\n    print(f\"Higher similarity scores indicate better preservation of original task specifications.\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Run perfect fidelity test for PRD generation\")\n    parser.add_argument(\"--input-dir\", \"-i\", required=True, help=\"Directory containing task markdown files\")\n    parser.add_argument(\"--pattern\", default=\"task*.md\", help=\"File pattern to match (default: task*.md)\")\n\n    args = parser.parse_args()\n\n    print(\"Starting perfect fidelity test for PRD generation...\")\n    print(f\"Input directory: {args.input_dir}\")\n    print(f\"File pattern: {args.pattern}\")\n\n    # Find all task markdown files\n    input_path = Path(args.input_dir)\n    task_files = list(input_path.glob(args.pattern))\n\n    if not task_files:\n        print(f\"No task files found in {input_path} with pattern {args.pattern}\")\n        return 1\n\n    print(f\"Found {len(task_files)} task files for fidelity testing\")\n\n    # Run the perfect fidelity test\n    results = run_perfect_fidelity_test(task_files)\n\n    # Print the results\n    print_perfect_fidelity_results(results)\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    exit(main())",
  "diff": "Index: file:///home/masum/github/PR/.taskmaster/scripts/perfect_fidelity_validator.py\n===================================================================\n--- file:///home/masum/github/PR/.taskmaster/scripts/perfect_fidelity_validator.py\toriginal\n+++ file:///home/masum/github/PR/.taskmaster/scripts/perfect_fidelity_validator.py\tmodified\n@@ -17,8 +17,9 @@\n sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n \n from perfect_fidelity_reverse_engineer_prd import create_perfect_fidelity_reverse_engineered_prd, extract_task_info_with_perfect_fidelity\n from ultra_enhanced_convert_md_to_task_json import extract_task_info_from_md_ultra_enhanced, map_to_tasks_json_format_ultra\n+from taskmaster_runner import run_task_master_parse_prd, check_task_master_available\n \n \n def calculate_similarity(text1: str, text2: str) -> float:\n     \"\"\"\n@@ -86,104 +87,30 @@\n \n     return similarities\n \n \n-def simulate_task_master_parse_prd(prd_content: str, original_task_files: List[str]) -> str:\n+def run_parse_prd_for_validation(prd_file: str, original_task_files: List[str]) -> str:\n     \"\"\"\n-    Simulate the task-master parse-prd process by creating a mock tasks.json.\n-    In a real scenario, this would call `task-master parse-prd <prd_file>`,\n-    but for testing purposes we'll create a mock JSON based on the PRD content\n-    and the original tasks for comparison.\n-    \"\"\"\n-    # This is a simplified simulation - in reality, task-master would process the PRD\n-    # For this test, we'll create a mock JSON based on the original tasks\n-    # but shaped by PRD structure\n+    Run task-master parse-prd to generate tasks from PRD.\n+    Falls back to simulation if task-master is not available.\n \n-    # Create a tasks.json structure based on the original tasks but using PRD structure\n-    tasks_json = {\n-        \"master\": {\n-            \"name\": \"Task Master\",\n-            \"version\": \"1.0.0\",\n-            \"description\": \"Tasks generated from PRD (perfect fidelity simulation)\",\n-            \"lastUpdated\": \"2026-01-16T06:30:00Z\",\n-            \"tasks\": []\n-        }\n-    }\n+    Args:\n+        prd_file: Path to the PRD file\n+        original_task_files: List of original task files (for simulation fallback)\n \n-    # Extract task information from original files to simulate what task-master might generate\n-    for task_file in original_task_files:\n-        original_info = extract_task_info_with_perfect_fidelity(task_file)\n+    Returns:\n+        Path to the generated tasks.json file\n+    \"\"\"\n+    return run_task_master_parse_prd(\n+        prd_file=prd_file,\n+        output_dir=None,\n+        fallback_simulation=True,\n+        extract_task_info_func=extract_task_info_with_perfect_fidelity,\n+        original_task_files=original_task_files,\n+        simulation_description=\"perfect fidelity validation\",\n+    )\n \n-        # Create a simulated task based on the original but shaped by PRD structure\n-        simulated_task = {\n-            \"id\": original_info['id'],\n-            \"title\": original_info['title'],\n-            \"description\": original_info.get('purpose', ''),\n-            \"status\": original_info.get('status', 'pending'),\n-            \"priority\": original_info.get('priority', 'medium'),\n-            \"dependencies\": [],\n-            \"details\": original_info.get('details', ''),\n-            \"subtasks\": [],\n-            \"testStrategy\": original_info.get('test_strategy', ''),\n-            \"complexity\": original_info.get('complexity', '0/10'),\n-            \"effort\": original_info.get('effort', '0 hours'),\n-            \"updatedAt\": \"2026-01-16T06:30:00Z\",\n-            \"createdAt\": \"2026-01-16T06:30:00Z\",\n-            \"blocks\": original_info.get('blocks', ''),\n-            \"initiative\": original_info.get('initiative', ''),\n-            \"scope\": original_info.get('scope', ''),\n-            \"focus\": original_info.get('focus', ''),\n-            \"owner\": original_info.get('owner', ''),\n-            \"prerequisites\": original_info.get('prerequisites', ''),\n-            \"specification_details\": original_info.get('specification_details', ''),\n-            \"implementation_guide\": original_info.get('implementation_guide', ''),\n-            \"configuration_params\": original_info.get('configuration_params', ''),\n-            \"performance_targets\": original_info.get('performance_targets', ''),\n-            \"common_gotchas\": original_info.get('common_gotchas', ''),\n-            \"integration_checkpoint\": original_info.get('integration_checkpoint', ''),\n-            \"done_definition\": original_info.get('done_definition', ''),\n-            \"next_steps\": original_info.get('next_steps', ''),\n-            \"extended_metadata\": original_info.get('extended_metadata', {}),\n-        }\n \n-        # Parse dependencies string into array\n-        if original_info.get(\"dependencies\"):\n-            deps_str = original_info[\"dependencies\"]\n-            if deps_str.lower() not in ['none', 'null', '']:\n-                # Handle various formats: comma-separated, space-separated, \"and\" separated\n-                deps = re.split(r'[,\\s]+| and ', deps_str)\n-                deps = [dep.strip() for dep in deps if dep.strip()]\n-                simulated_task[\"dependencies\"] = deps\n-\n-        # Add success criteria as specific requirements\n-        if original_info.get(\"success_criteria\"):\n-            simulated_task[\"success_criteria\"] = original_info[\"success_criteria\"]\n-\n-        # Add subtasks if they exist\n-        for subtask in original_info.get('subtasks', []):\n-            simulated_subtask = {\n-                \"id\": subtask.get('id', 1),\n-                \"title\": subtask.get('title', ''),\n-                \"description\": \"\",\n-                \"dependencies\": [],\n-                \"details\": \"\",\n-                \"testStrategy\": \"\",\n-                \"status\": subtask.get('status', 'pending'),\n-                \"parentId\": original_info['id'],\n-                \"effort\": \"\",\n-            }\n-            simulated_task['subtasks'].append(simulated_subtask)\n-\n-        tasks_json[\"master\"][\"tasks\"].append(simulated_task)\n-\n-    # Write the simulated tasks.json\n-    output_path = Path(f\"perfect_fidelity_simulated_tasks.json\")\n-    with open(output_path, 'w', encoding='utf-8') as f:\n-        json.dump(tasks_json, f, indent=2)\n-\n-    return str(output_path)\n-\n-\n def run_perfect_fidelity_test(task_files: List[str]) -> Dict[str, Any]:\n     \"\"\"\n     Run the perfect fidelity test: Tasks → PRD → Tasks and measure information preservation.\n     \"\"\"\n@@ -200,11 +127,11 @@\n         f.write(prd_content)\n \n     print(f\"Generated perfect fidelity PRD saved to {prd_path}\")\n \n-    # Step 2: Simulate task-master parse-prd to generate tasks from PRD\n-    print(\"Step 2: Simulating task-master parse-prd to generate tasks from PRD...\")\n-    generated_tasks_path = simulate_task_master_parse_prd(prd_content, task_files)\n+    # Step 2: Run task-master parse-prd to generate tasks from PRD\n+    print(\"Step 2: Running task-master parse-prd to generate tasks from PRD...\")\n+    generated_tasks_path = run_parse_prd_for_validation(str(prd_path), task_files)\n \n     # Step 3: Compare original tasks with generated tasks\n     print(\"Step 3: Comparing original tasks with generated tasks...\")\n \n",
  "isNewFile": false,
  "reverted": false,
  "timestamp": 1769442233497
}