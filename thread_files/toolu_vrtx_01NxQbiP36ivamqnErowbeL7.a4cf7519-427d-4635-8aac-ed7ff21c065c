{
  "id": "a4cf7519-427d-4635-8aac-ed7ff21c065c",
  "uri": "file:///home/masum/github/PR/.taskmaster/scripts/iterative_distance_minimizer.py",
  "before": "#!/usr/bin/env python3\n\"\"\"\nIterative Task Distance Minimizer\n\nThis script implements an iterative process to minimize the distance between:\n1. Original task markdown files\n2. Tasks recreated from a PRD\n\nThe process:\n1. Generate PRD from original tasks\n2. Measure distance between original and generated tasks\n3. Adjust PRD generation to improve similarity\n4. Repeat until distance is minimized or max iterations reached\n\nUsage:\n    python iterative_distance_minimizer.py --original-dir /path/to/original/tasks --max-iterations 10\n\"\"\"\n\nimport argparse\nimport json\nimport re\nfrom pathlib import Path\nfrom typing import Dict, List, Any\nimport copy\nfrom task_distance_analyzer import analyze_task_distance, print_analysis_report\nfrom reverse_engineer_prd import create_reverse_engineered_prd, extract_task_info_from_md\n\n\ndef generate_improved_prd(task_files: List[str], iteration: int, prev_results: Dict[str, Any] = None) -> str:\n    \"\"\"\n    Generate an improved PRD based on previous iteration results.\n    \n    Args:\n        task_files: List of paths to task markdown files\n        iteration: Current iteration number\n        prev_results: Results from previous iteration (if any)\n        \n    Returns:\n        String containing the improved PRD\n    \"\"\"\n    # Start with the basic reverse engineered PRD\n    prd_content = create_reverse_engineered_prd(task_files)\n    \n    # If this is not the first iteration, adjust based on previous results\n    if iteration > 1 and prev_results:\n        # Analyze what went wrong in the previous iteration\n        aggregated_metrics = prev_results.get('aggregated_metrics', {})\n        \n        # Identify low-scoring areas to improve\n        field_similarities = aggregated_metrics.get('field_similarities', {})\n        \n        # Add specific improvements based on weak areas\n        improvements = []\n        \n        # Check for low similarity in specific fields\n        for field, similarity in field_similarities.items():\n            if similarity < 0.7:  # Threshold for \"low\" similarity\n                if 'title' in field:\n                    improvements.append(\"# IMPROVEMENT NEEDED: Task titles not matching well\")\n                    improvements.append(\"# Focus on preserving original title structure in capability mapping\")\n                elif 'purpose' in field:\n                    improvements.append(\"# IMPROVEMENT NEEDED: Purpose/description not matching well\")\n                    improvements.append(\"# Ensure purpose sections are properly extracted and mapped\")\n                elif 'status' in field:\n                    improvements.append(\"# IMPROVEMENT NEEDED: Status not matching well\")\n                    improvements.append(\"# Ensure status is properly captured in PRD generation\")\n        \n        # Insert improvements into the PRD content\n        if improvements:\n            # Find a good place to insert improvement notes\n            insertion_point = prd_content.find(\"</functional-decomposition>\")\n            if insertion_point != -1:\n                insertion_point += len(\"</functional-decomposition>\")\n                prd_content = (\n                    prd_content[:insertion_point] +\n                    \"\\n\\n<!-- ITERATION \" + str(iteration) + \" IMPROVEMENTS -->\\n\" +\n                    \"\\n\".join(improvements) +\n                    \"\\n<!-- END IMPROVEMENTS -->\\n\\n\" +\n                    prd_content[insertion_point:]\n                )\n    \n    return prd_content\n\n\ndef simulate_task_generation_from_prd(prd_content: str, original_task_files: List[str]) -> str:\n    \"\"\"\n    Simulate the task-master parse-prd process by creating a mock tasks.json.\n    \n    In a real scenario, this would call `task-master parse-prd <prd_file>`,\n    but for simulation purposes we'll create a mock JSON based on the PRD content.\n    \n    Args:\n        prd_content: Content of the PRD file\n        original_task_files: List of original task files for reference\n        \n    Returns:\n        Path to the simulated tasks.json file\n    \"\"\"\n    # In a real implementation, we would call task-master here\n    # For simulation, we'll create a mock JSON based on the original tasks\n    # but using the structure implied by the PRD\n    \n    # For now, let's create a mock tasks.json that represents what task-master\n    # might generate from the PRD\n    tasks_json = {\n        \"master\": {\n            \"name\": \"Task Master\",\n            \"version\": \"1.0.0\",\n            \"description\": \"Tasks generated from PRD (iteration simulation)\",\n            \"tasks\": []\n        }\n    }\n    \n    # Extract task information from original files to simulate what might be generated\n    for task_file in original_task_files:\n        original_info = extract_task_info_from_md(task_file)\n        \n        # Create a simulated task based on the original but shaped by PRD structure\n        simulated_task = {\n            \"id\": original_info['id'],\n            \"title\": original_info['title'],\n            \"description\": original_info['purpose'],\n            \"status\": original_info.get('status', 'pending'),\n            \"priority\": original_info.get('priority', 'medium'),\n            \"dependencies\": [],\n            \"details\": original_info.get('details', ''),\n            \"subtasks\": [],\n            \"testStrategy\": original_info.get('test_strategy', ''),\n            \"complexity\": len(original_info.get('subtasks', [])),\n            \"recommendedSubtasks\": len(original_info.get('subtasks', [])),\n            \"expansionPrompt\": \"N/A - subtasks already defined.\",\n        }\n        \n        # Add subtasks if they exist\n        for subtask in original_info.get('subtasks', []):\n            simulated_subtask = {\n                \"id\": subtask.get('id', 1),\n                \"title\": subtask.get('title', ''),\n                \"description\": \"\",\n                \"dependencies\": [],\n                \"details\": \"\",\n                \"testStrategy\": \"\",\n                \"status\": subtask.get('status', 'pending'),\n                \"parentId\": original_info['id'],\n            }\n            simulated_task['subtasks'].append(simulated_subtask)\n        \n        tasks_json[\"master\"][\"tasks\"].append(simulated_task)\n    \n    # Write the simulated tasks.json\n    output_path = Path(f\"simulated_tasks_iteration.json\")\n    with open(output_path, 'w', encoding='utf-8') as f:\n        json.dump(tasks_json, f, indent=2)\n    \n    return str(output_path)\n\n\ndef iterative_distance_minimization(original_dir: str, max_iterations: int = 10) -> Dict[str, Any]:\n    \"\"\"\n    Perform iterative distance minimization between original and PRD-generated tasks.\n    \n    Args:\n        original_dir: Directory containing original task markdown files\n        max_iterations: Maximum number of iterations to run\n        \n    Returns:\n        Dictionary with final results and iteration history\n    \"\"\"\n    original_path = Path(original_dir)\n    original_files = list(original_path.glob(\"task-*.md\"))\n    \n    if not original_files:\n        raise ValueError(f\"No task files found in {original_dir}\")\n    \n    print(f\"Found {len(original_files)} original task files\")\n    \n    iteration_history = []\n    best_results = None\n    best_distance = float('inf')\n    \n    for iteration in range(1, max_iterations + 1):\n        print(f\"\\n--- ITERATION {iteration} ---\")\n        \n        # Generate improved PRD based on previous results\n        prev_results = iteration_history[-1]['results'] if iteration_history else None\n        prd_content = generate_improved_prd(original_files, iteration, prev_results)\n        \n        # Save the PRD for this iteration\n        prd_path = Path(f\"generated_prd_iteration_{iteration}.md\")\n        with open(prd_path, 'w', encoding='utf-8') as f:\n            f.write(prd_content)\n        \n        print(f\"Generated PRD for iteration {iteration}\")\n        \n        # Simulate task generation from PRD (in real scenario, this would call task-master)\n        tasks_json_path = simulate_task_generation_from_prd(prd_content, original_files)\n        \n        # Analyze distance between original and generated tasks\n        results = analyze_task_distance(original_dir, tasks_json_path)\n        \n        # Calculate average distance\n        avg_distance = results['aggregated_metrics'].get('average_overall_distance', 1.0)\n        \n        print(f\"Iteration {iteration} - Average distance: {avg_distance:.3f}\")\n        \n        # Store iteration results\n        iteration_data = {\n            'iteration': iteration,\n            'avg_distance': avg_distance,\n            'avg_similarity': results['aggregated_metrics'].get('average_overall_similarity', 0),\n            'results': results,\n            'prd_path': str(prd_path),\n            'tasks_json_path': tasks_json_path\n        }\n        \n        iteration_history.append(iteration_data)\n        \n        # Update best results if this iteration is better\n        if avg_distance < best_distance:\n            best_distance = avg_distance\n            best_results = copy.deepcopy(results)\n            print(f\"NEW BEST: Distance improved to {avg_distance:.3f}\")\n        \n        # Early stopping if we reach a good threshold\n        if avg_distance < 0.1:  # Very close to original\n            print(f\"EARLY STOPPING: Distance below threshold (0.1)\")\n            break\n    \n    # Prepare final results\n    final_results = {\n        'final_iteration': len(iteration_history),\n        'best_distance': best_distance,\n        'best_similarity': 1 - best_distance,\n        'total_iterations': max_iterations,\n        'iteration_history': iteration_history,\n        'best_results': best_results,\n        'original_task_count': len(original_files)\n    }\n    \n    return final_results\n\n\ndef print_final_report(results: Dict[str, Any]):\n    \"\"\"\n    Print a final report of the iterative minimization process.\n    \n    Args:\n        results: Results dictionary from iterative_distance_minimization\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"ITERATIVE DISTANCE MINIMIZATION - FINAL REPORT\")\n    print(\"=\" * 80)\n    \n    print(f\"\\nCONFIGURATION:\")\n    print(f\"  Original tasks: {results['original_task_count']}\")\n    print(f\"  Total iterations: {results['total_iterations']}\")\n    print(f\"  Final iteration: {results['final_iteration']}\")\n    \n    print(f\"\\nBEST RESULTS:\")\n    print(f\"  Best similarity: {results['best_similarity']:.3f}\")\n    print(f\"  Best distance: {results['best_distance']:.3f}\")\n    \n    print(f\"\\nITERATION HISTORY:\")\n    for iteration_data in results['iteration_history']:\n        print(f\"  Iteration {iteration_data['iteration']}: \"\n              f\"Distance={iteration_data['avg_distance']:.3f}, \"\n              f\"Similarity={iteration_data['avg_similarity']:.3f}\")\n    \n    print(f\"\\nBEST ITERATION DETAILS:\")\n    best_iteration = None\n    for iteration_data in results['iteration_history']:\n        if abs(iteration_data['avg_distance'] - results['best_distance']) < 0.001:\n            best_iteration = iteration_data\n            break\n    \n    if best_iteration:\n        print(f\"  Best iteration: {best_iteration['iteration']}\")\n        print(f\"  PRD file: {best_iteration['prd_path']}\")\n        print(f\"  Tasks JSON: {best_iteration['tasks_json_path']}\")\n        \n        print(f\"\\n  DETAILED ANALYSIS OF BEST ITERATION:\")\n        print_analysis_report(best_iteration['results'])\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Iteratively minimize distance between original and PRD-generated tasks\")\n    parser.add_argument(\"--original-dir\", \"-o\", required=True,\n                        help=\"Directory containing original task markdown files\")\n    parser.add_argument(\"--max-iterations\", \"-m\", type=int, default=10,\n                        help=\"Maximum number of iterations (default: 10)\")\n    parser.add_argument(\"--output\", \"-out\", help=\"Output file for final results (JSON format)\")\n    \n    args = parser.parse_args()\n    \n    print(\"Starting iterative task distance minimization...\")\n    print(f\"Original tasks dir: {args.original_dir}\")\n    print(f\"Max iterations: {args.max_iterations}\")\n    \n    results = iterative_distance_minimization(args.original_dir, args.max_iterations)\n    \n    # Print final report\n    print_final_report(results)\n    \n    # Save results if output file specified\n    if args.output:\n        output_path = Path(args.output)\n        with open(output_path, 'w', encoding='utf-8') as f:\n            json.dump(results, f, indent=2, ensure_ascii=False)\n        print(f\"\\nFinal results saved to: {output_path}\")\n    \n    return 0\n\n\nif __name__ == \"__main__\":\n    exit(main())",
  "after": "#!/usr/bin/env python3\n\"\"\"\nIterative Task Distance Minimizer\n\nThis script implements an iterative process to minimize the distance between:\n1. Original task markdown files\n2. Tasks recreated from a PRD\n\nThe process:\n1. Generate PRD from original tasks\n2. Measure distance between original and generated tasks\n3. Adjust PRD generation to improve similarity\n4. Repeat until distance is minimized or max iterations reached\n\nUsage:\n    python iterative_distance_minimizer.py --original-dir /path/to/original/tasks --max-iterations 10\n\"\"\"\n\nimport argparse\nimport json\nimport re\nfrom pathlib import Path\nfrom typing import Dict, List, Any\nimport copy\nfrom task_distance_analyzer import analyze_task_distance, print_analysis_report\nfrom reverse_engineer_prd import create_reverse_engineered_prd, extract_task_info_from_md\nfrom taskmaster_runner import run_task_master_parse_prd, check_task_master_available\n\n\ndef generate_improved_prd(task_files: List[str], iteration: int, prev_results: Dict[str, Any] = None) -> str:\n    \"\"\"\n    Generate an improved PRD based on previous iteration results.\n    \n    Args:\n        task_files: List of paths to task markdown files\n        iteration: Current iteration number\n        prev_results: Results from previous iteration (if any)\n        \n    Returns:\n        String containing the improved PRD\n    \"\"\"\n    # Start with the basic reverse engineered PRD\n    prd_content = create_reverse_engineered_prd(task_files)\n    \n    # If this is not the first iteration, adjust based on previous results\n    if iteration > 1 and prev_results:\n        # Analyze what went wrong in the previous iteration\n        aggregated_metrics = prev_results.get('aggregated_metrics', {})\n        \n        # Identify low-scoring areas to improve\n        field_similarities = aggregated_metrics.get('field_similarities', {})\n        \n        # Add specific improvements based on weak areas\n        improvements = []\n        \n        # Check for low similarity in specific fields\n        for field, similarity in field_similarities.items():\n            if similarity < 0.7:  # Threshold for \"low\" similarity\n                if 'title' in field:\n                    improvements.append(\"# IMPROVEMENT NEEDED: Task titles not matching well\")\n                    improvements.append(\"# Focus on preserving original title structure in capability mapping\")\n                elif 'purpose' in field:\n                    improvements.append(\"# IMPROVEMENT NEEDED: Purpose/description not matching well\")\n                    improvements.append(\"# Ensure purpose sections are properly extracted and mapped\")\n                elif 'status' in field:\n                    improvements.append(\"# IMPROVEMENT NEEDED: Status not matching well\")\n                    improvements.append(\"# Ensure status is properly captured in PRD generation\")\n        \n        # Insert improvements into the PRD content\n        if improvements:\n            # Find a good place to insert improvement notes\n            insertion_point = prd_content.find(\"</functional-decomposition>\")\n            if insertion_point != -1:\n                insertion_point += len(\"</functional-decomposition>\")\n                prd_content = (\n                    prd_content[:insertion_point] +\n                    \"\\n\\n<!-- ITERATION \" + str(iteration) + \" IMPROVEMENTS -->\\n\" +\n                    \"\\n\".join(improvements) +\n                    \"\\n<!-- END IMPROVEMENTS -->\\n\\n\" +\n                    prd_content[insertion_point:]\n                )\n    \n    return prd_content\n\n\ndef run_task_generation_from_prd(prd_file: str, original_task_files: List[str]) -> str:\n    \"\"\"\n    Run task-master parse-prd to generate tasks from PRD.\n    Falls back to simulation if task-master is not available.\n    \n    Args:\n        prd_file: Path to the PRD file\n        original_task_files: List of original task files for reference (for simulation fallback)\n        \n    Returns:\n        Path to the generated tasks.json file\n    \"\"\"\n    return run_task_master_parse_prd(\n        prd_file=prd_file,\n        output_dir=None,\n        fallback_simulation=True,\n        extract_task_info_func=extract_task_info_from_md,\n        original_task_files=original_task_files,\n        simulation_description=\"iterative distance minimization\",\n    )\n\n\ndef iterative_distance_minimization(original_dir: str, max_iterations: int = 10) -> Dict[str, Any]:\n    \"\"\"\n    Perform iterative distance minimization between original and PRD-generated tasks.\n    \n    Args:\n        original_dir: Directory containing original task markdown files\n        max_iterations: Maximum number of iterations to run\n        \n    Returns:\n        Dictionary with final results and iteration history\n    \"\"\"\n    original_path = Path(original_dir)\n    original_files = list(original_path.glob(\"task-*.md\"))\n    \n    if not original_files:\n        raise ValueError(f\"No task files found in {original_dir}\")\n    \n    print(f\"Found {len(original_files)} original task files\")\n    \n    iteration_history = []\n    best_results = None\n    best_distance = float('inf')\n    \n    for iteration in range(1, max_iterations + 1):\n        print(f\"\\n--- ITERATION {iteration} ---\")\n        \n        # Generate improved PRD based on previous results\n        prev_results = iteration_history[-1]['results'] if iteration_history else None\n        prd_content = generate_improved_prd(original_files, iteration, prev_results)\n        \n        # Save the PRD for this iteration\n        prd_path = Path(f\"generated_prd_iteration_{iteration}.md\")\n        with open(prd_path, 'w', encoding='utf-8') as f:\n            f.write(prd_content)\n        \n        print(f\"Generated PRD for iteration {iteration}\")\n        \n        # Run task-master parse-prd (falls back to simulation if unavailable)\n        tasks_json_path = run_task_generation_from_prd(str(prd_path), original_files)\n        \n        # Analyze distance between original and generated tasks\n        results = analyze_task_distance(original_dir, tasks_json_path)\n        \n        # Calculate average distance\n        avg_distance = results['aggregated_metrics'].get('average_overall_distance', 1.0)\n        \n        print(f\"Iteration {iteration} - Average distance: {avg_distance:.3f}\")\n        \n        # Store iteration results\n        iteration_data = {\n            'iteration': iteration,\n            'avg_distance': avg_distance,\n            'avg_similarity': results['aggregated_metrics'].get('average_overall_similarity', 0),\n            'results': results,\n            'prd_path': str(prd_path),\n            'tasks_json_path': tasks_json_path\n        }\n        \n        iteration_history.append(iteration_data)\n        \n        # Update best results if this iteration is better\n        if avg_distance < best_distance:\n            best_distance = avg_distance\n            best_results = copy.deepcopy(results)\n            print(f\"NEW BEST: Distance improved to {avg_distance:.3f}\")\n        \n        # Early stopping if we reach a good threshold\n        if avg_distance < 0.1:  # Very close to original\n            print(f\"EARLY STOPPING: Distance below threshold (0.1)\")\n            break\n    \n    # Prepare final results\n    final_results = {\n        'final_iteration': len(iteration_history),\n        'best_distance': best_distance,\n        'best_similarity': 1 - best_distance,\n        'total_iterations': max_iterations,\n        'iteration_history': iteration_history,\n        'best_results': best_results,\n        'original_task_count': len(original_files)\n    }\n    \n    return final_results\n\n\ndef print_final_report(results: Dict[str, Any]):\n    \"\"\"\n    Print a final report of the iterative minimization process.\n    \n    Args:\n        results: Results dictionary from iterative_distance_minimization\n    \"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"ITERATIVE DISTANCE MINIMIZATION - FINAL REPORT\")\n    print(\"=\" * 80)\n    \n    print(f\"\\nCONFIGURATION:\")\n    print(f\"  Original tasks: {results['original_task_count']}\")\n    print(f\"  Total iterations: {results['total_iterations']}\")\n    print(f\"  Final iteration: {results['final_iteration']}\")\n    \n    print(f\"\\nBEST RESULTS:\")\n    print(f\"  Best similarity: {results['best_similarity']:.3f}\")\n    print(f\"  Best distance: {results['best_distance']:.3f}\")\n    \n    print(f\"\\nITERATION HISTORY:\")\n    for iteration_data in results['iteration_history']:\n        print(f\"  Iteration {iteration_data['iteration']}: \"\n              f\"Distance={iteration_data['avg_distance']:.3f}, \"\n              f\"Similarity={iteration_data['avg_similarity']:.3f}\")\n    \n    print(f\"\\nBEST ITERATION DETAILS:\")\n    best_iteration = None\n    for iteration_data in results['iteration_history']:\n        if abs(iteration_data['avg_distance'] - results['best_distance']) < 0.001:\n            best_iteration = iteration_data\n            break\n    \n    if best_iteration:\n        print(f\"  Best iteration: {best_iteration['iteration']}\")\n        print(f\"  PRD file: {best_iteration['prd_path']}\")\n        print(f\"  Tasks JSON: {best_iteration['tasks_json_path']}\")\n        \n        print(f\"\\n  DETAILED ANALYSIS OF BEST ITERATION:\")\n        print_analysis_report(best_iteration['results'])\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Iteratively minimize distance between original and PRD-generated tasks\")\n    parser.add_argument(\"--original-dir\", \"-o\", required=True,\n                        help=\"Directory containing original task markdown files\")\n    parser.add_argument(\"--max-iterations\", \"-m\", type=int, default=10,\n                        help=\"Maximum number of iterations (default: 10)\")\n    parser.add_argument(\"--output\", \"-out\", help=\"Output file for final results (JSON format)\")\n    \n    args = parser.parse_args()\n    \n    print(\"Starting iterative task distance minimization...\")\n    print(f\"Original tasks dir: {args.original_dir}\")\n    print(f\"Max iterations: {args.max_iterations}\")\n    \n    results = iterative_distance_minimization(args.original_dir, args.max_iterations)\n    \n    # Print final report\n    print_final_report(results)\n    \n    # Save results if output file specified\n    if args.output:\n        output_path = Path(args.output)\n        with open(output_path, 'w', encoding='utf-8') as f:\n            json.dump(results, f, indent=2, ensure_ascii=False)\n        print(f\"\\nFinal results saved to: {output_path}\")\n    \n    return 0\n\n\nif __name__ == \"__main__\":\n    exit(main())",
  "diff": "Index: file:///home/masum/github/PR/.taskmaster/scripts/iterative_distance_minimizer.py\n===================================================================\n--- file:///home/masum/github/PR/.taskmaster/scripts/iterative_distance_minimizer.py\toriginal\n+++ file:///home/masum/github/PR/.taskmaster/scripts/iterative_distance_minimizer.py\tmodified\n@@ -23,8 +23,9 @@\n from typing import Dict, List, Any\n import copy\n from task_distance_analyzer import analyze_task_distance, print_analysis_report\n from reverse_engineer_prd import create_reverse_engineered_prd, extract_task_info_from_md\n+from taskmaster_runner import run_task_master_parse_prd, check_task_master_available\n \n \n def generate_improved_prd(task_files: List[str], iteration: int, prev_results: Dict[str, Any] = None) -> str:\n     \"\"\"\n@@ -81,81 +82,30 @@\n     \n     return prd_content\n \n \n-def simulate_task_generation_from_prd(prd_content: str, original_task_files: List[str]) -> str:\n+def run_task_generation_from_prd(prd_file: str, original_task_files: List[str]) -> str:\n     \"\"\"\n-    Simulate the task-master parse-prd process by creating a mock tasks.json.\n+    Run task-master parse-prd to generate tasks from PRD.\n+    Falls back to simulation if task-master is not available.\n     \n-    In a real scenario, this would call `task-master parse-prd <prd_file>`,\n-    but for simulation purposes we'll create a mock JSON based on the PRD content.\n-    \n     Args:\n-        prd_content: Content of the PRD file\n-        original_task_files: List of original task files for reference\n+        prd_file: Path to the PRD file\n+        original_task_files: List of original task files for reference (for simulation fallback)\n         \n     Returns:\n-        Path to the simulated tasks.json file\n+        Path to the generated tasks.json file\n     \"\"\"\n-    # In a real implementation, we would call task-master here\n-    # For simulation, we'll create a mock JSON based on the original tasks\n-    # but using the structure implied by the PRD\n+    return run_task_master_parse_prd(\n+        prd_file=prd_file,\n+        output_dir=None,\n+        fallback_simulation=True,\n+        extract_task_info_func=extract_task_info_from_md,\n+        original_task_files=original_task_files,\n+        simulation_description=\"iterative distance minimization\",\n+    )\n \n-    # For now, let's create a mock tasks.json that represents what task-master\n-    # might generate from the PRD\n-    tasks_json = {\n-        \"master\": {\n-            \"name\": \"Task Master\",\n-            \"version\": \"1.0.0\",\n-            \"description\": \"Tasks generated from PRD (iteration simulation)\",\n-            \"tasks\": []\n-        }\n-    }\n \n-    # Extract task information from original files to simulate what might be generated\n-    for task_file in original_task_files:\n-        original_info = extract_task_info_from_md(task_file)\n-        \n-        # Create a simulated task based on the original but shaped by PRD structure\n-        simulated_task = {\n-            \"id\": original_info['id'],\n-            \"title\": original_info['title'],\n-            \"description\": original_info['purpose'],\n-            \"status\": original_info.get('status', 'pending'),\n-            \"priority\": original_info.get('priority', 'medium'),\n-            \"dependencies\": [],\n-            \"details\": original_info.get('details', ''),\n-            \"subtasks\": [],\n-            \"testStrategy\": original_info.get('test_strategy', ''),\n-            \"complexity\": len(original_info.get('subtasks', [])),\n-            \"recommendedSubtasks\": len(original_info.get('subtasks', [])),\n-            \"expansionPrompt\": \"N/A - subtasks already defined.\",\n-        }\n-        \n-        # Add subtasks if they exist\n-        for subtask in original_info.get('subtasks', []):\n-            simulated_subtask = {\n-                \"id\": subtask.get('id', 1),\n-                \"title\": subtask.get('title', ''),\n-                \"description\": \"\",\n-                \"dependencies\": [],\n-                \"details\": \"\",\n-                \"testStrategy\": \"\",\n-                \"status\": subtask.get('status', 'pending'),\n-                \"parentId\": original_info['id'],\n-            }\n-            simulated_task['subtasks'].append(simulated_subtask)\n-        \n-        tasks_json[\"master\"][\"tasks\"].append(simulated_task)\n-    \n-    # Write the simulated tasks.json\n-    output_path = Path(f\"simulated_tasks_iteration.json\")\n-    with open(output_path, 'w', encoding='utf-8') as f:\n-        json.dump(tasks_json, f, indent=2)\n-    \n-    return str(output_path)\n-\n-\n def iterative_distance_minimization(original_dir: str, max_iterations: int = 10) -> Dict[str, Any]:\n     \"\"\"\n     Perform iterative distance minimization between original and PRD-generated tasks.\n     \n@@ -191,10 +141,10 @@\n             f.write(prd_content)\n         \n         print(f\"Generated PRD for iteration {iteration}\")\n         \n-        # Simulate task generation from PRD (in real scenario, this would call task-master)\n-        tasks_json_path = simulate_task_generation_from_prd(prd_content, original_files)\n+        # Run task-master parse-prd (falls back to simulation if unavailable)\n+        tasks_json_path = run_task_generation_from_prd(str(prd_path), original_files)\n         \n         # Analyze distance between original and generated tasks\n         results = analyze_task_distance(original_dir, tasks_json_path)\n         \n",
  "isNewFile": false,
  "reverted": false,
  "timestamp": 1769442260302
}