{
  "id": "51c1cd96-4e3a-43da-aaf7-2865ac567e57",
  "uri": "file:///home/masum/github/PR/.taskmaster/scripts/test_round_trip.py",
  "before": "#!/usr/bin/env python3\n\"\"\"\nRound-trip Test for PRD Generation\nTests the round-trip process: Tasks → PRD → Tasks and measures fidelity\n\"\"\"\n\nimport argparse\nimport tempfile\nimport os\nfrom pathlib import Path\nfrom typing import Dict, Any, List\nimport json\nimport subprocess\nimport sys\n\n# Add the current directory to the path to import the module\nimport sys\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\n\n# Import the ultra enhanced PRD generation\nfrom ultra_enhanced_reverse_engineer_prd import create_ultra_enhanced_reverse_engineered_prd, extract_task_info_from_md_ultra_enhanced\n\n\ndef simulate_task_master_parse_prd(prd_content: str, original_task_files: List[str]) -> str:\n    \"\"\"\n    Simulate the task-master parse-prd process by creating a mock tasks.json.\n\n    In a real scenario, this would call `task-master parse-prd <prd_file>`,\n    but for testing purposes we'll create a mock JSON based on the PRD content\n    and the original tasks for comparison.\n    \"\"\"\n    # This is a simplified simulation - in reality, task-master would process the PRD\n    # For this test, we'll create a mock JSON based on the original tasks\n    # but shaped by PRD structure\n\n    # Import the extraction function from the ultra enhanced approach\n    from ultra_enhanced_reverse_engineer_prd import extract_task_info_from_md_ultra_enhanced\n    \n    tasks_json = {\n        \"master\": {\n            \"name\": \"Task Master\",\n            \"version\": \"1.0.0\",\n            \"description\": \"Tasks generated from PRD (round-trip test)\",\n            \"tasks\": []\n        }\n    }\n\n    # Extract task information from original files to simulate what might be generated\n    for task_file in original_task_files:\n        original_info = extract_task_info_from_md_ultra_enhanced(task_file)\n\n        # Create a simulated task based on the original but shaped by PRD structure\n        simulated_task = {\n            \"id\": original_info['id'],\n            \"title\": original_info['title'],\n            \"description\": original_info['purpose'],\n            \"status\": original_info.get('status', 'pending'),\n            \"priority\": original_info.get('priority', 'medium'),\n            \"dependencies\": [],\n            \"details\": original_info.get('details', ''),\n            \"subtasks\": [],\n            \"testStrategy\": original_info.get('test_strategy', ''),\n            \"complexity\": len(original_info.get('subtasks', [])),\n            \"recommendedSubtasks\": len(original_info.get('subtasks', [])),\n            \"expansionPrompt\": \"N/A - subtasks already defined.\",\n        }\n\n        # Add subtasks if they exist\n        for subtask in original_info.get('subtasks', []):\n            simulated_subtask = {\n                \"id\": subtask.get('id', 1),\n                \"title\": subtask.get('title', ''),\n                \"description\": \"\",\n                \"dependencies\": [],\n                \"details\": \"\",\n                \"testStrategy\": \"\",\n                \"status\": subtask.get('status', 'pending'),\n                \"parentId\": original_info['id'],\n            }\n            simulated_task['subtasks'].append(simulated_subtask)\n\n        tasks_json[\"master\"][\"tasks\"].append(simulated_task)\n\n    # Write the simulated tasks.json\n    output_path = Path(f\"roundtrip_simulated_tasks.json\")\n    with open(output_path, 'w', encoding='utf-8') as f:\n        json.dump(tasks_json, f, indent=2)\n\n    return str(output_path)\n\n\ndef calculate_similarity(text1: str, text2: str) -> float:\n    \"\"\"\n    Calculate similarity between two text strings.\n\n    Args:\n        text1: First text string\n        text2: Second text string\n\n    Returns:\n        Float between 0 and 1 representing similarity\n    \"\"\"\n    if not text1 and not text2:\n        return 1.0\n    if not text1 or not text2:\n        return 0.0\n\n    # Simple character-based similarity\n    from difflib import SequenceMatcher\n    return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()\n\n\ndef calculate_task_similarity(original_task: Dict[str, Any], generated_task: Dict[str, Any]) -> Dict[str, float]:\n    \"\"\"\n    Calculate similarity between original and generated tasks.\n\n    Args:\n        original_task: Dictionary with original task information\n        generated_task: Dictionary with generated task information\n\n    Returns:\n        Dictionary with similarity scores for each field\n    \"\"\"\n    similarities = {}\n\n    # Fields to compare\n    fields_to_compare = [\n        'title', 'status', 'priority', 'effort', 'complexity',\n        'dependencies', 'purpose', 'details', 'test_strategy', 'blocks'\n    ]\n\n    for field in fields_to_compare:\n        orig_val = original_task.get(field, '')\n        gen_val = generated_task.get(field, '')\n\n        similarity = calculate_similarity(str(orig_val), str(gen_val))\n        similarities[f'{field}_similarity'] = similarity\n\n    # Compare success criteria\n    orig_criteria = original_task.get('success_criteria', [])\n    gen_criteria = generated_task.get('success_criteria', [])\n\n    if orig_criteria or gen_criteria:\n        # Simple overlap ratio\n        orig_set = set(str(c).lower() for c in orig_criteria)\n        gen_set = set(str(c).lower() for c in gen_criteria)\n\n        if orig_set or gen_set:\n            intersection = len(orig_set.intersection(gen_set))\n            union = len(orig_set.union(gen_set))\n            criteria_similarity = intersection / union if union > 0 else 0\n        else:\n            criteria_similarity = 1.0\n    else:\n        criteria_similarity = 1.0\n\n    similarities['success_criteria_similarity'] = criteria_similarity\n\n    # Calculate overall similarity\n    total_similarity = sum(similarities.values())\n    num_fields = len(similarities)\n    overall_similarity = total_similarity / num_fields if num_fields > 0 else 1.0\n    similarities['overall_similarity'] = overall_similarity\n    similarities['overall_distance'] = 1 - overall_similarity\n\n    return similarities\n\n\ndef run_round_trip_test(task_files: List[str]) -> Dict[str, Any]:\n    \"\"\"\n    Run the round-trip test: Tasks → PRD → Tasks and measure fidelity.\n\n    Args:\n        task_files: List of paths to task markdown files\n\n    Returns:\n        Dictionary with test results and similarity metrics\n    \"\"\"\n    print(\"Running round-trip test: Tasks → PRD → Tasks\")\n    print(f\"Processing {len(task_files)} task files\")\n\n    # Step 1: Generate PRD from original tasks\n    print(\"Step 1: Generating PRD from original tasks...\")\n    prd_content = create_ultra_enhanced_reverse_engineered_prd(task_files)\n\n    # Save the PRD for inspection\n    prd_path = Path(\"roundtrip_test_prd.md\")\n    with open(prd_path, 'w', encoding='utf-8') as f:\n        f.write(prd_content)\n\n    print(f\"Generated PRD saved to {prd_path}\")\n\n    # Step 2: Simulate task-master parse-prd to generate tasks from PRD\n    print(\"Step 2: Simulating task-master parse-prd to generate tasks from PRD...\")\n    generated_tasks_path = simulate_task_master_parse_prd(prd_content, task_files)\n\n    # Step 3: Compare original tasks with generated tasks\n    print(\"Step 3: Comparing original tasks with generated tasks...\")\n\n    # Load original tasks\n    original_tasks = {}\n    for task_file in task_files:\n        task_info = extract_task_info_from_md_ultra_enhanced(task_file)\n        original_tasks[task_info['id']] = task_info\n\n    # For this test, we'll use the original structure but compare the content\n    # In a real scenario, we'd parse the generated tasks.json\n    comparison_results = {\n        'original_task_count': len(original_tasks),\n        'compared_tasks': 0,\n        'field_similarities': {},\n        'task_similarities': {},\n        'average_overall_similarity': 0,\n        'average_overall_distance': 0\n    }\n\n    total_similarity = 0\n    compared_count = 0\n\n    # Since we're simulating, we'll compare the original tasks with themselves\n    # to validate our comparison logic\n    for task_id, original_task in original_tasks.items():\n        # In a real scenario, we would load the generated task here\n        # For now, we'll just use the original task to test the comparison logic\n        generated_task = original_task  # This is just for testing the framework\n\n        # Calculate similarity\n        task_similarities = calculate_task_similarity(original_task, generated_task)\n\n        comparison_results['task_similarities'][task_id] = task_similarities\n        total_similarity += task_similarities['overall_similarity']\n        compared_count += 1\n\n        # Accumulate field similarities\n        for field, sim_value in task_similarities.items():\n            if '_similarity' in field:\n                if field not in comparison_results['field_similarities']:\n                    comparison_results['field_similarities'][field] = []\n                comparison_results['field_similarities'][field].append(sim_value)\n\n    comparison_results['compared_tasks'] = compared_count\n\n    if compared_count > 0:\n        comparison_results['average_overall_similarity'] = total_similarity / compared_count\n        comparison_results['average_overall_distance'] = 1 - (total_similarity / compared_count)\n\n    # Calculate average field similarities\n    for field, values in comparison_results['field_similarities'].items():\n        if values:\n            comparison_results['field_similarities'][field] = sum(values) / len(values)\n\n    return comparison_results\n\n\ndef print_round_trip_results(results: Dict[str, Any]):\n    \"\"\"\n    Print the round-trip test results.\n\n    Args:\n        results: Results dictionary from run_round_trip_test\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"ROUND-TRIP TEST RESULTS\")\n    print(\"=\"*80)\n\n    print(f\"\\nTEST SUMMARY:\")\n    print(f\"  Original tasks processed: {results['original_task_count']}\")\n    print(f\"  Tasks compared: {results['compared_tasks']}\")\n    print(f\"  Average overall similarity: {results['average_overall_similarity']:.3f}\")\n    print(f\"  Average overall distance: {results['average_overall_distance']:.3f}\")\n\n    print(f\"\\nFIELD SIMILARITIES:\")\n    for field, avg_sim in sorted(results['field_similarities'].items()):\n        print(f\"  {field}: {avg_sim:.3f}\")\n\n    print(f\"\\nThe test validates the framework for measuring round-trip fidelity.\")\n    print(f\"In a real scenario, the generated tasks would be compared to the original tasks.\")\n    print(f\"Higher similarity scores indicate better preservation of information.\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Run round-trip test for PRD generation\")\n    parser.add_argument(\"--input-dir\", \"-i\", required=True, help=\"Directory containing task markdown files\")\n    parser.add_argument(\"--pattern\", default=\"task*.md\", help=\"File pattern to match (default: task*.md)\")\n\n    args = parser.parse_args()\n\n    print(\"Starting round-trip test for PRD generation...\")\n    print(f\"Input directory: {args.input_dir}\")\n    print(f\"File pattern: {args.pattern}\")\n\n    # Find all task markdown files\n    input_path = Path(args.input_dir)\n    task_files = list(input_path.glob(args.pattern))\n\n    if not task_files:\n        print(f\"No task files found in {input_path} with pattern {args.pattern}\")\n        return 1\n\n    print(f\"Found {len(task_files)} task files for testing\")\n\n    # Run the round-trip test\n    results = run_round_trip_test(task_files)\n\n    # Print the results\n    print_round_trip_results(results)\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    exit(main())",
  "after": "#!/usr/bin/env python3\n\"\"\"\nRound-trip Test for PRD Generation\nTests the round-trip process: Tasks → PRD → Tasks and measures fidelity\n\"\"\"\n\nimport argparse\nimport tempfile\nimport os\nfrom pathlib import Path\nfrom typing import Dict, Any, List\nimport json\nimport subprocess\nimport sys\n\n# Add the current directory to the path to import the module\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\n\n# Import the ultra enhanced PRD generation\nfrom ultra_enhanced_reverse_engineer_prd import create_ultra_enhanced_reverse_engineered_prd, extract_task_info_from_md_ultra_enhanced\nfrom taskmaster_runner import run_task_master_parse_prd, check_task_master_available\n\n\ndef run_parse_prd_for_round_trip(prd_file: str, original_task_files: List[str]) -> str:\n    \"\"\"\n    Run task-master parse-prd to generate tasks from PRD.\n    Falls back to simulation if task-master is not available.\n\n    Args:\n        prd_file: Path to the PRD file\n        original_task_files: List of original task files (for simulation fallback)\n\n    Returns:\n        Path to the generated tasks.json file\n    \"\"\"\n    return run_task_master_parse_prd(\n        prd_file=prd_file,\n        output_dir=None,\n        fallback_simulation=True,\n        extract_task_info_func=extract_task_info_from_md_ultra_enhanced,\n        original_task_files=original_task_files,\n        simulation_description=\"round-trip test\",\n    )\n\n\ndef calculate_similarity(text1: str, text2: str) -> float:\n    \"\"\"\n    Calculate similarity between two text strings.\n\n    Args:\n        text1: First text string\n        text2: Second text string\n\n    Returns:\n        Float between 0 and 1 representing similarity\n    \"\"\"\n    if not text1 and not text2:\n        return 1.0\n    if not text1 or not text2:\n        return 0.0\n\n    # Simple character-based similarity\n    from difflib import SequenceMatcher\n    return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()\n\n\ndef calculate_task_similarity(original_task: Dict[str, Any], generated_task: Dict[str, Any]) -> Dict[str, float]:\n    \"\"\"\n    Calculate similarity between original and generated tasks.\n\n    Args:\n        original_task: Dictionary with original task information\n        generated_task: Dictionary with generated task information\n\n    Returns:\n        Dictionary with similarity scores for each field\n    \"\"\"\n    similarities = {}\n\n    # Fields to compare\n    fields_to_compare = [\n        'title', 'status', 'priority', 'effort', 'complexity',\n        'dependencies', 'purpose', 'details', 'test_strategy', 'blocks'\n    ]\n\n    for field in fields_to_compare:\n        orig_val = original_task.get(field, '')\n        gen_val = generated_task.get(field, '')\n\n        similarity = calculate_similarity(str(orig_val), str(gen_val))\n        similarities[f'{field}_similarity'] = similarity\n\n    # Compare success criteria\n    orig_criteria = original_task.get('success_criteria', [])\n    gen_criteria = generated_task.get('success_criteria', [])\n\n    if orig_criteria or gen_criteria:\n        # Simple overlap ratio\n        orig_set = set(str(c).lower() for c in orig_criteria)\n        gen_set = set(str(c).lower() for c in gen_criteria)\n\n        if orig_set or gen_set:\n            intersection = len(orig_set.intersection(gen_set))\n            union = len(orig_set.union(gen_set))\n            criteria_similarity = intersection / union if union > 0 else 0\n        else:\n            criteria_similarity = 1.0\n    else:\n        criteria_similarity = 1.0\n\n    similarities['success_criteria_similarity'] = criteria_similarity\n\n    # Calculate overall similarity\n    total_similarity = sum(similarities.values())\n    num_fields = len(similarities)\n    overall_similarity = total_similarity / num_fields if num_fields > 0 else 1.0\n    similarities['overall_similarity'] = overall_similarity\n    similarities['overall_distance'] = 1 - overall_similarity\n\n    return similarities\n\n\ndef run_round_trip_test(task_files: List[str]) -> Dict[str, Any]:\n    \"\"\"\n    Run the round-trip test: Tasks → PRD → Tasks and measure fidelity.\n\n    Args:\n        task_files: List of paths to task markdown files\n\n    Returns:\n        Dictionary with test results and similarity metrics\n    \"\"\"\n    print(\"Running round-trip test: Tasks → PRD → Tasks\")\n    print(f\"Processing {len(task_files)} task files\")\n\n    # Step 1: Generate PRD from original tasks\n    print(\"Step 1: Generating PRD from original tasks...\")\n    prd_content = create_ultra_enhanced_reverse_engineered_prd(task_files)\n\n    # Save the PRD for inspection\n    prd_path = Path(\"roundtrip_test_prd.md\")\n    with open(prd_path, 'w', encoding='utf-8') as f:\n        f.write(prd_content)\n\n    print(f\"Generated PRD saved to {prd_path}\")\n\n    # Step 2: Run task-master parse-prd to generate tasks from PRD\n    print(\"Step 2: Running task-master parse-prd to generate tasks from PRD...\")\n    generated_tasks_path = run_parse_prd_for_round_trip(str(prd_path), task_files)\n\n    # Step 3: Compare original tasks with generated tasks\n    print(\"Step 3: Comparing original tasks with generated tasks...\")\n\n    # Load original tasks\n    original_tasks = {}\n    for task_file in task_files:\n        task_info = extract_task_info_from_md_ultra_enhanced(task_file)\n        original_tasks[task_info['id']] = task_info\n\n    # For this test, we'll use the original structure but compare the content\n    # In a real scenario, we'd parse the generated tasks.json\n    comparison_results = {\n        'original_task_count': len(original_tasks),\n        'compared_tasks': 0,\n        'field_similarities': {},\n        'task_similarities': {},\n        'average_overall_similarity': 0,\n        'average_overall_distance': 0\n    }\n\n    total_similarity = 0\n    compared_count = 0\n\n    # Since we're simulating, we'll compare the original tasks with themselves\n    # to validate our comparison logic\n    for task_id, original_task in original_tasks.items():\n        # In a real scenario, we would load the generated task here\n        # For now, we'll just use the original task to test the comparison logic\n        generated_task = original_task  # This is just for testing the framework\n\n        # Calculate similarity\n        task_similarities = calculate_task_similarity(original_task, generated_task)\n\n        comparison_results['task_similarities'][task_id] = task_similarities\n        total_similarity += task_similarities['overall_similarity']\n        compared_count += 1\n\n        # Accumulate field similarities\n        for field, sim_value in task_similarities.items():\n            if '_similarity' in field:\n                if field not in comparison_results['field_similarities']:\n                    comparison_results['field_similarities'][field] = []\n                comparison_results['field_similarities'][field].append(sim_value)\n\n    comparison_results['compared_tasks'] = compared_count\n\n    if compared_count > 0:\n        comparison_results['average_overall_similarity'] = total_similarity / compared_count\n        comparison_results['average_overall_distance'] = 1 - (total_similarity / compared_count)\n\n    # Calculate average field similarities\n    for field, values in comparison_results['field_similarities'].items():\n        if values:\n            comparison_results['field_similarities'][field] = sum(values) / len(values)\n\n    return comparison_results\n\n\ndef print_round_trip_results(results: Dict[str, Any]):\n    \"\"\"\n    Print the round-trip test results.\n\n    Args:\n        results: Results dictionary from run_round_trip_test\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"ROUND-TRIP TEST RESULTS\")\n    print(\"=\"*80)\n\n    print(f\"\\nTEST SUMMARY:\")\n    print(f\"  Original tasks processed: {results['original_task_count']}\")\n    print(f\"  Tasks compared: {results['compared_tasks']}\")\n    print(f\"  Average overall similarity: {results['average_overall_similarity']:.3f}\")\n    print(f\"  Average overall distance: {results['average_overall_distance']:.3f}\")\n\n    print(f\"\\nFIELD SIMILARITIES:\")\n    for field, avg_sim in sorted(results['field_similarities'].items()):\n        print(f\"  {field}: {avg_sim:.3f}\")\n\n    print(f\"\\nThe test validates the framework for measuring round-trip fidelity.\")\n    print(f\"In a real scenario, the generated tasks would be compared to the original tasks.\")\n    print(f\"Higher similarity scores indicate better preservation of information.\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Run round-trip test for PRD generation\")\n    parser.add_argument(\"--input-dir\", \"-i\", required=True, help=\"Directory containing task markdown files\")\n    parser.add_argument(\"--pattern\", default=\"task*.md\", help=\"File pattern to match (default: task*.md)\")\n\n    args = parser.parse_args()\n\n    print(\"Starting round-trip test for PRD generation...\")\n    print(f\"Input directory: {args.input_dir}\")\n    print(f\"File pattern: {args.pattern}\")\n\n    # Find all task markdown files\n    input_path = Path(args.input_dir)\n    task_files = list(input_path.glob(args.pattern))\n\n    if not task_files:\n        print(f\"No task files found in {input_path} with pattern {args.pattern}\")\n        return 1\n\n    print(f\"Found {len(task_files)} task files for testing\")\n\n    # Run the round-trip test\n    results = run_round_trip_test(task_files)\n\n    # Print the results\n    print_round_trip_results(results)\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    exit(main())",
  "diff": "Index: file:///home/masum/github/PR/.taskmaster/scripts/test_round_trip.py\n===================================================================\n--- file:///home/masum/github/PR/.taskmaster/scripts/test_round_trip.py\toriginal\n+++ file:///home/masum/github/PR/.taskmaster/scripts/test_round_trip.py\tmodified\n@@ -13,83 +13,37 @@\n import subprocess\n import sys\n \n # Add the current directory to the path to import the module\n-import sys\n sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n \n # Import the ultra enhanced PRD generation\n from ultra_enhanced_reverse_engineer_prd import create_ultra_enhanced_reverse_engineered_prd, extract_task_info_from_md_ultra_enhanced\n+from taskmaster_runner import run_task_master_parse_prd, check_task_master_available\n \n \n-def simulate_task_master_parse_prd(prd_content: str, original_task_files: List[str]) -> str:\n+def run_parse_prd_for_round_trip(prd_file: str, original_task_files: List[str]) -> str:\n     \"\"\"\n-    Simulate the task-master parse-prd process by creating a mock tasks.json.\n+    Run task-master parse-prd to generate tasks from PRD.\n+    Falls back to simulation if task-master is not available.\n \n-    In a real scenario, this would call `task-master parse-prd <prd_file>`,\n-    but for testing purposes we'll create a mock JSON based on the PRD content\n-    and the original tasks for comparison.\n+    Args:\n+        prd_file: Path to the PRD file\n+        original_task_files: List of original task files (for simulation fallback)\n+\n+    Returns:\n+        Path to the generated tasks.json file\n     \"\"\"\n-    # This is a simplified simulation - in reality, task-master would process the PRD\n-    # For this test, we'll create a mock JSON based on the original tasks\n-    # but shaped by PRD structure\n+    return run_task_master_parse_prd(\n+        prd_file=prd_file,\n+        output_dir=None,\n+        fallback_simulation=True,\n+        extract_task_info_func=extract_task_info_from_md_ultra_enhanced,\n+        original_task_files=original_task_files,\n+        simulation_description=\"round-trip test\",\n+    )\n \n-    # Import the extraction function from the ultra enhanced approach\n-    from ultra_enhanced_reverse_engineer_prd import extract_task_info_from_md_ultra_enhanced\n \n-    tasks_json = {\n-        \"master\": {\n-            \"name\": \"Task Master\",\n-            \"version\": \"1.0.0\",\n-            \"description\": \"Tasks generated from PRD (round-trip test)\",\n-            \"tasks\": []\n-        }\n-    }\n-\n-    # Extract task information from original files to simulate what might be generated\n-    for task_file in original_task_files:\n-        original_info = extract_task_info_from_md_ultra_enhanced(task_file)\n-\n-        # Create a simulated task based on the original but shaped by PRD structure\n-        simulated_task = {\n-            \"id\": original_info['id'],\n-            \"title\": original_info['title'],\n-            \"description\": original_info['purpose'],\n-            \"status\": original_info.get('status', 'pending'),\n-            \"priority\": original_info.get('priority', 'medium'),\n-            \"dependencies\": [],\n-            \"details\": original_info.get('details', ''),\n-            \"subtasks\": [],\n-            \"testStrategy\": original_info.get('test_strategy', ''),\n-            \"complexity\": len(original_info.get('subtasks', [])),\n-            \"recommendedSubtasks\": len(original_info.get('subtasks', [])),\n-            \"expansionPrompt\": \"N/A - subtasks already defined.\",\n-        }\n-\n-        # Add subtasks if they exist\n-        for subtask in original_info.get('subtasks', []):\n-            simulated_subtask = {\n-                \"id\": subtask.get('id', 1),\n-                \"title\": subtask.get('title', ''),\n-                \"description\": \"\",\n-                \"dependencies\": [],\n-                \"details\": \"\",\n-                \"testStrategy\": \"\",\n-                \"status\": subtask.get('status', 'pending'),\n-                \"parentId\": original_info['id'],\n-            }\n-            simulated_task['subtasks'].append(simulated_subtask)\n-\n-        tasks_json[\"master\"][\"tasks\"].append(simulated_task)\n-\n-    # Write the simulated tasks.json\n-    output_path = Path(f\"roundtrip_simulated_tasks.json\")\n-    with open(output_path, 'w', encoding='utf-8') as f:\n-        json.dump(tasks_json, f, indent=2)\n-\n-    return str(output_path)\n-\n-\n def calculate_similarity(text1: str, text2: str) -> float:\n     \"\"\"\n     Calculate similarity between two text strings.\n \n@@ -189,11 +143,11 @@\n         f.write(prd_content)\n \n     print(f\"Generated PRD saved to {prd_path}\")\n \n-    # Step 2: Simulate task-master parse-prd to generate tasks from PRD\n-    print(\"Step 2: Simulating task-master parse-prd to generate tasks from PRD...\")\n-    generated_tasks_path = simulate_task_master_parse_prd(prd_content, task_files)\n+    # Step 2: Run task-master parse-prd to generate tasks from PRD\n+    print(\"Step 2: Running task-master parse-prd to generate tasks from PRD...\")\n+    generated_tasks_path = run_parse_prd_for_round_trip(str(prd_path), task_files)\n \n     # Step 3: Compare original tasks with generated tasks\n     print(\"Step 3: Comparing original tasks with generated tasks...\")\n \n",
  "isNewFile": false,
  "reverted": false,
  "timestamp": 1769442351900
}